var documenterSearchIndex = {"docs":
[{"location":"manual/more_on_nodes/","page":"More on nodes","title":"More on nodes","text":"using Mill","category":"page"},{"location":"manual/more_on_nodes/#More-on-nodes","page":"More on nodes","title":"More on nodes","text":"","category":"section"},{"location":"manual/more_on_nodes/#Node-nesting","page":"More on nodes","title":"Node nesting","text":"","category":"section"},{"location":"manual/more_on_nodes/","page":"More on nodes","title":"More on nodes","text":"The main advantage of the Mill library is that it allows to arbitrarily nest and cross-product BagModels, as described in Theorem 5 in Tomáš Pevný , Vojtěch Kovařík  (2019). In other words, instances themselves may be represented in much more complex way than in the BagNode and BagModel example.","category":"page"},{"location":"manual/more_on_nodes/","page":"More on nodes","title":"More on nodes","text":"Let's start the demonstration by nesting two MIL problems. The outer MIL model contains three samples (outer-level bags), whose instances are (inner-level) bags themselves. The first outer-level bag contains one inner-level bag problem with two inner-level instances, the second outer-level bag contains two inner-level bags with total of three inner-level instances, and finally the third outer-level bag contains two inner bags with four instances:","category":"page"},{"location":"manual/more_on_nodes/","page":"More on nodes","title":"More on nodes","text":"ds = BagNode(BagNode(ArrayNode(randn(4, 10)),\n                     [1:2, 3:4, 5:5, 6:7, 8:10]),\n             [1:1, 2:3, 4:5])","category":"page"},{"location":"manual/more_on_nodes/","page":"More on nodes","title":"More on nodes","text":"Here is one example of a model, which is appropriate for this hierarchy:","category":"page"},{"location":"manual/more_on_nodes/","page":"More on nodes","title":"More on nodes","text":"using Flux: Dense, Chain, relu\nm = BagModel(\n        BagModel(\n            ArrayModel(Dense(4, 3, relu)),   \n            SegmentedMeanMax(3),\n            ArrayModel(Dense(7, 3, relu))),\n        SegmentedMeanMax(3),\n        ArrayModel(Chain(Dense(7, 3, relu), Dense(3, 2))))","category":"page"},{"location":"manual/more_on_nodes/","page":"More on nodes","title":"More on nodes","text":"and can be directly applied to obtain a result:","category":"page"},{"location":"manual/more_on_nodes/","page":"More on nodes","title":"More on nodes","text":"m(ds)","category":"page"},{"location":"manual/more_on_nodes/","page":"More on nodes","title":"More on nodes","text":"Here we again make use of the property that even if each instance is represented with an arbitrarily complex structure, we always obtain a vector representation after applying instance model im, regardless of the complexity of im and ds.data:","category":"page"},{"location":"manual/more_on_nodes/","page":"More on nodes","title":"More on nodes","text":"m.im(ds.data)","category":"page"},{"location":"manual/more_on_nodes/","page":"More on nodes","title":"More on nodes","text":"In one final example we demonstrate a complex model consisting of all types of nodes introduced so far:","category":"page"},{"location":"manual/more_on_nodes/","page":"More on nodes","title":"More on nodes","text":"ds = BagNode(ProductNode((BagNode(ArrayNode(randn(4, 10)),\n                                  [1:2, 3:4, 5:5, 6:7, 8:10]),\n                          ArrayNode(randn(3, 5)),\n                          BagNode(BagNode(ArrayNode(randn(2, 30)),\n                                          [i:i+1 for i in 1:2:30]),\n                                  [1:3, 4:6, 7:9, 10:12, 13:15]),\n                          ArrayNode(randn(2, 5)))),\n             [1:1, 2:3, 4:5])","category":"page"},{"location":"manual/more_on_nodes/","page":"More on nodes","title":"More on nodes","text":"Instead of defining a model manually, we make use of Model Reflection, another Mill.jl functionality, which simplifies model creation:","category":"page"},{"location":"manual/more_on_nodes/","page":"More on nodes","title":"More on nodes","text":"m = reflectinmodel(ds)\nm(ds)","category":"page"},{"location":"manual/more_on_nodes/#Node-conveniences","page":"More on nodes","title":"Node conveniences","text":"","category":"section"},{"location":"manual/more_on_nodes/","page":"More on nodes","title":"More on nodes","text":"To make the handling of data and model hierarchies easier, Mill.jl provides several tools. Let's setup some data:","category":"page"},{"location":"manual/more_on_nodes/","page":"More on nodes","title":"More on nodes","text":"AN = ArrayNode(Float32.([1 2 3 4; 5 6 7 8]))\nAM = reflectinmodel(AN)\nBN = BagNode(AN, [1:1, 2:3, 4:4])\nBM = reflectinmodel(BN)\nPN = ProductNode((a=ArrayNode(Float32.([1 2 3; 4 5 6])), b=BN))\nPM = reflectinmodel(PN)","category":"page"},{"location":"manual/more_on_nodes/#nobs","page":"More on nodes","title":"nobs","text":"","category":"section"},{"location":"manual/more_on_nodes/","page":"More on nodes","title":"More on nodes","text":"nobs method from StatsBase.jl returns a number of samples from the current level point of view. This number usually increases as we go down the tree when BagNodes are involved, as each bag may contain more than one instance.","category":"page"},{"location":"manual/more_on_nodes/","page":"More on nodes","title":"More on nodes","text":"using StatsBase: nobs\nnobs(AN)\nnobs(BN)\nnobs(PN)","category":"page"},{"location":"manual/more_on_nodes/#Indexing-and-Slicing","page":"More on nodes","title":"Indexing and Slicing","text":"","category":"section"},{"location":"manual/more_on_nodes/","page":"More on nodes","title":"More on nodes","text":"Indexing in Mill.jl operates on the level of observations:","category":"page"},{"location":"manual/more_on_nodes/","page":"More on nodes","title":"More on nodes","text":"AN[1]\nnobs(ans)\nBN[2]\nnobs(ans)\nPN[3]\nnobs(ans)\nAN[[1, 4]]\nnobs(ans)\nBN[1:2]\nnobs(ans)\nPN[[2, 3]]\nnobs(ans)\nPN[Int[]]\nnobs(ans)","category":"page"},{"location":"manual/more_on_nodes/","page":"More on nodes","title":"More on nodes","text":"This may be useful for creating minibatches and their permutations.","category":"page"},{"location":"manual/more_on_nodes/","page":"More on nodes","title":"More on nodes","text":"Note that apart from the perhaps apparent recurrent effect, this operation requires other implicit actions, such as properly recomputing bag indices:","category":"page"},{"location":"manual/more_on_nodes/","page":"More on nodes","title":"More on nodes","text":"BN.bags\nBN[[1, 3]].bags","category":"page"},{"location":"manual/more_on_nodes/#Concatenation","page":"More on nodes","title":"Concatenation","text":"","category":"section"},{"location":"manual/more_on_nodes/","page":"More on nodes","title":"More on nodes","text":"catobs concatenates several datasets (trees) together:","category":"page"},{"location":"manual/more_on_nodes/","page":"More on nodes","title":"More on nodes","text":"catobs(AN[1], AN[4])\ncatobs(BN[3], BN[[2, 1]])\ncatobs(PN[[1, 2]], PN[3:4]) == PN","category":"page"},{"location":"manual/more_on_nodes/","page":"More on nodes","title":"More on nodes","text":"Again, the effect is recurrent and everything is appropriately recomputed:","category":"page"},{"location":"manual/more_on_nodes/","page":"More on nodes","title":"More on nodes","text":"BN.bags\ncatobs(BN[3], BN[[1]]).bags","category":"page"},{"location":"manual/more_on_nodes/","page":"More on nodes","title":"More on nodes","text":"ukn: More tips\nFor more tips for handling datasets and models, see External tools","category":"page"},{"location":"references/#References","page":"-","title":"References","text":"","category":"section"},{"location":"references/","page":"-","title":"-","text":"","category":"page"},{"location":"examples/graphs/#GNNs-with-Mill-in-16-lines","page":"-","title":"GNNs with Mill in 16 lines","text":"","category":"section"},{"location":"examples/graphs/","page":"-","title":"-","text":"As has been mentioned in [Simon], the multi-instance learning is a essential piece to implement message passing in inference over graphs (called spatial Graph Neural Networks). It is really simple to make the idea fly is really small.","category":"page"},{"location":"examples/graphs/","page":"-","title":"-","text":"Let's assume a graph g, in this case created by barabasi_albert function, and let's assume that each vertex is described by a feature matrix x","category":"page"},{"location":"examples/graphs/","page":"-","title":"-","text":"using LightGraphs, Mill, Flux\ng = barabasi_albert(10, 3, 2)\nx = ArrayNode(randn(Float32, 7, 10))","category":"page"},{"location":"examples/graphs/","page":"-","title":"-","text":"What we do, is that we use Mill.ScatteredBags from Mill.jl to encode the neighbors of each vertex. That means that each vertex will be described by a bag of its neighbors. This information is by convenience stored in fadjlist of a graph g, therefore the bags are constructed as","category":"page"},{"location":"examples/graphs/","page":"-","title":"-","text":"b = Mill.ScatteredBags(g.fadjlist)","category":"page"},{"location":"examples/graphs/","page":"-","title":"-","text":"Finally, we create two models. First pre-process the description of vertices to some latent dimension for message passing, we will call this lift, and then a network realizing the message passing, we will call this one mp","category":"page"},{"location":"examples/graphs/","page":"-","title":"-","text":"lift = reflectinmodel(x, d -> Dense(d, 10), d -> SegmentedMean(d))\nxx = lift(x)\nmp = reflectinmodel(BagNode(xx, b), d -> Dense(d, 10), d -> SegmentedMean(d))","category":"page"},{"location":"examples/graphs/","page":"-","title":"-","text":"Notice that BagNode(xx, b) now essentially encodes the features about vertices and also the adjacency matrix. This also means that one step of message passing algorithm can be realized as mp(BagNode(xx,b)) and it is differentiable, which can be verified by executing gradient(() -> sum(sin.(mp(BagNode(xx,b)).data)), Flux.params(mp)). ","category":"page"},{"location":"examples/graphs/","page":"-","title":"-","text":"So if we put everything together, the GNN implementation is following block of code (16 lines of mostly sugar).","category":"page"},{"location":"examples/graphs/","page":"-","title":"-","text":"using Flux, Mill, LightGraphs, Statistics\n\nstruct GNN{L,M, R}\n\tlift::L\n\tmp::M\n\tm::R\nend\n\nFlux.@functor GNN\n\nfunction mpstep(m::GNN, xx::ArrayNode, bags, n)\n\tn == 0 && return(xx)\n\tmpstep(m, m.mp(BagNode(xx, bags)), bags, n - 1)\nend\n\nfunction (m::GNN)(g, x, n)\n\txx = m.lift(x)\n\tbags = Mill.ScatteredBags(g.fadjlist)\n\to = mpstep(m, xx, bags, n)\n\tm.m(vcat(mean(o.data, dims = 2), maximum(o.data, dims = 2)))\nend","category":"page"},{"location":"examples/graphs/","page":"-","title":"-","text":"The initialization of the model is little tedious, but defining two helper functions for creating feed-forward neural networks ffnn and aggregation agg helps a bit. On the end, the graph neural network is properly integrated with Flux ecosystem and suports automatic differentiation.","category":"page"},{"location":"examples/graphs/","page":"-","title":"-","text":"zdim = 10\nrounds = 5\n\nffnn(d) = Chain(Dense(d, zdim, relu), Dense(zdim, zdim))\nagg(d) = SegmentedMeanMax(d)\n\ng = barabasi_albert(10, 3, 2)\nx = ArrayNode(randn(Float32, 7, 10))\ngnn = GNN(reflectinmodel(x, ffnn, agg),\n\tBagModel(ffnn(zdim), agg(zdim), ffnn(2zdim)),\n\tffnn(2zdim)\n\t)\n\ngnn(g, x, rounds)\ngradient(() -> sum(sin.(gnn(g, x, rounds))), Flux.params(gnn))","category":"page"},{"location":"examples/graphs/","page":"-","title":"-","text":"The above implementation is surprisingly general, as it supports a rich description of vertices, by which we mean that the description can be anything expressible by Mill (full JSONs). The missing piece is putting weights on edges, which would be a bit more complicated.","category":"page"},{"location":"manual/strings/","page":"-","title":"-","text":"#NGRAMs","category":"page"},{"location":"tools/hierarchical/","page":"HierarchicalUtils.jl","title":"HierarchicalUtils.jl","text":"using Mill\nusing StatsBase: nobs","category":"page"},{"location":"tools/hierarchical/#HierarchicalUtils.jl","page":"HierarchicalUtils.jl","title":"HierarchicalUtils.jl","text":"","category":"section"},{"location":"tools/hierarchical/","page":"HierarchicalUtils.jl","title":"HierarchicalUtils.jl","text":"Mill.jl uses HierarchicalUtils.jl which brings a lot of additional features.","category":"page"},{"location":"tools/hierarchical/","page":"HierarchicalUtils.jl","title":"HierarchicalUtils.jl","text":"using HierarchicalUtils","category":"page"},{"location":"tools/hierarchical/#Printing","page":"HierarchicalUtils.jl","title":"Printing","text":"","category":"section"},{"location":"tools/hierarchical/","page":"HierarchicalUtils.jl","title":"HierarchicalUtils.jl","text":"For instance, Base.show with text/plain MIME calls HierarchicalUtils.printtree:","category":"page"},{"location":"tools/hierarchical/","page":"HierarchicalUtils.jl","title":"HierarchicalUtils.jl","text":"ds = BagNode(ProductNode((BagNode(ArrayNode(randn(4, 10)),\n                                  [1:2, 3:4, 5:5, 6:7, 8:10]),\n                          ArrayNode(randn(3, 5)),\n                          BagNode(BagNode(ArrayNode(randn(2, 30)),\n                                          [i:i+1 for i in 1:2:30]),\n                                  [1:3, 4:6, 7:9, 10:12, 13:15]),\n                          ArrayNode(randn(2, 5)))),\n             [1:1, 2:3, 4:5])\nprinttree(ds; htrunc=3)","category":"page"},{"location":"tools/hierarchical/","page":"HierarchicalUtils.jl","title":"HierarchicalUtils.jl","text":"This can be used to print a non-truncated version of a model:","category":"page"},{"location":"tools/hierarchical/","page":"HierarchicalUtils.jl","title":"HierarchicalUtils.jl","text":"printtree(ds)","category":"page"},{"location":"tools/hierarchical/#Traversal-encoding","page":"HierarchicalUtils.jl","title":"Traversal encoding","text":"","category":"section"},{"location":"tools/hierarchical/","page":"HierarchicalUtils.jl","title":"HierarchicalUtils.jl","text":"Callling with trav=true enables convenient traversal functionality with string indexing:","category":"page"},{"location":"tools/hierarchical/","page":"HierarchicalUtils.jl","title":"HierarchicalUtils.jl","text":"m = reflectinmodel(ds)\nprinttree(m; trav=true)","category":"page"},{"location":"tools/hierarchical/","page":"HierarchicalUtils.jl","title":"HierarchicalUtils.jl","text":"This way any node in the model tree is swiftly accessible, which may come in handy when inspecting model parameters or simply deleting/replacing/inserting nodes to tree (for instance when constructing adversarial samples). All tree nodes are accessible by indexing with the traversal code:.","category":"page"},{"location":"tools/hierarchical/","page":"HierarchicalUtils.jl","title":"HierarchicalUtils.jl","text":"m[\"Y\"]","category":"page"},{"location":"tools/hierarchical/","page":"HierarchicalUtils.jl","title":"HierarchicalUtils.jl","text":"The following two approaches give the same result:","category":"page"},{"location":"tools/hierarchical/","page":"HierarchicalUtils.jl","title":"HierarchicalUtils.jl","text":"m[\"Y\"] === m.im.ms[1]","category":"page"},{"location":"tools/hierarchical/#Counting-functions","page":"HierarchicalUtils.jl","title":"Counting functions","text":"","category":"section"},{"location":"tools/hierarchical/","page":"HierarchicalUtils.jl","title":"HierarchicalUtils.jl","text":"Other functions provided by HierarchicalUtils.jl:","category":"page"},{"location":"tools/hierarchical/","page":"HierarchicalUtils.jl","title":"HierarchicalUtils.jl","text":"nnodes(ds)\nnleafs(ds)\nNodeIterator(ds) |> collect\nNodeIterator(ds, m) |> collect\nLeafIterator(ds) |> collect\nTypeIterator(BagModel, m) |> collect\nPredicateIterator(x -> nobs(x) ≥ 10, ds) |> collect","category":"page"},{"location":"tools/hierarchical/","page":"HierarchicalUtils.jl","title":"HierarchicalUtils.jl","text":"For the complete showcase of possibilites, refer to HierarchicalUtils.jl and this notebook","category":"page"},{"location":"home/#Home","page":"Home","title":"Home","text":"","category":"section"},{"location":"home/","page":"Home","title":"Home","text":"Mill.jl is a library built on top of Flux.jl aimed to flexibly prototype hierarchical multi-instance learning models as described in Tomáš Pevný , Petr Somol  (2017) and  Tomáš Pevný , Petr Somol  (2016).","category":"page"},{"location":"home/","page":"Home","title":"Home","text":"Go to","category":"page"},{"location":"home/","page":"Home","title":"Home","text":"Motivation for a brief introduction into the philosophy of Mill.jl\nArchitecture of Mill \nExamples\nHelper tools\nReferences\nTODO finish this","category":"page"},{"location":"home/","page":"Home","title":"Home","text":"Modules = [Mill]\nOrder   = [:function, :type]","category":"page"},{"location":"home/#Mill._addmattvec!-Tuple{Array{T,2} where T,Any,Array{T,2} where T,AbstractArray{T,2} where T,Any}","page":"Home","title":"Mill._addmattvec!","text":"_addmattvec!(o, i, W, x, j)\n\nadd a product of a transposed matrix `W` with a j-th column of `x` to i-th columns of `o`\n\n\n\n\n\n","category":"method"},{"location":"home/#Mill._addmatvec!-Tuple{Array{T,2} where T,Any,Array{T,2} where T,Array{T,2} where T,Any}","page":"Home","title":"Mill._addmatvec!","text":"_addmatvec!(o, i, W, x, j)\n\nadd a product of matrix `W` with a j-th column of `x` to i-th columns of `o`\n\n\n\n\n\n","category":"method"},{"location":"home/#Mill._addvecvect!-Tuple{Array{T,2} where T,Array{T,2} where T,Any,AbstractArray{T,2} where T,Any}","page":"Home","title":"Mill._addvecvect!","text":"_outeradd!(W, Δ, i, x, j)\n\nadd an outer product of i-th column of `Δ` and transposed `j`-th columns of `x` to `W`\n\n\n\n\n\n","category":"method"},{"location":"home/#Mill.catobs","page":"Home","title":"Mill.catobs","text":"catobs(xs...)\n\nconcatenates all observations from all xs together\n\n\n\n\n\n","category":"function"},{"location":"home/#Mill.catobs-Tuple","page":"Home","title":"Mill.catobs","text":"catobs(as...)\n\nconcatenates `as...` into a single datanode while preserving their structure\n\n\n\n\n\n","category":"method"},{"location":"home/#Mill.countngrams!-Tuple{Any,Union{AbstractString, Base.CodeUnits, AbstractArray{var\"#s49\",1} where var\"#s49\"<:Integer},Int64,Int64}","page":"Home","title":"Mill.countngrams!","text":"function countngrams!(o,x,n::Int,b::Int)\n\ncounts number of of n grams of x with base b to o and store it to o\n\n\n\n\n\n","category":"method"},{"location":"home/#Mill.countngrams-Tuple{Any,Int64,Int64,Any}","page":"Home","title":"Mill.countngrams","text":"function countngrams(x,n::Int,b::Int)\n\ncounts number of of n grams of x with base b to o\n\n\n\n\n\n","category":"method"},{"location":"home/#Mill.data-Tuple{AbstractNode}","page":"Home","title":"Mill.data","text":"data(x::AbstractNode)\n\nreturn data hold by the datanode\n\n\n\n\n\n","category":"method"},{"location":"home/#Mill.getrange-Tuple{Any}","page":"Home","title":"Mill.getrange","text":"\tgetrange(n)\n\n\treturns block of indices for a particular thred\n\n\n\n\n\n","category":"method"},{"location":"home/#Mill.ngrams!-Tuple{Any,Union{AbstractString, Base.CodeUnits, AbstractArray{var\"#s49\",1} where var\"#s49\"<:Integer},Vararg{Any,N} where N}","page":"Home","title":"Mill.ngrams!","text":"ngrams!(o,x,n::Int,b::Int)\n\nstore indexes of n grams of x with base b to o\n\n\n\n\n\n","category":"method"},{"location":"home/#Mill.ngrams-Tuple{Union{AbstractString, Base.CodeUnits, AbstractArray{var\"#s49\",1} where var\"#s49\"<:Integer},Vararg{Any,N} where N}","page":"Home","title":"Mill.ngrams","text":"ngrams(x,n::Int,b::Int)\n\nindexes of n grams of x with base b\n\n\n\n\n\n","category":"method"},{"location":"home/#Mill.remapbag-Tuple{AlignedBags,AbstractArray{Int64,1}}","page":"Home","title":"Mill.remapbag","text":"function remapbag(b::Bags,idcs::Vector{Int})\n\nbags corresponding to indices with collected indices\n\n\n\n\n\n","category":"method"},{"location":"home/#Mill.sparsify-Tuple{Any,Any}","page":"Home","title":"Mill.sparsify","text":"\tsparsify(x,nnzrate)\n\n\treplace matrices with at most `nnzrate` fraction of non-zeros with SparseMatrixCSC\n\njulia> x = ProductNode((\n\t\t\t\tProductNode((\n\t\t\t\t\tMatrixNode(randn(5,5)),\n\t\t\t\t\tMatrixNode(zeros(5,5))\n\t\t\t\t\t\t)),\n\t\t\t\tMatrixNode(zeros(5,5))\n\t\t\t\t))\njulia> mapdata(i -> sparsify(i,0.05),x)\n\n\n\n\n\n\n","category":"method"},{"location":"home/#Mill.ArrayModel","page":"Home","title":"Mill.ArrayModel","text":"struct ArrayModel{T <: MillFunction} <: AbstractMillModel m::T end\n\nuse a Chain, Dense, or any other function on an ArrayNode\n\n\n\n\n\n","category":"type"},{"location":"home/#Mill.ArrayNode","page":"Home","title":"Mill.ArrayNode","text":"\n\n\n\n","category":"type"},{"location":"home/#Mill.BagChain","page":"Home","title":"Mill.BagChain","text":"BagChain(layers...) BagChain multiple layers / functions together, so that they are called in sequence on a given input supported by bags.\n\n\n\n\n\n","category":"type"},{"location":"home/#Mill.BagConv","page":"Home","title":"Mill.BagConv","text":"struct BagConv{T, F}\n\tW::T\n\tσ::F\nend\n\nConvolution over a matrix `X` correctly handing borders between bags. The convolution is little bit special, as it \nassumes that input is always a matrix (never a tensor) and the kernel always spans the full dimension of the vector.\n\nBagConv(d::Int, o::Int, n::Int, σ = identity)\n`d` --- input dimension\n`o` --- output dimension (number of channels)\n`n` --- size of convolution\n`σ` --- transfer function used after the convolution\n\nnote that of `n` equals one, then the convolution boils down to multiplication of input data `x` with a matrix `W`\n\n\n\n\n\n","category":"type"},{"location":"home/#Mill.BagModel","page":"Home","title":"Mill.BagModel","text":"struct BagModel{T <: AbstractMillModel, U <: AbstractMillModel} <: AbstractMillModel im::T a::Aggregation bm::U end\n\nuse a im model on data in BagNode, the uses function a to aggregate individual bags, and finally it uses bm model on the output\n\n\n\n\n\n","category":"type"},{"location":"home/#Mill.NGramIterator","page":"Home","title":"Mill.NGramIterator","text":"struct NGramIterator{T} s::T n::Int b::Int end\n\nIterates and enumerates ngrams of collection of integers s::T with zero padding. Enumeration is computed as in positional number systems, where items of s are digits and b is the base.\n\nIn order to reduce collisions when mixing ngrams of different order one should avoid zeros and negative integers in s and should set base b to be equal to the expected number of unique tokkens in s.\n\nExamples\n\njulia> it = Mill.NGramIterator(collect(1:9), 3, 10)\nNGramIterator{Array{Int64,1}}([1, 2, 3, 4, 5, 6, 7, 8, 9], 3, 10, 9223372036854775807)\n\njulia> Mill.string_start_code!(0); Mill.string_end_code!(0); collect(it)\n11-element Array{Int64,1}:\n   1\n  12\n 123\n 234\n 345\n 456\n 567\n 678\n 789\n 890\n 900\n\njulia> sit = Mill.NGramIterator(codeunits(\"deadbeef\"), 3, 256)    # creates collisions as codeunits returns tokens from 0x00:0xff\nNGramIterator{Base.CodeUnits{UInt8,String}}(UInt8[0x64, 0x65, 0x61, 0x64, 0x62, 0x65, 0x65, 0x66], 3, 256, 9223372036854775807)\n\njulia> collect(sit)\n10-element Array{Int64,1}:\n     100\n   25701\n 6579553\n 6644068\n 6382690\n 6578789\n 6448485\n 6645094\n 6645248\n 6684672\n\n\n\n\n\n","category":"type"},{"location":"home/#Mill.NGramMatrix","page":"Home","title":"Mill.NGramMatrix","text":"struct NGramMatrix{T} s::Vector{T} n::Int b::Int m::Int end\n\nRepresents strings stored in array s as ngrams of cardinality n. Strings are internally stored as strings and the multiplication with dense matrix is overloaded and b is a base for calculation of trigrams. Finally m is the modulo applied on indexes of ngrams.\n\nThe structure essentially represents module one-hot representation of strings, where each columns contains one observation (string). Therefore the structure can be viewed as a matrix with m rows and length(s) columns\n\n\n\n\n\n","category":"type"},{"location":"home/#Mill.ProductModel","page":"Home","title":"Mill.ProductModel","text":"struct ProductModel{N, T <: MillFunction} <: AbstractMillModel\n    ms::NTuple{N, AbstractMillModel}\n    m::ArrayModel{T}\nend\n\nuses each model in `ms` on each data in `ProductNode`, concatenate the output and pass it to the chainmodel `m`\n\n\n\n\n\n","category":"type"},{"location":"manual/custom/","page":"-","title":"-","text":"using Mill\nusing Flux","category":"page"},{"location":"manual/custom/#Adding-custom-nodes","page":"-","title":"Adding custom nodes","text":"","category":"section"},{"location":"manual/custom/","page":"-","title":"-","text":"Mill.jl data nodes are lightweight wrappers around data, such as Array, DataFrame, and others. When implementing custom nodes, it is recommended to equip them with the following functionality to fit better into Mill.jl environment:","category":"page"},{"location":"manual/custom/","page":"-","title":"-","text":"allow nesting (if needed)\nimplement getindex to obtain subsets of observations. For this purpose, Mill.jl defines a subset function for common datatypes, which can be used.\nallow concatenation of nodes with catobs. Optionally, implement reduce(catobs, ...) as well to avoid excessive compilations if a number of arguments will vary a lot\ndefine a specialized method for nobs\nregister the custom node with HierarchicalUtils.jl to obtain pretty printing, iterators and other functionality","category":"page"},{"location":"manual/custom/#Unix-path-example","page":"-","title":"Unix path example","text":"","category":"section"},{"location":"manual/custom/","page":"-","title":"-","text":"Let's define one custom node type for representing pathnames in Unix and one custom model type for processing it. We'll start by defining the structure holding pathnames:","category":"page"},{"location":"manual/custom/","page":"-","title":"-","text":"struct PathNode{S <: AbstractString, C} <: AbstractNode\n    data::Vector{S}\n    metadata::C\nend\n\nPathNode(data::Vector{S}) where {S <: AbstractString} = PathNode(data, nothing)","category":"page"},{"location":"manual/custom/","page":"-","title":"-","text":"struct PathNode{S <: AbstractString, C} <: AbstractNode\n    data::Vector{S}\n    metadata::C\nend\n\nPathNode(data::Vector{S}) where {S <: AbstractString} = PathNode(data, nothing)","category":"page"},{"location":"manual/custom/","page":"-","title":"-","text":"We will support nobs:","category":"page"},{"location":"manual/custom/","page":"-","title":"-","text":"import StatsBase: nobs\nBase.ndims(x::PathNode) = Colon()\nnobs(a::PathNode) = length(a.data)","category":"page"},{"location":"manual/custom/","page":"-","title":"-","text":"import StatsBase: nobs\nBase.ndims(x::PathNode) = Colon()\nnobs(a::PathNode) = length(a.data)","category":"page"},{"location":"manual/custom/","page":"-","title":"-","text":"concatenation:","category":"page"},{"location":"manual/custom/","page":"-","title":"-","text":"function Base.reduce(::typeof(catobs), as::Vector{T}) where {T <: PathNode}\n    data = reduce(vcat, [x.data for x in as])\n    metadata = reduce(catobs, [a.metadata for a in as])\n    PathNode(data, metadata)\nend","category":"page"},{"location":"manual/custom/","page":"-","title":"-","text":"and indexing:","category":"page"},{"location":"manual/custom/","page":"-","title":"-","text":"Base.getindex(x::PathNode, i::Mill.VecOrRange{<:Int}) = PathNode(subset(x.data, i), subset(x.metadata, i))","category":"page"},{"location":"manual/custom/","page":"-","title":"-","text":"The last touch is to add the definition needed by HierarchicalUtils.jl:","category":"page"},{"location":"manual/custom/","page":"-","title":"-","text":"import HierarchicalUtils\nHierarchicalUtils.NodeType(::Type{<:PathNode}) = HierarchicalUtils.LeafNode()\nHierarchicalUtils.noderepr(n::PathNode) = \"PathNode ($(nobs(n)) obs)\"","category":"page"},{"location":"manual/custom/","page":"-","title":"-","text":"import HierarchicalUtils\nHierarchicalUtils.NodeType(::Type{<:PathNode}) = HierarchicalUtils.LeafNode()\nHierarchicalUtils.noderepr(n::PathNode) = \"PathNode ($(nobs(n)) obs)\"","category":"page"},{"location":"manual/custom/","page":"-","title":"-","text":"Now, we are ready to create the first PathNode:","category":"page"},{"location":"manual/custom/","page":"-","title":"-","text":"ds = PathNode([\"/etc/passwd\", \"/home/tonda/.bashrc\"])","category":"page"},{"location":"manual/custom/","page":"-","title":"-","text":"Similarly, we define a ModelNode type which will be a counterpart processing the data:","category":"page"},{"location":"manual/custom/","page":"-","title":"-","text":"struct PathModel{T, F} <: AbstractMillModel\n    m::T\n    path2mill::F\nend\n\nFlux.@functor PathModel","category":"page"},{"location":"manual/custom/","page":"-","title":"-","text":"Note that the part of the ModelNode is a function which converts the pathname string to a Mill.jl structure. For simplicity, we use a trivial NGramMatrix representation in this example and define path2mill as follows:","category":"page"},{"location":"manual/custom/","page":"-","title":"-","text":"function path2mill(s::String)\n    ss = String.(split(s, \"/\"))\n    BagNode(ArrayNode(Mill.NGramMatrix(ss, 3)), AlignedBags([1:length(ss)]))\nend\n\npath2mill(ss::Vector{S}) where {S <: AbstractString} = reduce(catobs, map(path2mill, ss))\npath2mill(ds::PathNode) = path2mill(ds.data)","category":"page"},{"location":"manual/custom/","page":"-","title":"-","text":"function path2mill(s::String)\n    ss = String.(split(s, \"/\"))\n    BagNode(ArrayNode(Mill.NGramMatrix(ss, 3)), AlignedBags([1:length(ss)]))\nend\n\npath2mill(ss::Vector{S}) where {S <: AbstractString} = reduce(catobs, map(path2mill, ss))\npath2mill(ds::PathNode) = path2mill(ds.data)","category":"page"},{"location":"manual/custom/","page":"-","title":"-","text":"Now we define how the model node is applied:","category":"page"},{"location":"manual/custom/","page":"-","title":"-","text":"(m::PathModel)(x::PathNode)  = m.m(m.path2mill(x))","category":"page"},{"location":"manual/custom/","page":"-","title":"-","text":"And again, define everything needed in HierarchicalUtils.jl:","category":"page"},{"location":"manual/custom/","page":"-","title":"-","text":"HierarchicalUtils.NodeType(::Type{<:PathModel}) = HierarchicalUtils.LeafNode()\nHierarchicalUtils.noderepr(n::PathModel) = \"PathModel\"","category":"page"},{"location":"manual/custom/","page":"-","title":"-","text":"Let's test that everything works:","category":"page"},{"location":"manual/custom/","page":"-","title":"-","text":"pm = PathModel(reflectinmodel(path2mill(ds)), path2mill)\npm(ds).data","category":"page"},{"location":"manual/custom/","page":"-","title":"-","text":"The final touch would be to overload the reflectinmodel as","category":"page"},{"location":"manual/custom/","page":"-","title":"-","text":"function Mill.reflectinmodel(ds::PathNode, args...)\n    pm = reflectinmodel(path2mill(ds), args...)\n    PathModel(pm, path2mill)\nend","category":"page"},{"location":"manual/custom/","page":"-","title":"-","text":"which makes things even easier","category":"page"},{"location":"manual/custom/","page":"-","title":"-","text":"pm = reflectinmodel(ds)\npm(ds).data","category":"page"},{"location":"motivation/#Motivation","page":"Motivation","title":"Motivation","text":"","category":"section"},{"location":"motivation/","page":"Motivation","title":"Motivation","text":"In this section, we provide a short introduction into (hierarchical) multi instance learning. A much more detailed overview of this subject can be found in Šimon Mandlík  (2020).","category":"page"},{"location":"motivation/#What-is-a-Multiple-instance-learning-problem?","page":"Motivation","title":"What is a Multiple instance learning problem?","text":"","category":"section"},{"location":"motivation/","page":"Motivation","title":"Motivation","text":"In Multiple Instance Learning (MIL), also Multi-Instance Learning, the sample bmx is a set of vectors (or matrices) x_1ldotsx_l, where x_i in mathbbR^d. As a result, order does not matter, which makes MIL problems different from sequences. In MIL parlance, sample bmx is also called a bag and its elements x_1 ldots x_2 instances. MIL problems have been introduced in Thomas G. Dietterich , Richard H. Lathrop , Tomás Lozano-Pérez  (1997), and extended and generalized in a series of works Tomáš Pevný , Petr Somol  (2017), Tomáš Pevný , Petr Somol  (2016), Tomáš Pevný , Vojtěch Kovařík  (2019). The most comprehensive introduction known to authors is Šimon Mandlík  (2020).","category":"page"},{"location":"motivation/","page":"Motivation","title":"Motivation","text":"Why are MIL problems relevant? Since the seminal paper from Ronald A Fisher  (1936), the majority of machine learning problems deals with problems like the one shown below:[1]","category":"page"},{"location":"motivation/","page":"Motivation","title":"Motivation","text":"[1]: Iris flower data set","category":"page"},{"location":"motivation/","page":"Motivation","title":"Motivation","text":"(Image: )","category":"page"},{"location":"motivation/","page":"Motivation","title":"Motivation","text":"where the input sample bmx is a vector (or generally speaking any tensor) of a fixed dimension containing various measurements of the specimen.","category":"page"},{"location":"motivation/","page":"Motivation","title":"Motivation","text":"Most of the time, a skilled botanist is able to identify a specimen not by making use of any measuring device, but by visual or tactile inspection of its stem, leaves and blooms. For different species, different parts of the flower may need to be examined for indicators. At the same time, many species may have nearly identical-looking leaves or blooms, therefore, one needs to step back, consider the whole picture, and appropriately combine lower-level observations into high-level conclusions about the given specimen.","category":"page"},{"location":"motivation/","page":"Motivation","title":"Motivation","text":"If we want to use such more elaborate description of the Iris flower using fixed size structures, we will have a hard time, because every specimen can have a different amounts of leaves or blooms (or they may be completely missing). This means that to use the usual fixed dimension paradigm, we have to either somehow select a single leaf (blossom) and extract features from them, or design procedures for aggregating such features over whole sets, so that the output has fixed dimension. This is clearly undesirable. Mill.jl a framework that seamlessly deals with these challenges in data representation.","category":"page"},{"location":"motivation/#Hierarchical-Multiple-Instance-Learning","page":"Motivation","title":"Hierarchical Multiple Instance Learning","text":"","category":"section"},{"location":"motivation/","page":"Motivation","title":"Motivation","text":"In Hierarchical Multiple Instance Learning (HMIL) the input may consists of not only sets, but also sets of sets and Cartesian Products of these structures. Returning to the previous Iris flower example, a specimen can be represented like this for HMIL:","category":"page"},{"location":"motivation/","page":"Motivation","title":"Motivation","text":"(Image: )","category":"page"},{"location":"motivation/","page":"Motivation","title":"Motivation","text":"The only stem is represented by vector bmx_s encoding its distinctive properties such as shape, color, structure or texture. Next, we inspect all blooms. Each of the blooms may have distinctive discriminative signs, therefore, we describe all three in vectors bmx_b_1 bmx_b_2 bmx_b_3, one vector for each bloom, and group them to a set. Finally, bmx_u represents the only flower which has not blossomed. Likewise, we could describe all leaves of the specimen if any were present. Here we assume that each specimen of the considered species has only one stem, but may have multiple flowers or leaves. Hence, all blooms and buds are represented as unordered sets of vectors as opposed to stem representation, which consists of only one vector.","category":"page"},{"location":"motivation/","page":"Motivation","title":"Motivation","text":"How does MIL models cope with variability in numbers of flowers and leaves? Each MIL model consists of two feed-forward neural networks with an element-wise aggregation operator like mean (or maximum) sandwiched between them. Denoting those feed-forward networks (FFNs) as f_1 and f_2, the output of the model applied to a bag is calculated for example as f_2 left(frac1lsum_i=1^l f_1(x_i) right) if we use mean as an aggregation function.","category":"page"},{"location":"motivation/","page":"Motivation","title":"Motivation","text":"The HMIL model corresponding to the Iris example above would comprise two FFNs and an aggregation to convert set of leafs to a single vector, and another two FFNs and an aggregation to convert set of blossoms to a single vector. These two outputs would be concatenated with a description of a stem, which would be fed to yet another FFN providing the final output. Since the whole scheme is differentiable, we can compute gradients and use any available gradient-based method to optimize the whole model at once using only labels on the level of output[2].","category":"page"},{"location":"motivation/","page":"Motivation","title":"Motivation","text":"[2]: Some methods for MIL problems require instance-level labels as well, which are not always available.","category":"page"},{"location":"motivation/","page":"Motivation","title":"Motivation","text":"The Mill.jl library simplifies implementation of machine learning problems using (H)MIL representation. In theory, it can represent any problem that can be represented in JSONs. That is why we have created a separate tool, JsonGrinder.jl, which helps with processing JSON documents for learning.","category":"page"},{"location":"motivation/","page":"Motivation","title":"Motivation","text":"In Tomáš Pevný , Vojtěch Kovařík  (2019), authors have further extended the Universal approximation theorem to MIL problems, their Cartesian products, and nested MIL problems, i.e. a case where instances of one bag are in fact bags again.","category":"page"},{"location":"motivation/#Relation-to-Graph-Neural-Networks","page":"Motivation","title":"Relation to Graph Neural Networks","text":"","category":"section"},{"location":"motivation/","page":"Motivation","title":"Motivation","text":"HMIL problems can be seen as a special subset of general graphs. They differ in two important ways:","category":"page"},{"location":"motivation/","page":"Motivation","title":"Motivation","text":"In general graphs, vertices are of a small number of semantic type, whereas in HMIL problems, the number of semantic types of vertices is much higher (it is helpful to think about HMIL problems as about those for which JSON is a natural representation).\nThe computational graph of HMIL is a tree, which introduces assumption that there exist an efficient inference. Contrary, in general graphs (with loops) there is no efficient inference and one has to resort to message passing (Loopy belief propagation).\nOne update message in loopy belief propagation can be viewed as a MIL problem, as it has to produce a vector based on infomation inthe neighborhood, which can contain arbitrary number of vertices.","category":"page"},{"location":"motivation/#Difference-to-sequences","page":"Motivation","title":"Difference to sequences","text":"","category":"section"},{"location":"motivation/","page":"Motivation","title":"Motivation","text":"The major difference is that instances in bag are not ordered in any way. This means that if a sequence (abc) should be treated as a set, then the output of a function f should be the same for any permutation, i.e. f(abc) = f(cba) = f(bac) = ldots. This property has a dramatic implication on the computational complexity. Sequences are typically modeled using Recurrent Neural Networks (RNNs), where the output is calculated as f(abc) = g(a g(b g(c))) (slightly abusing the notation). During optimization, a gradient of g needs to be calculated recursively, giving raise to infamous vanishing / exploding gradient problems. In constrast, (H)MIL models calculate the output as f(frac13(g(a) + g(b) + g(c))) (slightly abusing notation again), which means that the gradient of g can be calculated in parallel and not recurrently. ","category":"page"},{"location":"examples/dag/#Directed-Acyclic-Graphs-in-Mill.jl","page":"-","title":"Directed Acyclic Graphs in Mill.jl","text":"","category":"section"},{"location":"examples/dag/","page":"-","title":"-","text":"In this exercice, I was interesting in a following problem. Imagine a data / knowlege base represented in a form of a directed acyclic graph (DAG), where the vertex would be modelled on basis of its parents (and their parents), but not on its descendants. I was further interested in a case, where the descendants of some vertex i would represent only subset of all nodes in the graph. How to do this with a least pain and efficiently?","category":"page"},{"location":"examples/dag/","page":"-","title":"-","text":"In the course of calculating the value of a vertex i, there can be vertices which can be used more that one time. This means that we would like to have a cache of already calculated values, which is difficult since Zygote does not like setindex operation. But, the cache is assigned only once, which means that it can be realized through Zygote.buffer. So, here we go.","category":"page"},{"location":"examples/dag/","page":"-","title":"-","text":"At first, we initiate the cache as","category":"page"},{"location":"examples/dag/","page":"-","title":"-","text":"initcache(g, k) = [Zygote.Buffer(zeros(Float32, k, 1)) for _ in 1:nv(g)]","category":"page"},{"location":"examples/dag/","page":"-","title":"-","text":"To get the value of a vertex, we just delegate the question to cache as ","category":"page"},{"location":"examples/dag/","page":"-","title":"-","text":"function (model::DagModel)(g::DagGraph, i)\n  cache = initcache(g.g, model.odim)\n  ArrayNode(getfromcache!(cache, g, model, i))\nend","category":"page"},{"location":"examples/dag/","page":"-","title":"-","text":"which means that the getfromcache! will do all the heavy lifting. But that function will just check, if the value in cache has been already calculated, or it calculates the value (applying model on millvertex!) and freezes the calculated item in cache.","category":"page"},{"location":"examples/dag/","page":"-","title":"-","text":"function getfromcache!(cache, g::DagGraph, model::DagModel, i::Int)\n  cache[i].freeze && return(copy(cache[i]))\n  ds = millvertex!(cache, g, model, i)\n  cache[i][:] = model.m(ds).data\n  return(copy(cache[i]))\nend","category":"page"},{"location":"examples/dag/","page":"-","title":"-","text":"and what millvertex! function does? It just takes the representation of ancestors (from cache) and put them together","category":"page"},{"location":"examples/dag/","page":"-","title":"-","text":"function millvertex!(cache, g::DagGraph, model::DagModel, i)\n  ProductNode((neighbours = millneighbors!(cache, g, model, i), \n    vertex = vertex_features[i])\n  )\nend","category":"page"},{"location":"examples/dag/","page":"-","title":"-","text":"Wait a sec, am I running in circles? Yes, and that is the art of recursion. Below is the complete example I have once written. Note that it is not the most efficient approach to implement this. It would be better to spent a little time with graphs to identify sets of vertices that can be processed in parallel and for which all ancestors are know. But this was a fun little exercise.","category":"page"},{"location":"examples/dag/","page":"-","title":"-","text":"using Flux, Zygote\nusing LightGraphs, MetaGraphs, Mill, Setfield\n\n\nstruct DagGraph{G<:SimpleDiGraph,T}\n  g::G\n  vertex_features::T\nend\n\nZygote.@nograd LightGraphs.inneighbors\n\nstruct DagModel{M}\n  m::M\n  odim::Int\nend\n\nFlux.@functor DagModel\n\n\nfunction (model::DagModel)(g::DagGraph, i)\n  cache = initcache(g.g, model.odim)\n  ArrayNode(getfromcache!(cache, g, model, i))\nend\n\n(model::DagModel)(g::SimpleDiGraph, vertex_features, i) = model(DagGraph(g, vertex_features), i)\n\n\ninitcache(g, k) = [Zygote.Buffer(zeros(Float32, k, 1)) for _ in 1:nv(g)]\nZygote.@nograd initcache\n\nfunction millvertex!(cache, g::DagGraph, model::DagModel, i)\n  ProductNode((neighbours = millneighbors!(cache, g, model, i), \n    vertex = vertex_features[i])\n  )\nend\n\nfunction getfromcache!(cache, g::DagGraph, model::DagModel, ii::Vector{Int}) \n  reduce(catobs, [getfromcache!(cache, g, model, i) for i in ii])\nend\n\nfunction getfromcache!(cache, g::DagGraph, model::DagModel, i::Int)\n  cache[i].freeze && return(copy(cache[i]))\n  ds = millvertex!(cache, g, model, i)\n  cache[i][:] = model.m(ds).data\n  return(copy(cache[i]))\nend\n\nfunction millneighbors!(cache, g::DagGraph, model::DagModel, ii::Vector{Int})\n  isempty(ii) && return(BagNode(missing, [0:-1]))\n  xs = [getfromcache!(cache, g, model, i) for i in  ii]\n  BagNode(ArrayNode(reduce(catobs, xs)), [1:length(xs)])\nend\n\nmillneighbors!(cache, g::DagGraph, model::DagModel, i::Int) = millneighbors!(cache, g, model, inneighbors(g.g, i))","category":"page"},{"location":"manual/missing/#Missing-data","page":"Missing data","title":"Missing data","text":"","category":"section"},{"location":"manual/missing/","page":"Missing data","title":"Missing data","text":"With the latest version of Mill, it is also possible to work with missing data, replacing a missing bag with a default constant value, and even to learn this value as well. Everything is done automatically.","category":"page"},{"location":"manual/missing/","page":"Missing data","title":"Missing data","text":"At the moment, Mill.jl features an initial and naive approach to missing values. We assume that ArrayNode have missing values replaced by zeros, which is not optimal but in many situations it works reasonably well.","category":"page"},{"location":"manual/missing/","page":"Missing data","title":"Missing data","text":"BagNodes with missing features are indicated by Bags being set to [0:-1] with missing as a data and metadata. This can be seamlessly concatenated or sub-set, if the operation makes sense.","category":"page"},{"location":"manual/missing/","page":"Missing data","title":"Missing data","text":"Couple examples from unit tests. Let's define full and empty BagNode","category":"page"},{"location":"manual/missing/","page":"Missing data","title":"Missing data","text":"julia> a = BagNode(ArrayNode(rand(3,4)),[1:4], nothing)\nBagNode with 1 bag(s)\n  └── ArrayNode(3, 4)\n\njulia> e = BagNode(missing, AlignedBags([0:-1]), nothing)\nBagNode with 1 empty bag(s)","category":"page"},{"location":"manual/missing/","page":"Missing data","title":"Missing data","text":"We can concatenate them as follows.","category":"page"},{"location":"manual/missing/","page":"Missing data","title":"Missing data","text":"julia> x = reduce(catobs,[a, e])\nBagNode with 2 bag(s)\n  └── ArrayNode(3, 4)","category":"page"},{"location":"manual/missing/","page":"Missing data","title":"Missing data","text":"Notice, that the ArrayNode has still the same dimension as ArrayNode of just a. The missing second element, corresponding to e is indicated by the second bags being 0:-1 as follows:","category":"page"},{"location":"manual/missing/","page":"Missing data","title":"Missing data","text":"julia> x.bags\nAlignedBags(UnitRange{Int64}[1:4, 0:-1])","category":"page"},{"location":"manual/missing/","page":"Missing data","title":"Missing data","text":"We can get back the missing second element as","category":"page"},{"location":"manual/missing/","page":"Missing data","title":"Missing data","text":"julia> x[2]\nBagNode with 1 empty bag(s)","category":"page"},{"location":"manual/missing/","page":"Missing data","title":"Missing data","text":"During forward (and backward) pass, the missing values in BagNodes are filled in aggregation by zeros. ** In order this feature to work, the Aggregation needs to know dimension, therefore use MissingAggregation, which can handle this.** In the future, MissingAggregation will be made default.","category":"page"},{"location":"manual/missing/","page":"Missing data","title":"Missing data","text":"Last but not least, ProductNodes cannot handle missing values, as the missingness is propagated to its leaves, i.e.","category":"page"},{"location":"manual/missing/","page":"Missing data","title":"Missing data","text":"julia> ProductNode((a,e))\nProductNode{2}\n  ├── BagNode with 1 bag(s)\n  │     └── ArrayNode(3, 4)\n  └── BagNode with 1 empty bag(s)","category":"page"},{"location":"manual/missing/#Representing-missing-values","page":"Missing data","title":"Representing missing values","text":"","category":"section"},{"location":"manual/missing/","page":"Missing data","title":"Missing data","text":"The library currently support two ways to represent bags with missing values. First one represent missing data using missing as a = BagNode(missing, [0:-1]) while the second as an empty vector as a = BagNode(zero(4,0), [0:-1]).  While off the shelf the library supports both approaches transparently, the difference is mainly when one uses getindex, and therefore there is a switch Mill.emptyismissing(false), which is by default false. Let me demonstrate the difference.","category":"page"},{"location":"manual/missing/","page":"Missing data","title":"Missing data","text":"julia> a = BagNode(ArrayNode(rand(3,2)), [1:2, 0:-1])\nBagNode with 2 bag(s)\n  └── ArrayNode(3, 2)\n\njulia> Mill.emptyismissing(false);\n\njulia> a[2].data\nArrayNode(3, 0)\n\njulia> Mill.emptyismissing(true)\ntrue\n\njulia> a[2].data\nmissing","category":"page"},{"location":"manual/missing/","page":"Missing data","title":"Missing data","text":"The advantage of the first approach, default, is that types are always the same, which is nice to the compiler (and Zygote). The advantage of the latter is that it is more compact and nicer.","category":"page"},{"location":"examples/musk/musk/","page":"Musk dataset","title":"Musk dataset","text":"using Random; Random.seed!(42)\n\nusing Pkg\nold_path = Pkg.project().path\nPkg.activate(pwd())\nPkg.instantiate()","category":"page"},{"location":"examples/musk/musk/#Musk-dataset","page":"Musk dataset","title":"Musk dataset","text":"","category":"section"},{"location":"examples/musk/musk/","page":"Musk dataset","title":"Musk dataset","text":"Musk dataset is a classic MIL problem of the field, introduced in Thomas G. Dietterich , Richard H. Lathrop , Tomás Lozano-Pérez  (1997). Below we demonstrate how to solve this problem using Mill.jl. The full example is also accessible here, as well as a Julia environment to run it.","category":"page"},{"location":"examples/musk/musk/","page":"Musk dataset","title":"Musk dataset","text":"For the demo, we load all dependencies:","category":"page"},{"location":"examples/musk/musk/","page":"Musk dataset","title":"Musk dataset","text":"using FileIO, JLD2, Statistics, Mill, Flux\nusing Flux: throttle, @epochs\nusing Mill: reflectinmodel\nusing Base.Iterators: repeated","category":"page"},{"location":"examples/musk/musk/","page":"Musk dataset","title":"Musk dataset","text":"and then load the dataset and transform it into a Mill.jl structure. The musk.jld2 file contains:","category":"page"},{"location":"examples/musk/musk/","page":"Musk dataset","title":"Musk dataset","text":"a matrix with features fMat:","category":"page"},{"location":"examples/musk/musk/","page":"Musk dataset","title":"Musk dataset","text":"fMat = load(\"musk.jld2\", \"fMat\")          # matrix with instances, each column is one sample","category":"page"},{"location":"examples/musk/musk/","page":"Musk dataset","title":"Musk dataset","text":"the id of sample (bag in MIL terminology) specifying to which each instance (column in fMat) belongs to:","category":"page"},{"location":"examples/musk/musk/","page":"Musk dataset","title":"Musk dataset","text":"bagids = load(\"musk.jld2\", \"bagids\")      # ties instances to bags","category":"page"},{"location":"examples/musk/musk/","page":"Musk dataset","title":"Musk dataset","text":"The resulting BagNode is a structure which holds (i) feature matrix and (ii) ranges identifying which columns in the feature matrix each bag spans. This representation ensures that feed-forward networks do not need to deal with bag boundaries and always process full continuous matrices:","category":"page"},{"location":"examples/musk/musk/","page":"Musk dataset","title":"Musk dataset","text":"ds = BagNode(ArrayNode(fMat), bagids)     # create a BagNode dataset","category":"page"},{"location":"examples/musk/musk/","page":"Musk dataset","title":"Musk dataset","text":"the label of each instance in y.  The label of a bag is a maximum of labels of its instances, i.e. one positive instance in a bag makes it positive:","category":"page"},{"location":"examples/musk/musk/","page":"Musk dataset","title":"Musk dataset","text":"y = load(\"musk.jld2\", \"y\")                # load labels\ny = map(i -> maximum(y[i]) + 1, ds.bags)  # create labels on bags\ny_oh = Flux.onehotbatch(y, 1:2)           # one-hot encoding","category":"page"},{"location":"examples/musk/musk/","page":"Musk dataset","title":"Musk dataset","text":"Once the data are in Mill internal format, we will manually create a model. BagModel is designed to implement a basic multi-instance learning model utilizing two feed-forward networks with an aggregaton operator in between:","category":"page"},{"location":"examples/musk/musk/","page":"Musk dataset","title":"Musk dataset","text":"model = BagModel(\n    ArrayModel(Dense(166, 10, Flux.relu)),                      # model on the level of Flows\n    SegmentedMeanMax(10),                                       # aggregation\n    ArrayModel(Chain(Dense(21, 10, Flux.relu), Dense(10, 2))))  # model on the level of bags","category":"page"},{"location":"examples/musk/musk/","page":"Musk dataset","title":"Musk dataset","text":"Instances are first passed through a single layer with 10 neurons (input dimension is 166) with relu non-linearity, then we use mean and max aggregation functions simultaneously (for some problems, max is better then mean, therefore we use both), and then we use one layer with 10 neurons and relu nonlinearity followed by linear layer with 2 neurons (output dimension).","category":"page"},{"location":"examples/musk/musk/","page":"Musk dataset","title":"Musk dataset","text":"Let's check that forward pass works:","category":"page"},{"location":"examples/musk/musk/","page":"Musk dataset","title":"Musk dataset","text":"model(ds)","category":"page"},{"location":"examples/musk/musk/","page":"Musk dataset","title":"Musk dataset","text":"Since Mill is entirely compatible with Flux.jl, we can use its cross-entropy loss function:","category":"page"},{"location":"examples/musk/musk/","page":"Musk dataset","title":"Musk dataset","text":"loss(ds, y_oh) = Flux.logitcrossentropy(model(ds).data, y_oh)","category":"page"},{"location":"examples/musk/musk/","page":"Musk dataset","title":"Musk dataset","text":"and run simple training procedure using its tooling:","category":"page"},{"location":"examples/musk/musk/","page":"Musk dataset","title":"Musk dataset","text":"evalcb = () -> @show(loss(ds, y_oh))\nopt = Flux.ADAM()\n@epochs 10 Flux.train!(loss, params(model), repeated((ds, y_oh), 100), opt, cb=throttle(evalcb, 1))","category":"page"},{"location":"examples/musk/musk/","page":"Musk dataset","title":"Musk dataset","text":"We can also calculate training error, which should be not so surprisingly low:","category":"page"},{"location":"examples/musk/musk/","page":"Musk dataset","title":"Musk dataset","text":"mean(mapslices(argmax, model(ds).data, dims=1)' .!= y)","category":"page"},{"location":"examples/musk/musk/","page":"Musk dataset","title":"Musk dataset","text":"Pkg.activate(old_path)","category":"page"},{"location":"manual/nodes/","page":"Nodes","title":"Nodes","text":"using Mill","category":"page"},{"location":"manual/nodes/","page":"Nodes","title":"Nodes","text":"ukw: Tip\nIt is recommended to read the Motivation section first to understand the crucial ideas behind hierarchical multiple instance learning.","category":"page"},{"location":"manual/nodes/#Nodes","page":"Nodes","title":"Nodes","text":"","category":"section"},{"location":"manual/nodes/","page":"Nodes","title":"Nodes","text":"Mill.jl enables representation of arbitrarily complex tree-like hierarchies and appropriate models for these hierarchies. It defines two core abstract types:","category":"page"},{"location":"manual/nodes/","page":"Nodes","title":"Nodes","text":"AbstractNode which stores data on any level of abstraction and its subtypes can be further nested\nAbstractModelNode which helps to define a corresponding model. For each specific implementation of AbstractNode we have one or more specific AbstractModelNode(s) for processing it.","category":"page"},{"location":"manual/nodes/","page":"Nodes","title":"Nodes","text":"Below we will go through implementation of ArrayNode, BagNode and ProductNode together with their corresponding models. It is possible to define data and model nodes for more complex behaviors (see Custom Nodes), however, these three core types are already sufficient for a lot of tasks, for instance, representing any JSON document and using appropriate models to convert it to a vector represention or classify it (see TODO).","category":"page"},{"location":"manual/nodes/#ArrayNode-and-ArrayModel","page":"Nodes","title":"ArrayNode and ArrayModel","text":"","category":"section"},{"location":"manual/nodes/","page":"Nodes","title":"Nodes","text":"ArrayNode thinly wraps an array of features (specifically any subtype of AbstractArray):","category":"page"},{"location":"manual/nodes/","page":"Nodes","title":"Nodes","text":"X = Float32.([1 2 3 4; 5 6 7 8])\nAN = ArrayNode(X)","category":"page"},{"location":"manual/nodes/","page":"Nodes","title":"Nodes","text":"Similarly, ArrayModel wraps any function performing operation over this array. In example below, we wrap a feature matrix X and a Dense model from Flux.jl:","category":"page"},{"location":"manual/nodes/","page":"Nodes","title":"Nodes","text":"using Flux: Dense","category":"page"},{"location":"manual/nodes/","page":"Nodes","title":"Nodes","text":"f = Dense(2, 3)\nAM = ArrayModel(f)","category":"page"},{"location":"manual/nodes/","page":"Nodes","title":"Nodes","text":"We can apply the model now with AM(AN) to get another ArrayNode and verify that the feedforward layer f is really applied:","category":"page"},{"location":"manual/nodes/","page":"Nodes","title":"Nodes","text":"AM(AN)\nf(X) == AM(AN).data","category":"page"},{"location":"manual/nodes/","page":"Nodes","title":"Nodes","text":"ukn: Model outputs\nA convenient property of all Mill.jl models is that after applying them to a corresponding data node we always obtain an ArrayNode as output regardless of the type and complexity of the model. This becomes important later.","category":"page"},{"location":"manual/nodes/","page":"Nodes","title":"Nodes","text":"The most common interpretation of the data inside ArrayNodes is that each column contains features of one sample and therefore the node AN carries size(AN.data, 2) samples. In this sense, ArrayNodes wrap the standard machine learning problem, where each sample is represented with a vector, a matrix or a more general tensor of features. Alternatively, one can obtain a number of samples of any AbstractNode with nobs function from StatsBase.jl package:","category":"page"},{"location":"manual/nodes/","page":"Nodes","title":"Nodes","text":"using StatsBase: nobs\nnobs(AN)","category":"page"},{"location":"manual/nodes/#BagNode-and-BagModel","page":"Nodes","title":"BagNode and BagModel","text":"","category":"section"},{"location":"manual/nodes/","page":"Nodes","title":"Nodes","text":"BagNode is represents the standard multiple instance learning problem, that is, each sample is a bag containing an arbitrary number of instances. In the simplest case, each instance is a vector:","category":"page"},{"location":"manual/nodes/","page":"Nodes","title":"Nodes","text":"BN = BagNode(AN, [1:1, 2:3, 4:4])","category":"page"},{"location":"manual/nodes/","page":"Nodes","title":"Nodes","text":"where for simplicity we used AN from the previous example. Each BagNode carries data and bags fields:","category":"page"},{"location":"manual/nodes/","page":"Nodes","title":"Nodes","text":"BN.data\nBN.bags","category":"page"},{"location":"manual/nodes/","page":"Nodes","title":"Nodes","text":"Here, data can be an arbitrary AbstractNode storing representation of instances (ArrayNode in this case) and bags field contains information, which instances belong to which bag. In this specific case bn stores three bags (samples). The first one consists of a single instance {[1.0, 5.0]} (first column of AN), the second one of two instances {[2.0, 6.0], [3.0, 7.0]}, and the last one of a single instance {[4.0, 8.0]}. We can see that we deal with three top-level samples (bags):","category":"page"},{"location":"manual/nodes/","page":"Nodes","title":"Nodes","text":"nobs(BN)","category":"page"},{"location":"manual/nodes/","page":"Nodes","title":"Nodes","text":"whereas they are formed using four instances:","category":"page"},{"location":"manual/nodes/","page":"Nodes","title":"Nodes","text":"nobs(AN)","category":"page"},{"location":"manual/nodes/","page":"Nodes","title":"Nodes","text":"In Mill.jl, there two ways to store indices of the bag's instances:","category":"page"},{"location":"manual/nodes/","page":"Nodes","title":"Nodes","text":"* in `AlignedBags` structure, which accepts a `Vector` of `UnitRange`s and requires all bag's instances stored continuously:","category":"page"},{"location":"manual/nodes/","page":"Nodes","title":"Nodes","text":"AlignedBags([1:3, 4:4, 5:6])","category":"page"},{"location":"manual/nodes/","page":"Nodes","title":"Nodes","text":"* and in `ScatteredBags` structure, which accepts a `Vector` of `Vectors`s storing not necessarily contiguous indices:","category":"page"},{"location":"manual/nodes/","page":"Nodes","title":"Nodes","text":"ScatteredBags([[3, 2, 1], [4], [6, 5]])","category":"page"},{"location":"manual/nodes/","page":"Nodes","title":"Nodes","text":"The two examples above are semantically equivalent, as bags are unordered collections of instances. An empty bag with no instances is in AlignedBags specified as an empty range [0:-1] and in ScatteredBags as an empty vector []. The constructor of BagNode accepts directly one of these two structures and tries to automagically decide the better type in other cases.","category":"page"},{"location":"manual/nodes/","page":"Nodes","title":"Nodes","text":"Each BagNode is processed by a BagModel, which contains two (sub)models and an aggregation operator:","category":"page"},{"location":"manual/nodes/","page":"Nodes","title":"Nodes","text":"im = ArrayModel(Dense(2, 3))\na = SegmentedMax(3)\nbm = ArrayModel(Dense(4, 4))\nBM = BagModel(im, a, bm)","category":"page"},{"location":"manual/nodes/","page":"Nodes","title":"Nodes","text":"The first network submodel (called instance model im) is responsible for converting the instance representation to a vector form:","category":"page"},{"location":"manual/nodes/","page":"Nodes","title":"Nodes","text":"y = im(AN)","category":"page"},{"location":"manual/nodes/","page":"Nodes","title":"Nodes","text":"Note that because of the property mentioned above, the output of instance model im will always be a matrix. We get four columns, one for each instance. This result is then used in Aggregation (a) which takes vector representation of all instances and produces a single vector per bag:","category":"page"},{"location":"manual/nodes/","page":"Nodes","title":"Nodes","text":"y = a(y, BN.bags)","category":"page"},{"location":"manual/nodes/","page":"Nodes","title":"Nodes","text":"unk: More about aggregation\nTo read more about aggregation operators and find out why there are four rows instead of three after applying the operator, see Bag aggregation section.","category":"page"},{"location":"manual/nodes/","page":"Nodes","title":"Nodes","text":"Finally, y is then passed to a feed forward model (called bag model bm) producing the final output per bag. In our example we therefore get a matrix with three columns:","category":"page"},{"location":"manual/nodes/","page":"Nodes","title":"Nodes","text":"y = bm(y)","category":"page"},{"location":"manual/nodes/","page":"Nodes","title":"Nodes","text":"However, the best way to use a bag model node is to simply apply it, which results into the same output:","category":"page"},{"location":"manual/nodes/","page":"Nodes","title":"Nodes","text":"BM(BN) == y","category":"page"},{"location":"manual/nodes/","page":"Nodes","title":"Nodes","text":"The whole procedure is depicted in the following picture:","category":"page"},{"location":"manual/nodes/","page":"Nodes","title":"Nodes","text":"(Image: )","category":"page"},{"location":"manual/nodes/","page":"Nodes","title":"Nodes","text":"Three instances of the BagNode are represented by red subtrees are first mapped with instance model im, aggregated (aggregation operator here is a concatenation of two different operators a_1 and a_2), and the results of aggregation are transformed with bag model bm.","category":"page"},{"location":"manual/nodes/","page":"Nodes","title":"Nodes","text":"ukn: Musk example\nAnother handy feature of Mill.jl models is that they are completely differentiable and therefore fit in the Flux.jl framework. Nodes for processing arrays and bags are sufficient to solve the classical Musk problem.","category":"page"},{"location":"manual/nodes/#ProductNodes-and-ProductModels","page":"Nodes","title":"ProductNodes and ProductModels","text":"","category":"section"},{"location":"manual/nodes/","page":"Nodes","title":"Nodes","text":"ProductNode can be thought of as a Cartesian Product or a Dictionary. It holds a Tuple or NamedTuple of nodes (not necessarily of the same type). For example, a ProductNode with a BagNode and ArrayNode as children would look like this:","category":"page"},{"location":"manual/nodes/","page":"Nodes","title":"Nodes","text":"PN = ProductNode((a=ArrayNode(Float32.([1 2 3; 4 5 6])), b=BN))","category":"page"},{"location":"manual/nodes/","page":"Nodes","title":"Nodes","text":"Analogically, the ProductModel contains a (Named)Tuple of (sub)models processing each of its children (stored in ms field standing for models), as well as one more (sub)model m:","category":"page"},{"location":"manual/nodes/","page":"Nodes","title":"Nodes","text":"ms = (a=AM, b=BM)\nm = ArrayModel(Dense(7, 2))\nPM = ProductModel(ms, m)","category":"page"},{"location":"manual/nodes/","page":"Nodes","title":"Nodes","text":"Again, since the library is based on the property that the output of each model is an ArrayNode, the product model applies models from ms to appropriate children and vertically concatenates the output, which is then processed by model m. An example of model processing the above sample would be:","category":"page"},{"location":"manual/nodes/","page":"Nodes","title":"Nodes","text":"y = PM.m(vcat(PM[:a](PN[:a]), PM[:b](PN[:b])))","category":"page"},{"location":"manual/nodes/","page":"Nodes","title":"Nodes","text":"which is equivalent to:","category":"page"},{"location":"manual/nodes/","page":"Nodes","title":"Nodes","text":"PM(PN) == y","category":"page"},{"location":"manual/nodes/","page":"Nodes","title":"Nodes","text":"Application of a product model (this time with four subtrees (keys)) can be visualized as follows:","category":"page"},{"location":"manual/nodes/","page":"Nodes","title":"Nodes","text":"(Image: )","category":"page"},{"location":"manual/nodes/","page":"Nodes","title":"Nodes","text":"unk: Indexing in product nodes\nIn general, we recommend to use NamedTuples, because the key can be used for indexing both ProductNodes and ProductModels.","category":"page"},{"location":"manual/reflectin/","page":"Model Reflection","title":"Model Reflection","text":"using Mill","category":"page"},{"location":"manual/reflectin/#Model-Reflection","page":"Model Reflection","title":"Model Reflection","text":"","category":"section"},{"location":"manual/reflectin/","page":"Model Reflection","title":"Model Reflection","text":"Since constructions of large models can be a tedious and error-prone process, Mill.jl provides reflectinmodel function that helps to automate it. The simplest definition accepts only one argument, a sample ds, and returns a compatible model:","category":"page"},{"location":"manual/reflectin/","page":"Model Reflection","title":"Model Reflection","text":"ds = BagNode(ProductNode((BagNode(ArrayNode(randn(4, 10)),\n                                  [1:2, 3:4, 5:5, 6:7, 8:10]),\n                          ArrayNode(randn(3, 5)),\n                          BagNode(BagNode(ArrayNode(randn(2, 30)),\n                                          [i:i+1 for i in 1:2:30]),\n                                  [1:3, 4:6, 7:9, 10:12, 13:15]),\n                          ArrayNode(randn(2, 5)))),\n             [1:1, 2:3, 4:5]);\nprinttree(ds)\n\nm = reflectinmodel(ds);\nprinttree(m)\n\nm(ds)","category":"page"},{"location":"manual/reflectin/","page":"Model Reflection","title":"Model Reflection","text":"The sample ds serves here as a specimen needed to specify a structure of the problem and calculate dimensions.","category":"page"},{"location":"manual/reflectin/#Optional-arguments","page":"Model Reflection","title":"Optional arguments","text":"","category":"section"},{"location":"manual/reflectin/","page":"Model Reflection","title":"Model Reflection","text":"To have better control over the topology, reflectinmodel accepts up to two more optional arguments and four keyword arguments:","category":"page"},{"location":"manual/reflectin/","page":"Model Reflection","title":"Model Reflection","text":"The first optional argument expects a function that returns a layer (or a set of layers) given input dimension d (defaults to d -> Flux.Dense(d, 10)).\nThe second optional argument is a function returning aggregation function for BagModel nodes (defaults to d -> SegmentedMean(d)).","category":"page"},{"location":"manual/reflectin/","page":"Model Reflection","title":"Model Reflection","text":"Compare the following example to the previous one:","category":"page"},{"location":"manual/reflectin/","page":"Model Reflection","title":"Model Reflection","text":"using Flux","category":"page"},{"location":"manual/reflectin/","page":"Model Reflection","title":"Model Reflection","text":"m = reflectinmodel(ds, d -> Dense(d, 5, relu), d -> SegmentedMax(d));\nprinttree(m)\n\nm(ds)","category":"page"},{"location":"manual/reflectin/#Keyword-arguments","page":"Model Reflection","title":"Keyword arguments","text":"","category":"section"},{"location":"manual/reflectin/","page":"Model Reflection","title":"Model Reflection","text":"The reflectinmodel allows even further customization. To index into the sample (or model), we can use printtree(ds; trav=true) from HierarchicalUtils.jl that prints the sample together with identifiers of individual nodes:","category":"page"},{"location":"manual/reflectin/","page":"Model Reflection","title":"Model Reflection","text":"using HierarchicalUtils","category":"page"},{"location":"manual/reflectin/","page":"Model Reflection","title":"Model Reflection","text":"printtree(ds; trav=true)","category":"page"},{"location":"manual/reflectin/","page":"Model Reflection","title":"Model Reflection","text":"These identifiers can be used to override the default construction functions. Note that the output, i.e. the last feed-forward network of the whole model is always tagged with an empty string \"\", which simplifies putting linear layer with an appropriate output dimension on the end. Dictionaries with these overrides can be passed in as keyword arguments:","category":"page"},{"location":"manual/reflectin/","page":"Model Reflection","title":"Model Reflection","text":"fsm overrides constructions of feed-forward models\nfsa overrides construction of aggregation functions.","category":"page"},{"location":"manual/reflectin/","page":"Model Reflection","title":"Model Reflection","text":"For example to specify just the last feed forward neural network:","category":"page"},{"location":"manual/reflectin/","page":"Model Reflection","title":"Model Reflection","text":"reflectinmodel(ds, d -> Dense(d, 5, relu), d -> SegmentedMeanMax(d);\n    fsm = Dict(\"\" => d -> Chain(Dense(d, 20, relu), Dense(20, 12)))) |> printtree","category":"page"},{"location":"manual/reflectin/","page":"Model Reflection","title":"Model Reflection","text":"Both keyword arguments in action:","category":"page"},{"location":"manual/reflectin/","page":"Model Reflection","title":"Model Reflection","text":"reflectinmodel(ds, d -> Dense(d, 5, relu), d -> SegmentedMeanMax(d);\n    fsm = Dict(\"\" => d -> Chain(Dense(d, 20, relu), Dense(20, 12))),\n    fsa = Dict(\"Y\" => d -> SegmentedMean(d), \"g\" => d -> SegmentedPNorm(d))) |> printtree","category":"page"},{"location":"manual/aggregation/","page":"Bag aggregation","title":"Bag aggregation","text":"using Mill","category":"page"},{"location":"manual/aggregation/#Bag-aggregation","page":"Bag aggregation","title":"Bag aggregation","text":"","category":"section"},{"location":"manual/aggregation/","page":"Bag aggregation","title":"Bag aggregation","text":"A wrapper type Aggregation and all subtypes of AggregationOperator it wraps are structures that are responsible for mapping of vector representations of multiple instances into a single vector. They all operate element-wise and independently of dimension and thus the output has the same size as representations on the input, unless the Concatenation of multiple operators is used or Bag Count is enabled.","category":"page"},{"location":"manual/aggregation/","page":"Bag aggregation","title":"Bag aggregation","text":"Some setup:","category":"page"},{"location":"manual/aggregation/","page":"Bag aggregation","title":"Bag aggregation","text":"d = 2\nX = Float32.([1 2 3 4; 8 7 6 5])\nn = ArrayNode(X)\nbags = AlignedBags([1:1, 2:3, 4:4])\n\nMill.bagcount!(false)","category":"page"},{"location":"manual/aggregation/","page":"Bag aggregation","title":"Bag aggregation","text":"Different choice of operator, or their combinations, are suitable for different problems. Nevertheless, because the input is interpreted as an unordered bag of instances, every operator is invariant to permutation and also does not scale when increasing size of the bag.","category":"page"},{"location":"manual/aggregation/#Non-parametric-aggregation","page":"Bag aggregation","title":"Non-parametric aggregation","text":"","category":"section"},{"location":"manual/aggregation/","page":"Bag aggregation","title":"Bag aggregation","text":"SegmentedMax is the most straightforward operator defined in one dimension as follows:","category":"page"},{"location":"manual/aggregation/","page":"Bag aggregation","title":"Bag aggregation","text":"a_max(x_1 ldots x_k) = max_i = 1 ldots k x_i","category":"page"},{"location":"manual/aggregation/","page":"Bag aggregation","title":"Bag aggregation","text":"where x_1 ldots x_k are all instances of the given bag. In Mill.jl, the operator is constructed this way:","category":"page"},{"location":"manual/aggregation/","page":"Bag aggregation","title":"Bag aggregation","text":"a_max = SegmentedMax(d)","category":"page"},{"location":"manual/aggregation/","page":"Bag aggregation","title":"Bag aggregation","text":"ukn: Dimension\nThe dimension of input is required so that the default parameters ψ can be properly instantiated (see Missing data for details).","category":"page"},{"location":"manual/aggregation/","page":"Bag aggregation","title":"Bag aggregation","text":"The application is straightforward and can be performed on both raw AbstractArrays or ArrayNodes:","category":"page"},{"location":"manual/aggregation/","page":"Bag aggregation","title":"Bag aggregation","text":"a_max(X, bags)\na_max(n, bags)","category":"page"},{"location":"manual/aggregation/","page":"Bag aggregation","title":"Bag aggregation","text":"Since we have three bags, we have three columns in the output, each storing the maximal element over all instances of the given bag.","category":"page"},{"location":"manual/aggregation/#SegmentedMean","page":"Bag aggregation","title":"SegmentedMean","text":"","category":"section"},{"location":"manual/aggregation/","page":"Bag aggregation","title":"Bag aggregation","text":"SegmentedMean is defined as:","category":"page"},{"location":"manual/aggregation/","page":"Bag aggregation","title":"Bag aggregation","text":"a_operatornamemean(x_1 ldots x_k) = frac1k sum_i = 1^k x_i","category":"page"},{"location":"manual/aggregation/","page":"Bag aggregation","title":"Bag aggregation","text":"and used the same way:","category":"page"},{"location":"manual/aggregation/","page":"Bag aggregation","title":"Bag aggregation","text":"a_mean = SegmentedMean(d)\na_mean(X, bags)\na_mean(n, bags)","category":"page"},{"location":"manual/aggregation/","page":"Bag aggregation","title":"Bag aggregation","text":"ukn: Sufficiency of the mean operator\nIn theory, SegmentedMean is sufficient for approximation (Tomáš Pevný , Vojtěch Kovařík  (2019)), but in practice, a combination of multiple operators performes better.","category":"page"},{"location":"manual/aggregation/","page":"Bag aggregation","title":"Bag aggregation","text":"The max aggregation is suitable for cases when one instance in the bag may give evidence strong enough to predict the label. On the other side of the spectrum lies the mean aggregation function, which detects well trends identifiable globally over the whole bag.","category":"page"},{"location":"manual/aggregation/#Parametric-aggregation","page":"Bag aggregation","title":"Parametric aggregation","text":"","category":"section"},{"location":"manual/aggregation/","page":"Bag aggregation","title":"Bag aggregation","text":"Whereas non-parametric aggregations do not use any parameter, parametric aggregations represent an entire class of functions parametrized by one or more real vectors of parameters, which can be even learned during training.","category":"page"},{"location":"manual/aggregation/#SegmentedLSE","page":"Bag aggregation","title":"SegmentedLSE","text":"","category":"section"},{"location":"manual/aggregation/","page":"Bag aggregation","title":"Bag aggregation","text":"SegmentedLSE (log-sum-exp) aggregation (Oren Z. Kraus , Lei Jimmy Ba , Brendan Frey  (2015)) is parametrized by a vector of positive numbers bmr in (mathbbR^+)^d m that specifies one real parameter for computation in each output dimension:","category":"page"},{"location":"manual/aggregation/","page":"Bag aggregation","title":"Bag aggregation","text":"a_operatornamelse(x_1 ldots x_k r) = frac1rlog left(frac1k sum_i = 1^k exp(rcdot x_i)right)","category":"page"},{"location":"manual/aggregation/","page":"Bag aggregation","title":"Bag aggregation","text":"With different values of r, LSE behaves differently and in fact both max and mean operators are limiting cases of LSE. If r is very small, the output approaches simple mean, and on the other hand, if r is a large number, LSE becomes a smooth approximation of the max function. Naively implementing the definition above may lead to numerical instabilities, however, the Mill.jl implementation is numerically stable.","category":"page"},{"location":"manual/aggregation/","page":"Bag aggregation","title":"Bag aggregation","text":"a_lse = SegmentedLSE(d)\na_lse(X, bags)","category":"page"},{"location":"manual/aggregation/#SegmentedPNorm","page":"Bag aggregation","title":"SegmentedPNorm","text":"","category":"section"},{"location":"manual/aggregation/","page":"Bag aggregation","title":"Bag aggregation","text":"(Normalized) p-norm operator (Caglar Gulcehre , Kyunghyun Cho , Razvan Pascanu , Yoshua Bengio  (2014)) is parametrized by a vector of real numbers bmp in (mathbbR^+)^d, where forall i in 1 ldots m  colon p_i geq 1, and another vector bmc in (mathbbR^+)^d. It is computed with formula:","category":"page"},{"location":"manual/aggregation/","page":"Bag aggregation","title":"Bag aggregation","text":"a_operatornamepnorm(x_1 ldots x_k p c) = left(frac1k sum_i = 1^k vert x_i - c vert ^ p right)^frac1p","category":"page"},{"location":"manual/aggregation/","page":"Bag aggregation","title":"Bag aggregation","text":"Again, the Mill.jl implementation is stable.","category":"page"},{"location":"manual/aggregation/","page":"Bag aggregation","title":"Bag aggregation","text":"a_pnorm = SegmentedPNorm(d)\na_pnorm(X, bags)","category":"page"},{"location":"manual/aggregation/","page":"Bag aggregation","title":"Bag aggregation","text":"Because all parameter constraints are included implicitly (field \\rho in both types is a real number that undergoes appropriate transformation before being used), both parametric operators are easy to use and do not require any special treatment. Replacing the definition of aggregation operators while constructing a model (either manually or with reflectinmodel) is enough.","category":"page"},{"location":"manual/aggregation/#Concatenation","page":"Bag aggregation","title":"Concatenation","text":"","category":"section"},{"location":"manual/aggregation/","page":"Bag aggregation","title":"Bag aggregation","text":"To use a concatenation of two or more operators, one can use an Aggregation constructor:","category":"page"},{"location":"manual/aggregation/","page":"Bag aggregation","title":"Bag aggregation","text":"a = Aggregation(a_mean, a_max)\na(X, bags)","category":"page"},{"location":"manual/aggregation/","page":"Bag aggregation","title":"Bag aggregation","text":"For the most common combinations, Mill.jl provides some convenience definitions:","category":"page"},{"location":"manual/aggregation/","page":"Bag aggregation","title":"Bag aggregation","text":"SegmentedMeanMax(d)","category":"page"},{"location":"manual/aggregation/#Weighted-aggregation","page":"Bag aggregation","title":"Weighted aggregation","text":"","category":"section"},{"location":"manual/aggregation/","page":"Bag aggregation","title":"Bag aggregation","text":"Sometimes, different instances in the bag are not equally important and contribute to output to a different extent. For instance, this may come in handy when performing importance sampling over very large bags. SegmentedMean and SegmentedPNorm have definitions taking weights into account:","category":"page"},{"location":"manual/aggregation/","page":"Bag aggregation","title":"Bag aggregation","text":"a_operatornamemean((x_i w_i)_i=1^k) = frac1sum_i=1^k w_i sum_i = 1^k w_i cdot x_i","category":"page"},{"location":"manual/aggregation/","page":"Bag aggregation","title":"Bag aggregation","text":"a_operatornamepnorm(x_i w_i_i=1^k p c) = left(frac1sum_i=1^k w_i sum_i = 1^k w_icdotvert x_i - c vert ^ p right)^frac1p","category":"page"},{"location":"manual/aggregation/","page":"Bag aggregation","title":"Bag aggregation","text":"This is done in Mill.jl by passing an additional parameter:","category":"page"},{"location":"manual/aggregation/","page":"Bag aggregation","title":"Bag aggregation","text":"w = Float32.([1.0, 0.2, 0.8, 0.5])\na_mean(X, bags, w)\na_pnorm(X, bags, w)","category":"page"},{"location":"manual/aggregation/","page":"Bag aggregation","title":"Bag aggregation","text":"For SegmentedMax (and SegmentedLSE) it is possible to pass in weights, but they are ignored during computation:","category":"page"},{"location":"manual/aggregation/","page":"Bag aggregation","title":"Bag aggregation","text":"a_max(X, bags, w) == a_max(X, bags)","category":"page"},{"location":"manual/aggregation/#WeightedBagNode","page":"Bag aggregation","title":"WeightedBagNode","text":"","category":"section"},{"location":"manual/aggregation/","page":"Bag aggregation","title":"Bag aggregation","text":"WeightedBagNode is used to store instance weights into a dataset. It accepts weights in the constructor:","category":"page"},{"location":"manual/aggregation/","page":"Bag aggregation","title":"Bag aggregation","text":"wbn = WeightedBagNode(n, bags, w)","category":"page"},{"location":"manual/aggregation/","page":"Bag aggregation","title":"Bag aggregation","text":"and passes them to aggregation operators:","category":"page"},{"location":"manual/aggregation/","page":"Bag aggregation","title":"Bag aggregation","text":"m = reflectinmodel(wbn)\nm(wbn)","category":"page"},{"location":"manual/aggregation/","page":"Bag aggregation","title":"Bag aggregation","text":"Otherwise, WeightedBagNode behaves exactly like the standard BagNode.","category":"page"},{"location":"manual/aggregation/#Bag-count","page":"Bag aggregation","title":"Bag count","text":"","category":"section"},{"location":"manual/aggregation/","page":"Bag aggregation","title":"Bag aggregation","text":"For some problems, it may be beneficial to use the size of the bag directly and feed it to subsequent layers. Whether this is the case is controlled by Mill.bagcount!(::Bool) function. It is on by default, however, it was disabled at the beginning of this section for demonstration purposes. Let's turn it back on:","category":"page"},{"location":"manual/aggregation/","page":"Bag aggregation","title":"Bag aggregation","text":"Mill.bagcount!(true)","category":"page"},{"location":"manual/aggregation/","page":"Bag aggregation","title":"Bag aggregation","text":"In the aggregation phase, bag count appends one more element which stores the bag size to the output after all operators are applied. Furthermore, in Mill.jl, we opted to perform a mapping x mapsto log(x) + 1 on top of that:","category":"page"},{"location":"manual/aggregation/","page":"Bag aggregation","title":"Bag aggregation","text":"a_mean(X, bags)","category":"page"},{"location":"manual/aggregation/","page":"Bag aggregation","title":"Bag aggregation","text":"The matrix now has three rows, the last one storing the size of the bag.","category":"page"},{"location":"manual/aggregation/","page":"Bag aggregation","title":"Bag aggregation","text":"When the bag count is on, one needs to have a model accepting corresponding sizes:","category":"page"},{"location":"manual/aggregation/","page":"Bag aggregation","title":"Bag aggregation","text":"bn = BagNode(n, bags)\nbm = reflectinmodel(bn)","category":"page"},{"location":"manual/aggregation/","page":"Bag aggregation","title":"Bag aggregation","text":"Note that the bm (sub)model field of the BagNode has size of (11, 10), 10 for aggregation output and 1 for sizes of bags.","category":"page"},{"location":"manual/aggregation/","page":"Bag aggregation","title":"Bag aggregation","text":"bm(bn)","category":"page"},{"location":"manual/aggregation/","page":"Bag aggregation","title":"Bag aggregation","text":"Model reflection takes bag count toggle into account. If we disable it again, bm (sub)model has size (10, 10):","category":"page"},{"location":"manual/aggregation/","page":"Bag aggregation","title":"Bag aggregation","text":"Mill.bagcount!(false)\nbm = reflectinmodel(bn)","category":"page"},{"location":"manual/aggregation/#Default-aggregation-values","page":"Bag aggregation","title":"Default aggregation values","text":"","category":"section"},{"location":"manual/aggregation/","page":"Bag aggregation","title":"Bag aggregation","text":"When all aggregation operators are printed, one may notice that all of them store one additional vector ψ. This is a vector of default parameters, initialized to all zeros, that are used for empty bags:","category":"page"},{"location":"manual/aggregation/","page":"Bag aggregation","title":"Bag aggregation","text":"bags = AlignedBags([1:1, 0:-1, 2:3, 0:-1, 4:4])\na_mean(X, bags)","category":"page"},{"location":"manual/aggregation/","page":"Bag aggregation","title":"Bag aggregation","text":"See Missing data page for more information.","category":"page"}]
}
