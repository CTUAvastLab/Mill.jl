var documenterSearchIndex = {"docs":
[{"location":"missing/#Missing-values","page":"Missing values","title":"Missing values","text":"","category":"section"},{"location":"missing/","page":"Missing values","title":"Missing values","text":"At the moment, Mill.jl features an initial and naive approach to missing values. We assume that ArrayNode have missing values replaced by zeros, which is not optimal but in many situations it works reasonably well.","category":"page"},{"location":"missing/","page":"Missing values","title":"Missing values","text":"BagNodes with missing features are indicated by Bags being set to [0:-1] with missing as a data and metadata. This can be seamlessly concatenated or sub-set, if the operation makes sense.","category":"page"},{"location":"missing/","page":"Missing values","title":"Missing values","text":"Couple examples from unit tests. Let's define full and empty BagNode","category":"page"},{"location":"missing/","page":"Missing values","title":"Missing values","text":"julia> a = BagNode(ArrayNode(rand(3,4)),[1:4], nothing)\nBagNode with 1 bag(s)\n  └── ArrayNode(3, 4)\n\njulia> e = BagNode(missing, AlignedBags([0:-1]), nothing)\nBagNode with 1 empty bag(s)","category":"page"},{"location":"missing/","page":"Missing values","title":"Missing values","text":"We can concatenate them as follows.","category":"page"},{"location":"missing/","page":"Missing values","title":"Missing values","text":"julia> x = reduce(catobs,[a, e])\nBagNode with 2 bag(s)\n  └── ArrayNode(3, 4)","category":"page"},{"location":"missing/","page":"Missing values","title":"Missing values","text":"Notice, that the ArrayNode has still the same dimension as ArrayNode of just a. The missing second element, corresponding to e is indicated by the second bags being 0:-1 as follows:","category":"page"},{"location":"missing/","page":"Missing values","title":"Missing values","text":"julia> x.bags\nAlignedBags(UnitRange{Int64}[1:4, 0:-1])","category":"page"},{"location":"missing/","page":"Missing values","title":"Missing values","text":"We can get back the missing second element as","category":"page"},{"location":"missing/","page":"Missing values","title":"Missing values","text":"julia> x[2]\nBagNode with 1 empty bag(s)","category":"page"},{"location":"missing/","page":"Missing values","title":"Missing values","text":"During forward (and backward) pass, the missing values in BagNodes are filled in aggregation by zeros. ** In order this feature to work, the Aggregation needs to know dimension, therefore use MissingAggregation, which can handle this.** In the future, MissingAggregation will be made default.","category":"page"},{"location":"missing/","page":"Missing values","title":"Missing values","text":"Last but not least, ProductNodes cannot handle missing values, as the missingness is propagated to its leaves, i.e.","category":"page"},{"location":"missing/","page":"Missing values","title":"Missing values","text":"julia> ProductNode((a,e))\nProductNode{2}\n  ├── BagNode with 1 bag(s)\n  │     └── ArrayNode(3, 4)\n  └── BagNode with 1 empty bag(s)","category":"page"},{"location":"#Mill-–-Multiple-Instance-Learning-Library","page":"Mill – Multiple Instance Learning Library","title":"Mill – Multiple Instance Learning Library","text":"","category":"section"},{"location":"","page":"Mill – Multiple Instance Learning Library","title":"Mill – Multiple Instance Learning Library","text":"Mill is a library build on top of Flux.jl aimed to flexibly prototype hierarchical multi-instance learning models as described in [1] and  [2]","category":"page"},{"location":"#What-is-Multiple-instance-learning-(MIL)-problem?","page":"Mill – Multiple Instance Learning Library","title":"What is Multiple instance learning (MIL) problem?","text":"","category":"section"},{"location":"","page":"Mill – Multiple Instance Learning Library","title":"Mill – Multiple Instance Learning Library","text":"In the prototypical machine learning problem the input sample (Image: equation) is a vector or matrix of a fixed dimension, or a sequence. In MIL problems the sample (Image: equation) is a set of vectors (or matrices) (Image: equation), which means that order does not matter, and which is also the feature making MIL problems different from sequences. The multi-instance problems have been introduced in by Tom Diettrich in [4] in 1997.","category":"page"},{"location":"","page":"Mill – Multiple Instance Learning Library","title":"Mill – Multiple Instance Learning Library","text":"Pevný and Somol ([1] and  [2]) (and later \"indepently\" Zaheer et al. [5]) have proposed simple way to solve MIL problems with neural networks. The network consists from two non-linear layers, with aggregation operator like mean (or maximum) sandwiched between nonlinearities. Denoting (Image: equation), (Image: equation) layers of neural network, the output is calculated as (Image: equation). In [3], authors have further extended the universal approximation theorem to MIL problems.","category":"page"},{"location":"#Multiple-instance-learning-on-Musk-1","page":"Mill – Multiple Instance Learning Library","title":"Multiple instance learning on Musk 1","text":"","category":"section"},{"location":"","page":"Mill – Multiple Instance Learning Library","title":"Mill – Multiple Instance Learning Library","text":"Musk dataset is a classic problem of the field used in publication [4], which has given the class of problems its name. ","category":"page"},{"location":"","page":"Mill – Multiple Instance Learning Library","title":"Mill – Multiple Instance Learning Library","text":"Below is a little walk-through how to solve the problem using Mill library. The full example is shown in example/musk.jl, which also contains Julia environment to run the whole thing.","category":"page"},{"location":"","page":"Mill – Multiple Instance Learning Library","title":"Mill – Multiple Instance Learning Library","text":"Let's start by importing all libraries","category":"page"},{"location":"","page":"Mill – Multiple Instance Learning Library","title":"Mill – Multiple Instance Learning Library","text":"julia> using FileIO, JLD2, Statistics, Mill, Flux\njulia> using Flux: throttle, @epochs\njulia> using Mill: reflectinmodel\njulia> using Base.Iterators: repeated","category":"page"},{"location":"","page":"Mill – Multiple Instance Learning Library","title":"Mill – Multiple Instance Learning Library","text":"Loading a dataset from file and folding it in Mill's data-structures is done in the following function. musk.jld2 contains matrix with features, fMat, the id of sample (called bag in MIL terminology) to which each instance (column in fMat) belongs to, and finally a label of each instance in y.  BagNode is a structure which holds feature matrix and ranges of columns of each bag. Finally, BagNode can be concatenated (use catobs) and you can get subset using getindex.","category":"page"},{"location":"","page":"Mill – Multiple Instance Learning Library","title":"Mill – Multiple Instance Learning Library","text":"julia> fMat = load(\"musk.jld2\", \"fMat\");         # matrix with instances, each column is one sample\njulia> bagids = load(\"musk.jld2\", \"bagids\");     # ties instances to bags\njulia> x = BagNode(ArrayNode(fMat), bagids);     # create BagDataset\njulia> y = load(\"musk.jld2\", \"y\");               # load labels\njulia> y = map(i -> maximum(y[i]) + 1, x.bags);  # create labels on bags\njulia> y_oh = Flux.onehotbatch(y, 1:2);          # one-hot encoding","category":"page"},{"location":"","page":"Mill – Multiple Instance Learning Library","title":"Mill – Multiple Instance Learning Library","text":"Once we have data, we can manually create a model. BagModel is designed to implement a basic multi-instance learning model as described above. Below, we use a simple model, where instances are first passed through a single layer with 10 neurons (input dimension is 166) with tanh non-linearity, then we use mean and max aggregation functions simultaneously (for some problems, max is better then mean, therefore we use both), and then we use one layer with 10 neurons and tanh nonlinearity followed by output linear layer with 2 neurons (output dimension).","category":"page"},{"location":"","page":"Mill – Multiple Instance Learning Library","title":"Mill – Multiple Instance Learning Library","text":"julia> model = BagModel(\n    ArrayModel(Dense(166, 10, Flux.tanh)),                      # model on the level of Flows\n    SegmentedMeanMax(10),                                       # aggregation\n    ArrayModel(Chain(Dense(20, 10, Flux.tanh), Dense(10, 2))))  # model on the level of bags\n    \nBagModel ↦ ⟨SegmentedMean(10), SegmentedMax(10)⟩ ↦ ArrayModel(Chain(Dense(20, 10, tanh), Dense(10, 2)))\n  └── ArrayModel(Dense(166, 10, tanh))","category":"page"},{"location":"","page":"Mill – Multiple Instance Learning Library","title":"Mill – Multiple Instance Learning Library","text":"The loss function is standard cross-entropy:","category":"page"},{"location":"","page":"Mill – Multiple Instance Learning Library","title":"Mill – Multiple Instance Learning Library","text":"julia> loss(x, y_oh) = Flux.logitcrossentropy(model(x).data, y_oh);","category":"page"},{"location":"","page":"Mill – Multiple Instance Learning Library","title":"Mill – Multiple Instance Learning Library","text":"Finally, we put everything together. The below code should resemble an example from Flux.jl library.","category":"page"},{"location":"","page":"Mill – Multiple Instance Learning Library","title":"Mill – Multiple Instance Learning Library","text":"julia> evalcb = () -> @show(loss(x, y_oh));\njulia> opt = Flux.ADAM();\njulia> @epochs 10 Flux.train!(loss, params(model), repeated((x, y_oh), 1000), opt, cb=throttle(evalcb, 1))\n\n[ Info: Epoch 1\nloss(x, y_oh) = 87.793724f0\n[ Info: Epoch 2\nloss(x, y_oh) = 4.3207192f0\n[ Info: Epoch 3\nloss(x, y_oh) = 4.2778687f0\n[ Info: Epoch 4\nloss(x, y_oh) = 0.662226f0\n[ Info: Epoch 5\nloss(x, y_oh) = 5.76351f-6\n[ Info: Epoch 6\nloss(x, y_oh) = 3.8146973f-6\n[ Info: Epoch 7\nloss(x, y_oh) = 2.8195589f-6\n[ Info: Epoch 8\nloss(x, y_oh) = 2.4878461f-6\n[ Info: Epoch 9\nloss(x, y_oh) = 2.1561332f-6\n[ Info: Epoch 10\nloss(x, y_oh) = 1.7414923f-6","category":"page"},{"location":"","page":"Mill – Multiple Instance Learning Library","title":"Mill – Multiple Instance Learning Library","text":"Because we did not leave any data for validation, we can only calculate error on the training data, which should be not so surprisingly low.","category":"page"},{"location":"","page":"Mill – Multiple Instance Learning Library","title":"Mill – Multiple Instance Learning Library","text":"mean(mapslices(argmax, model(x).data, dims=1)' .!= y)\n\n0.0","category":"page"},{"location":"#More-complicated-models","page":"Mill – Multiple Instance Learning Library","title":"More complicated models","text":"","category":"section"},{"location":"","page":"Mill – Multiple Instance Learning Library","title":"Mill – Multiple Instance Learning Library","text":"The main advantage of the Mill library is that it allows to arbitrarily nest and cross-product BagModels, as is described in Theorem 5 of [3].  Let's start the demonstration by nesting two MIL problems. The outer MIL model contains three samples. The first sample contains another bag (inner MIL) problem with two instances, the second sample contains two inner bags with total of three instances, and finally the third sample contains two inner bags with four instances.","category":"page"},{"location":"","page":"Mill – Multiple Instance Learning Library","title":"Mill – Multiple Instance Learning Library","text":"julia> ds = BagNode(BagNode(ArrayNode(randn(4,10)),[1:2,3:4,5:5,6:7,8:10]),[1:1,2:3,4:5])\nBagNode with 3 bag(s)\n  └── BagNode with 5 bag(s)\n        └── ArrayNode(4, 10)","category":"page"},{"location":"","page":"Mill – Multiple Instance Learning Library","title":"Mill – Multiple Instance Learning Library","text":"We can create the model manually as in the case of Musk as","category":"page"},{"location":"","page":"Mill – Multiple Instance Learning Library","title":"Mill – Multiple Instance Learning Library","text":"julia> m = BagModel(\n    BagModel(\n        ArrayModel(Dense(4, 3, Flux.relu)),   \n        SegmentedMeanMax(3),\n        ArrayModel(Dense(6, 3, Flux.relu))),\n    SegmentedMeanMax(3),\n    ArrayModel(Chain(Dense(6, 3, Flux.relu), Dense(3,2))))\n\nBagModel ↦ ⟨SegmentedMean(3), SegmentedMax(3)⟩ ↦ ArrayModel(Chain(Dense(6, 3, relu), Dense(3, 2)))\n  └── BagModel ↦ ⟨SegmentedMean(3), SegmentedMax(3)⟩ ↦ ArrayModel(Dense(6, 3, relu))\n        └── ArrayModel(Dense(4, 3, relu))","category":"page"},{"location":"","page":"Mill – Multiple Instance Learning Library","title":"Mill – Multiple Instance Learning Library","text":"and we can apply the model as","category":"page"},{"location":"","page":"Mill – Multiple Instance Learning Library","title":"Mill – Multiple Instance Learning Library","text":"julia> m(ds)\n\nArrayNode(2, 3)","category":"page"},{"location":"","page":"Mill – Multiple Instance Learning Library","title":"Mill – Multiple Instance Learning Library","text":"Since constructions of large models can be a process prone to errors, there is a function reflectinmodel which tries to automatize it keeping track of dimensions. It accepts as a first parameter a sample ds. Using the function on the above example creates a model:","category":"page"},{"location":"","page":"Mill – Multiple Instance Learning Library","title":"Mill – Multiple Instance Learning Library","text":"julia> m = reflectinmodel(ds)\n\nBagModel ↦ SegmentedMean(10) ↦ ArrayModel(Dense(10, 10))\n  └── BagModel ↦ SegmentedMean(10) ↦ ArrayModel(Dense(10, 10))\n        └── ArrayModel(Dense(4, 10))","category":"page"},{"location":"","page":"Mill – Multiple Instance Learning Library","title":"Mill – Multiple Instance Learning Library","text":"To have better control over the topology, reflectinmodel accepts up to four additional parameters. The second parameter is a function returning layer (or set of layers) with input dimension d, and the third function is a function returning aggregation functions for BagModel:","category":"page"},{"location":"","page":"Mill – Multiple Instance Learning Library","title":"Mill – Multiple Instance Learning Library","text":"julia> m = reflectinmodel(ds, d -> Dense(d, 5, relu), d -> SegmentedMeanMax(d))\n\nBagModel ↦ ⟨SegmentedMean(5), SegmentedMax(5)⟩ ↦ ArrayModel(Dense(10, 5, relu))\n  └── BagModel ↦ ⟨SegmentedMean(5), SegmentedMax(5)⟩ ↦ ArrayModel(Dense(10, 5, relu))\n        └── ArrayModel(Dense(4, 5, relu))","category":"page"},{"location":"","page":"Mill – Multiple Instance Learning Library","title":"Mill – Multiple Instance Learning Library","text":"Let's test the model","category":"page"},{"location":"","page":"Mill – Multiple Instance Learning Library","title":"Mill – Multiple Instance Learning Library","text":"julia> m(ds).data\n\n5×3 Array{Float32,2}:\n 0.0542484   0.733629  0.553823\n 0.062246    0.866254  1.03062 \n 0.027454    1.04703   1.63135 \n 0.00796955  0.36415   1.18108 \n 0.034735    0.17383   0.0","category":"page"},{"location":"#Even-more-complicated-models","page":"Mill – Multiple Instance Learning Library","title":"Even more complicated models","text":"","category":"section"},{"location":"","page":"Mill – Multiple Instance Learning Library","title":"Mill – Multiple Instance Learning Library","text":"As already mentioned above, the datasets can contain Cartesian products of MIL and normal (non-MIL) problems. Let's do a quick demo.","category":"page"},{"location":"","page":"Mill – Multiple Instance Learning Library","title":"Mill – Multiple Instance Learning Library","text":"julia> ds = BagNode(\n    ProductNode(\n        (BagNode(ArrayNode(randn(4,10)),[1:2,3:4,5:5,6:7,8:10]),\n        ArrayNode(randn(3,5)),\n        BagNode(\n            BagNode(ArrayNode(randn(2,30)),[i:i+1 for i in 1:2:30]),\n            [1:3,4:6,7:9,10:12,13:15]),\n        ArrayNode(randn(2,5)))),\n    [1:1,2:3,4:5])\n\nBagNode with 3 bag(s)\n  └── ProductNode\n        ├── BagNode with 5 bag(s)\n        │     ⋮\n        ├── ArrayNode(3, 5)\n        ├── BagNode with 5 bag(s)\n        │     ⋮\n        └── ArrayNode(2, 5)","category":"page"},{"location":"","page":"Mill – Multiple Instance Learning Library","title":"Mill – Multiple Instance Learning Library","text":"For this, we really want to create model automatically despite it being sub-optimal.","category":"page"},{"location":"","page":"Mill – Multiple Instance Learning Library","title":"Mill – Multiple Instance Learning Library","text":"julia> m = reflectinmodel(ds, d -> Dense(d, 3, relu), d -> SegmentedMeanMax(d))\n\nBagModel ↦ ⟨SegmentedMean(3), SegmentedMax(3)⟩ ↦ ArrayModel(Dense(6, 3, relu))\n  └── ProductModel ↦ ArrayModel(Dense(12, 3, relu))\n        ├── BagModel ↦ ⟨SegmentedMean(3), SegmentedMax(3)⟩ ↦ ArrayModel(Dense(6, 3, relu))\n        │     ⋮\n        ├── ArrayModel(Dense(3, 3, relu))\n        ├── BagModel ↦ ⟨SegmentedMean(3), SegmentedMax(3)⟩ ↦ ArrayModel(Dense(6, 3, relu))\n        │     ⋮\n        └── ArrayModel(Dense(2, 3, relu))","category":"page"},{"location":"#Hierarchical-utils","page":"Mill – Multiple Instance Learning Library","title":"Hierarchical utils","text":"","category":"section"},{"location":"","page":"Mill – Multiple Instance Learning Library","title":"Mill – Multiple Instance Learning Library","text":"Mill.jl uses HierarchicalUtils.jl which brings a lot of additional features. For instance, if you want to print a non-truncated version of a model, call:","category":"page"},{"location":"","page":"Mill – Multiple Instance Learning Library","title":"Mill – Multiple Instance Learning Library","text":"julia> printtree(m; trunc=Inf)\n\nBagModel ↦ ⟨SegmentedMean(3), SegmentedMax(3)⟩ ↦ ArrayModel(Dense(6, 3, relu))\n  └── ProductModel ↦ ArrayModel(Dense(12, 3, relu))\n        ├── BagModel ↦ ⟨SegmentedMean(3), SegmentedMax(3)⟩ ↦ ArrayModel(Dense(6, 3, relu))\n        │     └── ArrayModel(Dense(4, 3, relu))\n        ├── ArrayModel(Dense(3, 3, relu))\n        ├── BagModel ↦ ⟨SegmentedMean(3), SegmentedMax(3)⟩ ↦ ArrayModel(Dense(6, 3, relu))\n        │     └── BagModel ↦ ⟨SegmentedMean(3), SegmentedMax(3)⟩ ↦ ArrayModel(Dense(6, 3, relu))\n        │           └── ArrayModel(Dense(2, 3, relu))\n        └── ArrayModel(Dense(2, 3, relu))","category":"page"},{"location":"","page":"Mill – Multiple Instance Learning Library","title":"Mill – Multiple Instance Learning Library","text":"Callling with trav=true enables convenient traversal functionality with string indexing:","category":"page"},{"location":"","page":"Mill – Multiple Instance Learning Library","title":"Mill – Multiple Instance Learning Library","text":"julia>  printtree(m; trunc=Inf, trav=true)\n\nBagModel ↦ ⟨SegmentedMean(3), SegmentedMax(3)⟩ ↦ ArrayModel(Dense(6, 3, relu)) [\"\"]\n  └── ProductModel ↦ ArrayModel(Dense(12, 3, relu)) [\"U\"]\n        ├── BagModel ↦ ⟨SegmentedMean(3), SegmentedMax(3)⟩ ↦ ArrayModel(Dense(6, 3, relu)) [\"Y\"]\n        │     └── ArrayModel(Dense(4, 3, relu)) [\"a\"]\n        ├── ArrayModel(Dense(3, 3, relu)) [\"c\"]\n        ├── BagModel ↦ ⟨SegmentedMean(3), SegmentedMax(3)⟩ ↦ ArrayModel(Dense(6, 3, relu)) [\"g\"]\n        │     └── BagModel ↦ ⟨SegmentedMean(3), SegmentedMax(3)⟩ ↦ ArrayModel(Dense(6, 3, relu)) [\"i\"]\n        │           └── ArrayModel(Dense(2, 3, relu)) [\"j\"]\n        └── ArrayModel(Dense(2, 3, relu)) [\"k\"]","category":"page"},{"location":"","page":"Mill – Multiple Instance Learning Library","title":"Mill – Multiple Instance Learning Library","text":"This way any node in the model tree is swiftly accessible, which may come in handy when inspecting model parameters or simply deleting/replacing/inserting nodes to tree (for instance when constructing adversarial samples). All tree nodes are accessible by indexing with the traversal code:.","category":"page"},{"location":"","page":"Mill – Multiple Instance Learning Library","title":"Mill – Multiple Instance Learning Library","text":"julia> m[\"Y\"]\n\nBagModel ↦ ⟨SegmentedMean(3), SegmentedMax(3)⟩ ↦ ArrayModel(Dense(6, 3, relu))\n  └── ArrayModel(Dense(4, 3, relu))","category":"page"},{"location":"","page":"Mill – Multiple Instance Learning Library","title":"Mill – Multiple Instance Learning Library","text":"The following two approaches give the same result:","category":"page"},{"location":"","page":"Mill – Multiple Instance Learning Library","title":"Mill – Multiple Instance Learning Library","text":"julia> m[\"Y\"] === m.im.ms[1]\n\ntrue","category":"page"},{"location":"","page":"Mill – Multiple Instance Learning Library","title":"Mill – Multiple Instance Learning Library","text":"Other functions provided by HierarchicalUtils.jl:","category":"page"},{"location":"","page":"Mill – Multiple Instance Learning Library","title":"Mill – Multiple Instance Learning Library","text":"julia> nnodes(m)\n\n9\n\njulia> nleafs(m)\n\n4\n\njulia> NodeIterator(m) |> collect\n\n9-element Array{AbstractMillModel,1}:\n BagModel\n ProductModel\n BagModel\n ArrayModel\n ArrayModel\n BagModel\n BagModel\n ArrayModel\n ArrayModel\n\njulia> LeafIterator(m) |> collect\n\n4-element Array{ArrayModel{Dense{typeof(relu),Array{Float32,2},Array{Float32,1}}},1}:\n ArrayModel\n ArrayModel\n ArrayModel\n ArrayModel\n\njulia> TypeIterator(m, BagModel) |> collect\n\n4-element Array{BagModel{T,Aggregation{2},ArrayModel{Dense{typeof(relu),Array{Float32,2},Array{Float32,1}}}} where T<:AbstractMillModel,1}:\n BagModel\n BagModel\n BagModel\n BagModel","category":"page"},{"location":"","page":"Mill – Multiple Instance Learning Library","title":"Mill – Multiple Instance Learning Library","text":"... and many others, see HierarchicalUtils.jl.","category":"page"},{"location":"#Default-aggregation-values","page":"Mill – Multiple Instance Learning Library","title":"Default aggregation values","text":"","category":"section"},{"location":"","page":"Mill – Multiple Instance Learning Library","title":"Mill – Multiple Instance Learning Library","text":"With the latest version of Mill, it is also possible to work with missing data, replacing a missing bag with a default constant value, and even to learn this value as well. Everything is done automatically.","category":"page"},{"location":"#Representing-missing-values","page":"Mill – Multiple Instance Learning Library","title":"Representing missing values","text":"","category":"section"},{"location":"","page":"Mill – Multiple Instance Learning Library","title":"Mill – Multiple Instance Learning Library","text":"The library currently support two ways to represent bags with missing values. First one represent missing data using missing as a = BagNode(missing, [0:-1]) while the second as an empty vector as a = BagNode(zero(4,0), [0:-1]).  While off the shelf the library supports both approaches transparently, the difference is mainly when one uses getindex, and therefore there is a switch Mill.emptyismissing(false), which is by default false. Let me demonstrate the difference.","category":"page"},{"location":"","page":"Mill – Multiple Instance Learning Library","title":"Mill – Multiple Instance Learning Library","text":"julia> a = BagNode(ArrayNode(rand(3,2)), [1:2, 0:-1])\nBagNode with 2 bag(s)\n  └── ArrayNode(3, 2)\n\njulia> Mill.emptyismissing(false);\n\njulia> a[2].data\nArrayNode(3, 0)\n\njulia> Mill.emptyismissing(true)\ntrue\n\njulia> a[2].data\nmissing","category":"page"},{"location":"","page":"Mill – Multiple Instance Learning Library","title":"Mill – Multiple Instance Learning Library","text":"The advantage of the first approach, default, is that types are always the same, which is nice to the compiler (and Zygote). The advantage of the latter is that it is more compact and nicer.","category":"page"},{"location":"#References","page":"Mill – Multiple Instance Learning Library","title":"References","text":"","category":"section"},{"location":"","page":"Mill – Multiple Instance Learning Library","title":"Mill – Multiple Instance Learning Library","text":"<a name=\"cit1\"><b>1</b></a> Discriminative models for multi-instance problems with tree-structure, Tomáš Pevný, Petr Somol, 2016, https://arxiv.org/abs/1703.02868","category":"page"},{"location":"","page":"Mill – Multiple Instance Learning Library","title":"Mill – Multiple Instance Learning Library","text":"<a name=\"cit2\"><b>2</b></a> Using Neural Network Formalism to Solve Multiple-Instance Problems, Tomáš Pevný, Petr Somol, 2016, https://arxiv.org/abs/1609.07257. ","category":"page"},{"location":"","page":"Mill – Multiple Instance Learning Library","title":"Mill – Multiple Instance Learning Library","text":"<a name=\"cit3\"><b>3</b></a> Approximation capability of neural networks on sets of probability measures and tree-structured data, Tomáš Pevný, Vojtěch Kovařík, 2019, https://openreview.net/forum?id=HklJV3A9Ym","category":"page"},{"location":"","page":"Mill – Multiple Instance Learning Library","title":"Mill – Multiple Instance Learning Library","text":"<a name=\"cit4\"><b>4</b></a> Solving the multiple instance problem with axis-parallel rectangles, Dietterich, Thomas G., Richard H. Lathrop, and Tomás Lozano-Pérez, 1997","category":"page"},{"location":"","page":"Mill – Multiple Instance Learning Library","title":"Mill – Multiple Instance Learning Library","text":"<a name=\"cit5\"><b>5</b></a> Deep sets, Zaheer, Manzil, et al., 2017,","category":"page"},{"location":"datanodes/#Architecture-of-Data-nodes","page":"Architecture of Data nodes","title":"Architecture of Data nodes","text":"","category":"section"},{"location":"datanodes/","page":"Architecture of Data nodes","title":"Architecture of Data nodes","text":"DataNodes are lightweight wrappers around data, such as Array, DataFrames, etc. Their primary purpose is to allow their nesting. It is possible to create subsets using getindex and concatenate them using cat. Note that internally, nodes are concatenated using reduce(catobs, ...). Similarly getindex is redirected to subset. This is needed such that operations over Matrices, Vectors, and DataFrames are consistent with the library.","category":"page"},{"location":"datanodes/","page":"Architecture of Data nodes","title":"Architecture of Data nodes","text":"Let's take a look on an example on a simple ArrayNode holding a matrix or a vectors.","category":"page"},{"location":"datanodes/","page":"Architecture of Data nodes","title":"Architecture of Data nodes","text":"ArrayNode has a simple structure holding only Array, which is considered the data and optionally some metadata, which can be literally anything.","category":"page"},{"location":"datanodes/","page":"Architecture of Data nodes","title":"Architecture of Data nodes","text":"struct ArrayNode{A,C} <: AbstractNode\n    data::A\n    metadata::C\nend","category":"page"},{"location":"datanodes/","page":"Architecture of Data nodes","title":"Architecture of Data nodes","text":"ArrayNode had overloaded a getindex to support indexing. But the getindex just calls subset(x::ArrayNode, idxs), which is used to correctly slice arrays according to the last dimension.","category":"page"},{"location":"datanodes/","page":"Architecture of Data nodes","title":"Architecture of Data nodes","text":"This mean that if you want to define your own DataNode, in order to be compatible with the rest of the library it has to implement subset and reduce(::typeof{catobs}, Vector{T}) where {T<:YourType}","category":"page"},{"location":"datanodes/#An-Example-of-how-to-add-a-custom-datatype-–-a-container-for-unix-pathnames-struct","page":"Architecture of Data nodes","title":"An Example of how to add a custom datatype –- a container for unix pathnames struct","text":"","category":"section"},{"location":"datanodes/","page":"Architecture of Data nodes","title":"Architecture of Data nodes","text":"We give it a twist, such that the extractor will be part of the model definition, which is going to be cute.","category":"page"},{"location":"datanodes/","page":"Architecture of Data nodes","title":"Architecture of Data nodes","text":"Let's start by defining the structure holding pathnames, supporting nobs joining of two structures and indexing into the structure. A last touch is to extend the pretty printing.","category":"page"},{"location":"datanodes/","page":"Architecture of Data nodes","title":"Architecture of Data nodes","text":"struct PathNode{S<:AbstractString,C} <: AbstractNode\n    data::Vector{S}\n    metadata::C\nend\n\nPathNode(data::Vector{S}) where {S<:AbstractString} = PathNode(data, nothing)\n\nBase.ndims(x::PathNode) = Colon()\nStatsBase.nobs(a::PathNode) = length(a.data)\nStatsBase.nobs(a::PathNode, ::Type{ObsDim.Last}) = nobs(a)\n\nfunction Base.reduce(::typeof(Mill.catobs), as::Vector{T}) where {T<:PathNode}\n    data = reduce(vcat, [x.data for x in as])\n    metadata = reduce(catobs, [a.metadata for a in as])\n    PathNode(data, metadata)\nend\n\nBase.getindex(x::PathNode, i::VecOrRange) = PathNode(subset(x.data, i), subset(x.metadata, i))\n\nMill.dsprint(io::IO, n::PathNode; pad=[], s=\"\", tr=false) = paddedprint(io, \"PathNode$(size(n.data))$(tr_repr(s, tr))\")","category":"page"},{"location":"datanodes/","page":"Architecture of Data nodes","title":"Architecture of Data nodes","text":"Similarly, we define a ModelNode which will be a counterpart processing the data. Note that the part of the ModelNode is a function which converts the pathanme string to Matrix (or other Mill structures). Again, we add a support for pretty printing.","category":"page"},{"location":"datanodes/","page":"Architecture of Data nodes","title":"Architecture of Data nodes","text":"struct PathModel{T,F} <: AbstractMillModel\n    m::T\n    path2mill::F\nend\n\nFlux.@functor PathModel\n\n(m::PathModel)(x::PathNode)  = m.m(m.path2mill(x))\n\nfunction Mill.modelprint(io::IO, m::PathModel; pad=[], s=\"\", tr=false)\n    c = COLORS[(length(pad)%length(COLORS))+1]\n    paddedprint(io, \"PathModel$(tr_repr(s, tr))\\n\", color=c)\n    paddedprint(io, \"  └── \", color=c, pad=pad)\n    modelprint(io, m.m, pad=[pad; (c, \"      \")])\nend","category":"page"},{"location":"datanodes/","page":"Architecture of Data nodes","title":"Architecture of Data nodes","text":"Finally, let's define function path2mill, which converts a list of strings to Mill internal structure.","category":"page"},{"location":"datanodes/","page":"Architecture of Data nodes","title":"Architecture of Data nodes","text":"function path2mill(s::String)\n\tss = String.(split(s, \"/\"))\n\tBagNode(ArrayNode(Mill.NGramMatrix(ss, 3, 256, 2053)), AlignedBags([1:length(ss)]))\nend\n\npath2mill(ss::Vector{S}) where {S<:AbstractString} = reduce(catobs, map(path2mill, ss))\npath2mill(ds::PathNode) = path2mill(ds.data)\n","category":"page"},{"location":"datanodes/","page":"Architecture of Data nodes","title":"Architecture of Data nodes","text":"And then, let's test the solution","category":"page"},{"location":"datanodes/","page":"Architecture of Data nodes","title":"Architecture of Data nodes","text":"ds = PathNode([\"/etc/passwd\", \"/home/tonda/.bashrc\"])\npm = PathModel(reflectinmodel(path2mill(ds), d -> Dense(d, 10, relu)), path2mill)\npm(ds).data","category":"page"},{"location":"datanodes/","page":"Architecture of Data nodes","title":"Architecture of Data nodes","text":"A final touch would be to overload the reflectinmodel as","category":"page"},{"location":"datanodes/","page":"Architecture of Data nodes","title":"Architecture of Data nodes","text":"\nfunction Mill.reflectinmodel(ds::PathNode, args...)\n\tpm = reflectinmodel(path2mill(ds), args...)\n\tPathModel(pm, path2mill)\nend\n","category":"page"},{"location":"datanodes/","page":"Architecture of Data nodes","title":"Architecture of Data nodes","text":"which can make it seamless","category":"page"},{"location":"datanodes/","page":"Architecture of Data nodes","title":"Architecture of Data nodes","text":"ds = PathNode([\"/etc/passwd\", \"/home/tonda/.bashrc\"])\npm = reflectinmodel(ds, d -> Dense(d, 10, relu))\npm(ds).data","category":"page"}]
}
