var documenterSearchIndex = {"docs":
[{"location":"manual/more_on_nodes/","page":"More on nodes","title":"More on nodes","text":"using Mill","category":"page"},{"location":"manual/more_on_nodes/#More-on-nodes","page":"More on nodes","title":"More on nodes","text":"","category":"section"},{"location":"manual/more_on_nodes/#Node-nesting","page":"More on nodes","title":"Node nesting","text":"","category":"section"},{"location":"manual/more_on_nodes/","page":"More on nodes","title":"More on nodes","text":"The main advantage of the Mill library is that it allows to arbitrarily nest and cross-product BagModels, as described in Theorem 5 in Tomáš Pevný , Vojtěch Kovařík  (2019). In other words, instances themselves may be represented in much more complex way than in the BagNode and BagModel example.","category":"page"},{"location":"manual/more_on_nodes/","page":"More on nodes","title":"More on nodes","text":"Let's start the demonstration by nesting two MIL problems. The outer MIL model contains three samples (outer-level bags), whose instances are (inner-level) bags themselves. The first outer-level bag contains one inner-level bag problem with two inner-level instances, the second outer-level bag contains two inner-level bags with total of three inner-level instances, and finally the third outer-level bag contains two inner bags with four instances:","category":"page"},{"location":"manual/more_on_nodes/","page":"More on nodes","title":"More on nodes","text":"ds = BagNode(BagNode(ArrayNode(randn(4, 10)),\n                     [1:2, 3:4, 5:5, 6:7, 8:10]),\n             [1:1, 2:3, 4:5])","category":"page"},{"location":"manual/more_on_nodes/","page":"More on nodes","title":"More on nodes","text":"Here is one example of a model, which is appropriate for this hierarchy:","category":"page"},{"location":"manual/more_on_nodes/","page":"More on nodes","title":"More on nodes","text":"using Flux: Dense, Chain, relu\nm = BagModel(\n        BagModel(\n            ArrayModel(Dense(4, 3, relu)),   \n            SegmentedMeanMax(3),\n            ArrayModel(Dense(7, 3, relu))),\n        SegmentedMeanMax(3),\n        ArrayModel(Chain(Dense(7, 3, relu), Dense(3, 2))))","category":"page"},{"location":"manual/more_on_nodes/","page":"More on nodes","title":"More on nodes","text":"and can be directly applied to obtain a result:","category":"page"},{"location":"manual/more_on_nodes/","page":"More on nodes","title":"More on nodes","text":"m(ds)","category":"page"},{"location":"manual/more_on_nodes/","page":"More on nodes","title":"More on nodes","text":"Here we again make use of the property that even if each instance is represented with an arbitrarily complex structure, we always obtain a vector representation after applying instance model im, regardless of the complexity of im and ds.data:","category":"page"},{"location":"manual/more_on_nodes/","page":"More on nodes","title":"More on nodes","text":"m.im(ds.data)","category":"page"},{"location":"manual/more_on_nodes/","page":"More on nodes","title":"More on nodes","text":"In one final example we demonstrate a complex model consisting of all types of nodes introduced so far:","category":"page"},{"location":"manual/more_on_nodes/","page":"More on nodes","title":"More on nodes","text":"ds = BagNode(ProductNode((BagNode(ArrayNode(randn(4, 10)),\n                                  [1:2, 3:4, 5:5, 6:7, 8:10]),\n                          ArrayNode(randn(3, 5)),\n                          BagNode(BagNode(ArrayNode(randn(2, 30)),\n                                          [i:i+1 for i in 1:2:30]),\n                                  [1:3, 4:6, 7:9, 10:12, 13:15]),\n                          ArrayNode(randn(2, 5)))),\n             [1:1, 2:3, 4:5])","category":"page"},{"location":"manual/more_on_nodes/","page":"More on nodes","title":"More on nodes","text":"Instead of defining a model manually, we make use of Model Reflection, another Mill.jl functionality, which simplifies model creation:","category":"page"},{"location":"manual/more_on_nodes/","page":"More on nodes","title":"More on nodes","text":"m = reflectinmodel(ds)\nm(ds)","category":"page"},{"location":"manual/more_on_nodes/#Node-conveniences","page":"More on nodes","title":"Node conveniences","text":"","category":"section"},{"location":"manual/more_on_nodes/","page":"More on nodes","title":"More on nodes","text":"To make the handling of data and model hierarchies easier, Mill.jl provides several tools. Let's setup some data:","category":"page"},{"location":"manual/more_on_nodes/","page":"More on nodes","title":"More on nodes","text":"AN = ArrayNode(Float32.([1 2 3 4; 5 6 7 8]))\nAM = reflectinmodel(AN)\nBN = BagNode(AN, [1:1, 2:3, 4:4])\nBM = reflectinmodel(BN)\nPN = ProductNode((a=ArrayNode(Float32.([1 2 3; 4 5 6])), b=BN))\nPM = reflectinmodel(PN)","category":"page"},{"location":"manual/more_on_nodes/#nobs","page":"More on nodes","title":"nobs","text":"","category":"section"},{"location":"manual/more_on_nodes/","page":"More on nodes","title":"More on nodes","text":"nobs method from StatsBase.jl returns a number of samples from the current level point of view. This number usually increases as we go down the tree when BagNodes are involved, as each bag may contain more than one instance.","category":"page"},{"location":"manual/more_on_nodes/","page":"More on nodes","title":"More on nodes","text":"using StatsBase: nobs\nnobs(AN)\nnobs(BN)\nnobs(PN)","category":"page"},{"location":"manual/more_on_nodes/#Indexing-and-Slicing","page":"More on nodes","title":"Indexing and Slicing","text":"","category":"section"},{"location":"manual/more_on_nodes/","page":"More on nodes","title":"More on nodes","text":"Indexing in Mill.jl operates on the level of observations:","category":"page"},{"location":"manual/more_on_nodes/","page":"More on nodes","title":"More on nodes","text":"AN[1]\nnobs(ans)\nBN[2]\nnobs(ans)\nPN[3]\nnobs(ans)\nAN[[1, 4]]\nnobs(ans)\nBN[1:2]\nnobs(ans)\nPN[[2, 3]]\nnobs(ans)\nPN[Int[]]\nnobs(ans)","category":"page"},{"location":"manual/more_on_nodes/","page":"More on nodes","title":"More on nodes","text":"This may be useful for creating minibatches and their permutations.","category":"page"},{"location":"manual/more_on_nodes/","page":"More on nodes","title":"More on nodes","text":"Note that apart from the perhaps apparent recurrent effect, this operation requires other implicit actions, such as properly recomputing bag indices:","category":"page"},{"location":"manual/more_on_nodes/","page":"More on nodes","title":"More on nodes","text":"BN.bags\nBN[[1, 3]].bags","category":"page"},{"location":"manual/more_on_nodes/#Concatenation","page":"More on nodes","title":"Concatenation","text":"","category":"section"},{"location":"manual/more_on_nodes/","page":"More on nodes","title":"More on nodes","text":"catobs concatenates several datasets (trees) together:","category":"page"},{"location":"manual/more_on_nodes/","page":"More on nodes","title":"More on nodes","text":"catobs(AN[1], AN[4])\ncatobs(BN[3], BN[[2, 1]])\ncatobs(PN[[1, 2]], PN[3:4]) == PN","category":"page"},{"location":"manual/more_on_nodes/","page":"More on nodes","title":"More on nodes","text":"Again, the effect is recurrent and everything is appropriately recomputed:","category":"page"},{"location":"manual/more_on_nodes/","page":"More on nodes","title":"More on nodes","text":"BN.bags\ncatobs(BN[3], BN[[1]]).bags","category":"page"},{"location":"manual/more_on_nodes/","page":"More on nodes","title":"More on nodes","text":"ukn: More tips\nFor more tips for handling datasets and models, see External tools","category":"page"},{"location":"references/#References","page":"References","title":"References","text":"","category":"section"},{"location":"references/","page":"References","title":"References","text":"","category":"page"},{"location":"examples/graphs/","page":"GNNs in 16 lines","title":"GNNs in 16 lines","text":"using Mill\n\nusing LightGraphs, GraphPlot, Cairo, Compose\n\ng = barabasi_albert(10, 3, 2)\nfor e in [(1, 2), (1, 3), (2, 4), (2, 5), (3, 5),\n          (3, 6), (5, 7), (6, 5), (6, 8), (7, 8)]\n    add_edge!(g, e...)\nend\n\ngp = gplot(g; nodelabel=range('a'; length=nv(g)))\ndraw(SVG(\"graph.svg\"), gp)","category":"page"},{"location":"examples/graphs/#GNNs-in-16-lines","page":"GNNs in 16 lines","title":"GNNs in 16 lines","text":"","category":"section"},{"location":"examples/graphs/","page":"GNNs in 16 lines","title":"GNNs in 16 lines","text":"As has been mentioned in Šimon Mandlík , Tomáš Pevný  (2020), multiple instance learning is an essential piece for implementing message passing inference over graphs, the main concept behind spatial Graph Neural Networks (GNNs). It is straightforward and quick to achieve this with Mill.jl. We begin with some dependencies:","category":"page"},{"location":"examples/graphs/","page":"GNNs in 16 lines","title":"GNNs in 16 lines","text":"using Flux, LightGraphs, Statistics","category":"page"},{"location":"examples/graphs/","page":"GNNs in 16 lines","title":"GNNs in 16 lines","text":"Let's assume a graph g, in this case created by barabasi_albert function from LightGraphs.jl","category":"page"},{"location":"examples/graphs/","page":"GNNs in 16 lines","title":"GNNs in 16 lines","text":"g = barabasi_albert(10, 3, 2)","category":"page"},{"location":"examples/graphs/","page":"GNNs in 16 lines","title":"GNNs in 16 lines","text":"(Image: )","category":"page"},{"location":"examples/graphs/","page":"GNNs in 16 lines","title":"GNNs in 16 lines","text":"Furthermore, let's assume that each vertex is described by seven features stored in a matrix X:","category":"page"},{"location":"examples/graphs/","page":"GNNs in 16 lines","title":"GNNs in 16 lines","text":"X = ArrayNode(randn(Float32, 7, 10))","category":"page"},{"location":"examples/graphs/","page":"GNNs in 16 lines","title":"GNNs in 16 lines","text":"We use ScatteredBags from Mill.jl to encode neighbors of each vertex. In other words, each vertex is described by a bag of its neighbors. This information is conveniently stored in fadjlist field of g, therefore the bags can be constructed as:","category":"page"},{"location":"examples/graphs/","page":"GNNs in 16 lines","title":"GNNs in 16 lines","text":"b = ScatteredBags(g.fadjlist)","category":"page"},{"location":"examples/graphs/","page":"GNNs in 16 lines","title":"GNNs in 16 lines","text":"Finally, we create two models. First model called lift will pre-process the description of vertices to some latent space for message passing, and the second one will realize the message passing itself, which we will call mp:","category":"page"},{"location":"examples/graphs/","page":"GNNs in 16 lines","title":"GNNs in 16 lines","text":"lift = reflectinmodel(X, d -> Dense(d, 10), d -> SegmentedMean(d))\nU = lift(X)\nmp = reflectinmodel(BagNode(U, b), d -> Dense(d, 10), d -> SegmentedMean(d))","category":"page"},{"location":"examples/graphs/","page":"GNNs in 16 lines","title":"GNNs in 16 lines","text":"Notice that BagNode(U, b) now essentially encodes vertex features as well as the adjacency matrix. This also means that one step of message passing algorithm can be realized as:","category":"page"},{"location":"examples/graphs/","page":"GNNs in 16 lines","title":"GNNs in 16 lines","text":"Y = mp(BagNode(U, b))","category":"page"},{"location":"examples/graphs/","page":"GNNs in 16 lines","title":"GNNs in 16 lines","text":"and it is differentiable, which can be verified by executing:","category":"page"},{"location":"examples/graphs/","page":"GNNs in 16 lines","title":"GNNs in 16 lines","text":"gradient(() -> sum(sin.(mp(BagNode(U, b)).data)), Flux.params(mp))","category":"page"},{"location":"examples/graphs/","page":"GNNs in 16 lines","title":"GNNs in 16 lines","text":"If we put everything together, the GNN implementation is implemented in the following block of code (16 lines of mostly sugar).","category":"page"},{"location":"examples/graphs/","page":"GNNs in 16 lines","title":"GNNs in 16 lines","text":"struct GNN{L,M, R}\n    lift::L\n    mp::M\n    m::R\nend\n\nFlux.@functor GNN\n\nfunction mpstep(m::GNN, U::ArrayNode, bags, n)\n    n == 0 && return(U)\n    mpstep(m, m.mp(BagNode(U, bags)), bags, n - 1)\nend\n\nfunction (m::GNN)(g, X, n)\n    U = m.lift(X)\n    bags = Mill.ScatteredBags(g.fadjlist)\n    o = mpstep(m, U, bags, n)\n    m.m(vcat(mean(o.data, dims = 2), maximum(o.data, dims = 2)))\nend\n\nnothing # hide","category":"page"},{"location":"examples/graphs/","page":"GNNs in 16 lines","title":"GNNs in 16 lines","text":"As it is the case with whole Mill.jl, even this graph neural network is properly integrated with Flux.jl ecosystem and suports automatic differentiation:","category":"page"},{"location":"examples/graphs/","page":"GNNs in 16 lines","title":"GNNs in 16 lines","text":"zd = 10\nf(d) = Chain(Dense(d, zd, relu), Dense(zd, zd))\nagg(d) = SegmentedMeanMax(d)\ngnn = GNN(reflectinmodel(X, f, agg),\n          BagModel(f(zd), agg(zd), f(2zd + 1)),\n          f(2zd)) \nnothing # hide","category":"page"},{"location":"examples/graphs/","page":"GNNs in 16 lines","title":"GNNs in 16 lines","text":"gnn(g, X, 5)\ngradient(() -> gnn(g, X, 5) |> sum, Flux.params(gnn))","category":"page"},{"location":"examples/graphs/","page":"GNNs in 16 lines","title":"GNNs in 16 lines","text":"The above implementation is surprisingly general, as it supports an arbitrarily rich description of vertices. For simplicity, we used only vectors in X, however, any Mill.jl hierarchy is applicable.","category":"page"},{"location":"examples/graphs/","page":"GNNs in 16 lines","title":"GNNs in 16 lines","text":"To put different weights on edges, one can use WeightedBagNodes instead.","category":"page"},{"location":"tools/hierarchical/","page":"HierarchicalUtils.jl","title":"HierarchicalUtils.jl","text":"using Mill\nusing StatsBase: nobs","category":"page"},{"location":"tools/hierarchical/#HierarchicalUtils.jl","page":"HierarchicalUtils.jl","title":"HierarchicalUtils.jl","text":"","category":"section"},{"location":"tools/hierarchical/","page":"HierarchicalUtils.jl","title":"HierarchicalUtils.jl","text":"Mill.jl uses HierarchicalUtils.jl which brings a lot of additional features.","category":"page"},{"location":"tools/hierarchical/","page":"HierarchicalUtils.jl","title":"HierarchicalUtils.jl","text":"using HierarchicalUtils","category":"page"},{"location":"tools/hierarchical/#Printing","page":"HierarchicalUtils.jl","title":"Printing","text":"","category":"section"},{"location":"tools/hierarchical/","page":"HierarchicalUtils.jl","title":"HierarchicalUtils.jl","text":"For instance, Base.show with text/plain MIME calls HierarchicalUtils.printtree:","category":"page"},{"location":"tools/hierarchical/","page":"HierarchicalUtils.jl","title":"HierarchicalUtils.jl","text":"ds = BagNode(ProductNode((BagNode(ArrayNode(randn(4, 10)),\n                                  [1:2, 3:4, 5:5, 6:7, 8:10]),\n                          ArrayNode(randn(3, 5)),\n                          BagNode(BagNode(ArrayNode(randn(2, 30)),\n                                          [i:i+1 for i in 1:2:30]),\n                                  [1:3, 4:6, 7:9, 10:12, 13:15]),\n                          ArrayNode(randn(2, 5)))),\n             [1:1, 2:3, 4:5])\nprinttree(ds; htrunc=3)","category":"page"},{"location":"tools/hierarchical/","page":"HierarchicalUtils.jl","title":"HierarchicalUtils.jl","text":"This can be used to print a non-truncated version of a model:","category":"page"},{"location":"tools/hierarchical/","page":"HierarchicalUtils.jl","title":"HierarchicalUtils.jl","text":"printtree(ds)","category":"page"},{"location":"tools/hierarchical/#Traversal-encoding","page":"HierarchicalUtils.jl","title":"Traversal encoding","text":"","category":"section"},{"location":"tools/hierarchical/","page":"HierarchicalUtils.jl","title":"HierarchicalUtils.jl","text":"Callling with trav=true enables convenient traversal functionality with string indexing:","category":"page"},{"location":"tools/hierarchical/","page":"HierarchicalUtils.jl","title":"HierarchicalUtils.jl","text":"m = reflectinmodel(ds)\nprinttree(m; trav=true)","category":"page"},{"location":"tools/hierarchical/","page":"HierarchicalUtils.jl","title":"HierarchicalUtils.jl","text":"This way any node in the model tree is swiftly accessible, which may come in handy when inspecting model parameters or simply deleting/replacing/inserting nodes to tree (for instance when constructing adversarial samples). All tree nodes are accessible by indexing with the traversal code:.","category":"page"},{"location":"tools/hierarchical/","page":"HierarchicalUtils.jl","title":"HierarchicalUtils.jl","text":"m[\"Y\"]","category":"page"},{"location":"tools/hierarchical/","page":"HierarchicalUtils.jl","title":"HierarchicalUtils.jl","text":"The following two approaches give the same result:","category":"page"},{"location":"tools/hierarchical/","page":"HierarchicalUtils.jl","title":"HierarchicalUtils.jl","text":"m[\"Y\"] === m.im.ms[1]","category":"page"},{"location":"tools/hierarchical/#Counting-functions","page":"HierarchicalUtils.jl","title":"Counting functions","text":"","category":"section"},{"location":"tools/hierarchical/","page":"HierarchicalUtils.jl","title":"HierarchicalUtils.jl","text":"Other functions provided by HierarchicalUtils.jl:","category":"page"},{"location":"tools/hierarchical/","page":"HierarchicalUtils.jl","title":"HierarchicalUtils.jl","text":"nnodes(ds)\nnleafs(ds)\nNodeIterator(ds) |> collect\nNodeIterator(ds, m) |> collect\nLeafIterator(ds) |> collect\nTypeIterator(BagModel, m) |> collect\nPredicateIterator(x -> nobs(x) ≥ 10, ds) |> collect","category":"page"},{"location":"tools/hierarchical/","page":"HierarchicalUtils.jl","title":"HierarchicalUtils.jl","text":"For the complete showcase of possibilites, refer to HierarchicalUtils.jl and this notebook","category":"page"},{"location":"manual/custom/","page":"Custom nodes","title":"Custom nodes","text":"using Mill\nusing Flux","category":"page"},{"location":"manual/custom/#Adding-custom-nodes","page":"Custom nodes","title":"Adding custom nodes","text":"","category":"section"},{"location":"manual/custom/","page":"Custom nodes","title":"Custom nodes","text":"Mill.jl data nodes are lightweight wrappers around data, such as Array, DataFrame, and others. When implementing custom nodes, it is recommended to equip them with the following functionality to fit better into Mill.jl environment:","category":"page"},{"location":"manual/custom/","page":"Custom nodes","title":"Custom nodes","text":"allow nesting (if needed)\nimplement getindex to obtain subsets of observations. For this purpose, Mill.jl defines a subset function for common datatypes, which can be used.\nallow concatenation of nodes with catobs. Optionally, implement reduce(catobs, ...) as well to avoid excessive compilations if a number of arguments will vary a lot\ndefine a specialized method for nobs\nregister the custom node with HierarchicalUtils.jl to obtain pretty printing, iterators and other functionality","category":"page"},{"location":"manual/custom/#Unix-path-example","page":"Custom nodes","title":"Unix path example","text":"","category":"section"},{"location":"manual/custom/","page":"Custom nodes","title":"Custom nodes","text":"Let's define one custom node type for representing pathnames in Unix and one custom model type for processing it. We'll start by defining the structure holding pathnames:","category":"page"},{"location":"manual/custom/","page":"Custom nodes","title":"Custom nodes","text":"struct PathNode{S <: AbstractString, C} <: AbstractNode\n    data::Vector{S}\n    metadata::C\nend\n\nPathNode(data::Vector{S}) where {S <: AbstractString} = PathNode(data, nothing)\nnothing # hide","category":"page"},{"location":"manual/custom/","page":"Custom nodes","title":"Custom nodes","text":"We will support nobs:","category":"page"},{"location":"manual/custom/","page":"Custom nodes","title":"Custom nodes","text":"import StatsBase: nobs\nBase.ndims(x::PathNode) = Colon()\nnobs(a::PathNode) = length(a.data)\nnothing # hide","category":"page"},{"location":"manual/custom/","page":"Custom nodes","title":"Custom nodes","text":"concatenation:","category":"page"},{"location":"manual/custom/","page":"Custom nodes","title":"Custom nodes","text":"function Base.reduce(::typeof(catobs), as::Vector{T}) where {T <: PathNode}\n    PathNode(data, reduce(vcat, data.(as)), reduce(catobs, metadata.(as)))\nend","category":"page"},{"location":"manual/custom/","page":"Custom nodes","title":"Custom nodes","text":"and indexing:","category":"page"},{"location":"manual/custom/","page":"Custom nodes","title":"Custom nodes","text":"Base.getindex(x::PathNode, i::Mill.VecOrRange{<:Int}) = PathNode(subset(x.data, i), subset(x.metadata, i))","category":"page"},{"location":"manual/custom/","page":"Custom nodes","title":"Custom nodes","text":"The last touch is to add the definition needed by HierarchicalUtils.jl:","category":"page"},{"location":"manual/custom/","page":"Custom nodes","title":"Custom nodes","text":"import HierarchicalUtils\nHierarchicalUtils.NodeType(::Type{<:PathNode}) = HierarchicalUtils.LeafNode()\nHierarchicalUtils.noderepr(n::PathNode) = \"PathNode ($(nobs(n)) obs)\"\nnothing # hide","category":"page"},{"location":"manual/custom/","page":"Custom nodes","title":"Custom nodes","text":"Now, we are ready to create the first PathNode:","category":"page"},{"location":"manual/custom/","page":"Custom nodes","title":"Custom nodes","text":"ds = PathNode([\"/etc/passwd\", \"/home/tonda/.bashrc\"])","category":"page"},{"location":"manual/custom/","page":"Custom nodes","title":"Custom nodes","text":"Similarly, we define a ModelNode type which will be a counterpart processing the data:","category":"page"},{"location":"manual/custom/","page":"Custom nodes","title":"Custom nodes","text":"struct PathModel{T, F} <: AbstractMillModel\n    m::T\n    path2mill::F\nend\n\nFlux.@functor PathModel","category":"page"},{"location":"manual/custom/","page":"Custom nodes","title":"Custom nodes","text":"Note that the part of the ModelNode is a function which converts the pathname string to a Mill.jl structure. For simplicity, we use a trivial NGramMatrix representation in this example and define path2mill as follows:","category":"page"},{"location":"manual/custom/","page":"Custom nodes","title":"Custom nodes","text":"function path2mill(s::String)\n    ss = String.(split(s, \"/\"))\n    BagNode(ArrayNode(Mill.NGramMatrix(ss, 3)), AlignedBags([1:length(ss)]))\nend\n\npath2mill(ss::Vector{S}) where {S <: AbstractString} = reduce(catobs, map(path2mill, ss))\npath2mill(ds::PathNode) = path2mill(ds.data)\nnothing # hide","category":"page"},{"location":"manual/custom/","page":"Custom nodes","title":"Custom nodes","text":"Now we define how the model node is applied:","category":"page"},{"location":"manual/custom/","page":"Custom nodes","title":"Custom nodes","text":"(m::PathModel)(x::PathNode) = m.m(m.path2mill(x))","category":"page"},{"location":"manual/custom/","page":"Custom nodes","title":"Custom nodes","text":"And again, define everything needed in HierarchicalUtils.jl:","category":"page"},{"location":"manual/custom/","page":"Custom nodes","title":"Custom nodes","text":"HierarchicalUtils.NodeType(::Type{<:PathModel}) = HierarchicalUtils.LeafNode()\nHierarchicalUtils.noderepr(n::PathModel) = \"PathModel\"","category":"page"},{"location":"manual/custom/","page":"Custom nodes","title":"Custom nodes","text":"Let's test that everything works:","category":"page"},{"location":"manual/custom/","page":"Custom nodes","title":"Custom nodes","text":"pm = PathModel(reflectinmodel(path2mill(ds)), path2mill)\npm(ds).data","category":"page"},{"location":"manual/custom/","page":"Custom nodes","title":"Custom nodes","text":"The final touch would be to overload the reflectinmodel as","category":"page"},{"location":"manual/custom/","page":"Custom nodes","title":"Custom nodes","text":"function Mill.reflectinmodel(ds::PathNode, args...)\n    pm = reflectinmodel(path2mill(ds), args...)\n    PathModel(pm, path2mill)\nend","category":"page"},{"location":"manual/custom/","page":"Custom nodes","title":"Custom nodes","text":"which makes things even easier","category":"page"},{"location":"manual/custom/","page":"Custom nodes","title":"Custom nodes","text":"pm = reflectinmodel(ds)\npm(ds).data","category":"page"},{"location":"motivation/#Motivation","page":"Motivation","title":"Motivation","text":"","category":"section"},{"location":"motivation/","page":"Motivation","title":"Motivation","text":"In this section, we provide a short introduction into (hierarchical) multi instance learning. A much more detailed overview of this subject can be found in Šimon Mandlík , Tomáš Pevný  (2020).","category":"page"},{"location":"motivation/#What-is-a-Multiple-instance-learning-problem?","page":"Motivation","title":"What is a Multiple instance learning problem?","text":"","category":"section"},{"location":"motivation/","page":"Motivation","title":"Motivation","text":"In Multiple Instance Learning (MIL), also Multi-Instance Learning, the sample bmx is a set of vectors (or matrices) x_1ldotsx_l, where x_i in mathbbR^d. As a result, order does not matter, which makes MIL problems different from sequences. In MIL parlance, sample bmx is also called a bag and its elements x_1 ldots x_2 instances. MIL problems have been introduced in Thomas G. Dietterich , Richard H. Lathrop , Tomás Lozano-Pérez  (1997), and extended and generalized in a series of works Tomáš Pevný , Petr Somol  (2017), Tomáš Pevný , Petr Somol  (2016), Tomáš Pevný , Vojtěch Kovařík  (2019). The most comprehensive introduction known to authors is Šimon Mandlík , Tomáš Pevný  (2020).","category":"page"},{"location":"motivation/","page":"Motivation","title":"Motivation","text":"Why are MIL problems relevant? Since the seminal paper from Ronald A Fisher  (1936), the majority of machine learning problems deals with problems like the one shown below:[1]","category":"page"},{"location":"motivation/","page":"Motivation","title":"Motivation","text":"[1]: Iris flower data set","category":"page"},{"location":"motivation/","page":"Motivation","title":"Motivation","text":"(Image: )","category":"page"},{"location":"motivation/","page":"Motivation","title":"Motivation","text":"where the input sample bmx is a vector (or generally speaking any tensor) of a fixed dimension containing various measurements of the specimen.","category":"page"},{"location":"motivation/","page":"Motivation","title":"Motivation","text":"Most of the time, a skilled botanist is able to identify a specimen not by making use of any measuring device, but by visual or tactile inspection of its stem, leaves and blooms. For different species, different parts of the flower may need to be examined for indicators. At the same time, many species may have nearly identical-looking leaves or blooms, therefore, one needs to step back, consider the whole picture, and appropriately combine lower-level observations into high-level conclusions about the given specimen.","category":"page"},{"location":"motivation/","page":"Motivation","title":"Motivation","text":"If we want to use such more elaborate description of the Iris flower using fixed size structures, we will have a hard time, because every specimen can have a different amounts of leaves or blooms (or they may be completely missing). This means that to use the usual fixed dimension paradigm, we have to either somehow select a single leaf (blossom) and extract features from them, or design procedures for aggregating such features over whole sets, so that the output has fixed dimension. This is clearly undesirable. Mill.jl a framework that seamlessly deals with these challenges in data representation.","category":"page"},{"location":"motivation/#Hierarchical-Multiple-Instance-Learning","page":"Motivation","title":"Hierarchical Multiple Instance Learning","text":"","category":"section"},{"location":"motivation/","page":"Motivation","title":"Motivation","text":"In Hierarchical Multiple Instance Learning (HMIL) the input may consists of not only sets, but also sets of sets and Cartesian Products of these structures. Returning to the previous Iris flower example, a specimen can be represented like this for HMIL:","category":"page"},{"location":"motivation/","page":"Motivation","title":"Motivation","text":"(Image: )","category":"page"},{"location":"motivation/","page":"Motivation","title":"Motivation","text":"The only stem is represented by vector bmx_s encoding its distinctive properties such as shape, color, structure or texture. Next, we inspect all blooms. Each of the blooms may have distinctive discriminative signs, therefore, we describe all three in vectors bmx_b_1 bmx_b_2 bmx_b_3, one vector for each bloom, and group them to a set. Finally, bmx_u represents the only flower which has not blossomed. Likewise, we could describe all leaves of the specimen if any were present. Here we assume that each specimen of the considered species has only one stem, but may have multiple flowers or leaves. Hence, all blooms and buds are represented as unordered sets of vectors as opposed to stem representation, which consists of only one vector.","category":"page"},{"location":"motivation/","page":"Motivation","title":"Motivation","text":"How does MIL models cope with variability in numbers of flowers and leaves? Each MIL model consists of two feed-forward neural networks with an element-wise aggregation operator like mean (or maximum) sandwiched between them. Denoting those feed-forward networks (FFNs) as f_1 and f_2, the output of the model applied to a bag is calculated for example as f_2 left(frac1lsum_i=1^l f_1(x_i) right) if we use mean as an aggregation function.","category":"page"},{"location":"motivation/","page":"Motivation","title":"Motivation","text":"The HMIL model corresponding to the Iris example above would comprise two FFNs and an aggregation to convert set of leafs to a single vector, and another two FFNs and an aggregation to convert set of blossoms to a single vector. These two outputs would be concatenated with a description of a stem, which would be fed to yet another FFN providing the final output. Since the whole scheme is differentiable, we can compute gradients and use any available gradient-based method to optimize the whole model at once using only labels on the level of output[2].","category":"page"},{"location":"motivation/","page":"Motivation","title":"Motivation","text":"[2]: Some methods for MIL problems require instance-level labels as well, which are not always available.","category":"page"},{"location":"motivation/","page":"Motivation","title":"Motivation","text":"The Mill.jl library simplifies implementation of machine learning problems using (H)MIL representation. In theory, it can represent any problem that can be represented in JSONs. That is why we have created a separate tool, JsonGrinder.jl, which helps with processing JSON documents for learning.","category":"page"},{"location":"motivation/","page":"Motivation","title":"Motivation","text":"In Tomáš Pevný , Vojtěch Kovařík  (2019), authors have further extended the Universal approximation theorem to MIL problems, their Cartesian products, and nested MIL problems, i.e. a case where instances of one bag are in fact bags again.","category":"page"},{"location":"motivation/#Relation-to-Graph-Neural-Networks","page":"Motivation","title":"Relation to Graph Neural Networks","text":"","category":"section"},{"location":"motivation/","page":"Motivation","title":"Motivation","text":"HMIL problems can be seen as a special subset of general graphs. They differ in two important ways:","category":"page"},{"location":"motivation/","page":"Motivation","title":"Motivation","text":"In general graphs, vertices are of a small number of semantic type, whereas in HMIL problems, the number of semantic types of vertices is much higher (it is helpful to think about HMIL problems as about those for which JSON is a natural representation).\nThe computational graph of HMIL is a tree, which introduces assumption that there exist an efficient inference. Contrary, in general graphs (with loops) there is no efficient inference and one has to resort to message passing (Loopy belief propagation).\nOne update message in loopy belief propagation can be viewed as a MIL problem, as it has to produce a vector based on infomation inthe neighborhood, which can contain arbitrary number of vertices.","category":"page"},{"location":"motivation/#Difference-to-sequences","page":"Motivation","title":"Difference to sequences","text":"","category":"section"},{"location":"motivation/","page":"Motivation","title":"Motivation","text":"The major difference is that instances in bag are not ordered in any way. This means that if a sequence (abc) should be treated as a set, then the output of a function f should be the same for any permutation, i.e. f(abc) = f(cba) = f(bac) = ldots. This property has a dramatic implication on the computational complexity. Sequences are typically modeled using Recurrent Neural Networks (RNNs), where the output is calculated as f(abc) = g(a g(b g(c))) (slightly abusing the notation). During optimization, a gradient of g needs to be calculated recursively, giving raise to infamous vanishing / exploding gradient problems. In constrast, (H)MIL models calculate the output as f(frac13(g(a) + g(b) + g(c))) (slightly abusing notation again), which means that the gradient of g can be calculated in parallel and not recurrently. ","category":"page"},{"location":"examples/dag/","page":"DAGs","title":"DAGs","text":"using Mill\n\nusing LightGraphs, GraphPlot, Cairo, Compose\n\ng = SimpleDiGraph(8)\nfor e in [(1, 2), (1, 3), (2, 4), (2, 5), (3, 5),\n          (3, 6), (5, 7), (6, 5), (6, 8), (7, 8)]\n    add_edge!(g, e...)\nend\n\ngp = gplot(g; nodelabel=range('a'; length=nv(g)))\ndraw(SVG(\"dag.svg\"), gp)","category":"page"},{"location":"examples/dag/#DAGs","page":"DAGs","title":"DAGs","text":"","category":"section"},{"location":"examples/dag/","page":"DAGs","title":"DAGs","text":"using Flux, Zygote, LightGraphs","category":"page"},{"location":"examples/dag/","page":"DAGs","title":"DAGs","text":"Imagine a data/knowledge base represented in a form of a directed acyclic graph (DAG), where a vertex would be modelled based on its parents (and their parents), but not on its descendants. We will make one assumption (common in graphical models) that two children are independent given their parent or, in other words, once we have access to the data or inferred values of the parent, we do not have to inspect its other children.","category":"page"},{"location":"examples/dag/","page":"DAGs","title":"DAGs","text":"For example, in the graph below, when we infer some value for vertex e, we ignore vertices d, g, and h:","category":"page"},{"location":"examples/dag/","page":"DAGs","title":"DAGs","text":"(Image: )","category":"page"},{"location":"examples/dag/","page":"DAGs","title":"DAGs","text":"Firstly, we define a new type for our graph that would be able to store a structure of the graph together with vertex features:","category":"page"},{"location":"examples/dag/","page":"DAGs","title":"DAGs","text":"struct DagGraph{G <: SimpleDiGraph,T}\n    g::G\n    vertex_features::T\nend\n\nnothing # hide","category":"page"},{"location":"examples/dag/","page":"DAGs","title":"DAGs","text":"Then, we specify a model type:","category":"page"},{"location":"examples/dag/","page":"DAGs","title":"DAGs","text":"struct DagModel{M}\n    m::M\n    od::Int\nend\n\nFlux.@functor DagModel\n\nnothing # hide","category":"page"},{"location":"examples/dag/","page":"DAGs","title":"DAGs","text":"Here, m is a function realizing one step of the message passing procedure, and od is output dimension.","category":"page"},{"location":"examples/dag/","page":"DAGs","title":"DAGs","text":"In the course of calculating the value of vertex e, it may happen that for some vertices we will have to compute their value multiple times (for example for c, once when reached from e and once from f). Ideally, we would like to have a cache of already calculated values, which is difficult to do when autodifferentiating with Zygote as it does not support setindex operation. However, since the cache is assigned only once this can be realized through Zygote.buffer. We begin by initializing the cache:","category":"page"},{"location":"examples/dag/","page":"DAGs","title":"DAGs","text":"initcache(g, k) = [Zygote.Buffer(zeros(Float32, k, 1)) for _ in 1:nv(g)]\nZygote.@nograd initcache\nnothing # hide","category":"page"},{"location":"examples/dag/","page":"DAGs","title":"DAGs","text":"To get the already computed value of a vertex when using the model, we just delegate the question to cache as ","category":"page"},{"location":"examples/dag/","page":"DAGs","title":"DAGs","text":"function (model::DagModel)(g::DagGraph, i)\n    cache = initcache(g.g, model.od)\n    ArrayNode(getfromcache!(cache, g, model, i))\nend\n\n(model::DagModel)(g::SimpleDiGraph, vertex_features, i) = model(DagGraph(g, vertex_features), i)\n\nnothing # hide","category":"page"},{"location":"examples/dag/","page":"DAGs","title":"DAGs","text":"which means that the getfromcache! will do all the heavy lifting. It turns out that this function just has to check if the value in cache has been already calculated. If not, it will calculate the value (applying model on millvertex!) and freeze the calculated item in cache:","category":"page"},{"location":"examples/dag/","page":"DAGs","title":"DAGs","text":"function getfromcache!(cache, g::DagGraph, model::DagModel, i::Int)\n    cache[i].freeze && return(copy(cache[i]))\n    ds = millvertex!(cache, g, model, i)\n    cache[i][:] = model.m(ds).data\n    return(copy(cache[i]))\nend\n\nfunction getfromcache!(cache, g::DagGraph, model::DagModel, ii::Vector{Int}) \n    reduce(catobs, [getfromcache!(cache, g, model, i) for i in ii])\nend\n\nnothing # hide","category":"page"},{"location":"examples/dag/","page":"DAGs","title":"DAGs","text":"And what does millvertex! function do? It just takes the representation of ancestors (from cache) and put them together:","category":"page"},{"location":"examples/dag/","page":"DAGs","title":"DAGs","text":"function millvertex!(cache, g::DagGraph, model::DagModel, i)\n    ProductNode((neighbours = millneighbors!(cache, g, model, i), \n      vertex = vertex_features[i])\n    )\nend\nnothing # hide","category":"page"},{"location":"examples/dag/","page":"DAGs","title":"DAGs","text":"The last missing piece is millneighbors! definition:","category":"page"},{"location":"examples/dag/","page":"DAGs","title":"DAGs","text":"function millneighbors!(cache, g::DagGraph, model::DagModel, ii::Vector{Int})\n    isempty(ii) && return(BagNode(missing, [0:-1]))\n    xs = [getfromcache!(cache, g, model, i) for i in  ii]\n    BagNode(ArrayNode(reduce(catobs, xs)), [1:length(xs)])\nend\n\nmillneighbors!(cache, g::DagGraph, model::DagModel, i::Int) = millneighbors!(cache, g, model, inneighbors(g.g, i))\n\nZygote.@nograd LightGraphs.inneighbors\nnothing # hide","category":"page"},{"location":"examples/dag/","page":"DAGs","title":"DAGs","text":"Note that this recursive approach is not the most efficient way to implement this. It would be better to spent a little time with graphs to identify sets of vertices that can be processed in parallel and for which all ancestors are known. But this was a fun little exercise.","category":"page"},{"location":"examples/jsons/","page":"Processing JSONs","title":"Processing JSONs","text":"<p align=\"center\">\n    <a href=\"https://github.com/pevnak/JsonGrinder.jl\">\n        <img src=\"https://raw.githubusercontent.com/pevnak/JsonGrinder.jl/master/docs/src/assets/logo.svg\" alt=\"JsonGrinder.jl logo\"/>\n    </a>\n</p>","category":"page"},{"location":"examples/jsons/#Processing-JSONs","page":"Processing JSONs","title":"Processing JSONs","text":"","category":"section"},{"location":"examples/jsons/","page":"Processing JSONs","title":"Processing JSONs","text":"Processing JSONs is actually one of the main motivations for building Mill.jl. As a matter of fact, with Mill.jl one is now able to process a set of valid JSON documents that follow the same meta schema. JsonGrinder.jl is a library that helps with infering the schema and other steps in the pipeline. For some examples, please refer to its documentation.","category":"page"},{"location":"api/#API","page":"API","title":"API","text":"","category":"section"},{"location":"api/#Contents","page":"API","title":"Contents","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"Pages = [\"api.md\"]","category":"page"},{"location":"api/#Index","page":"API","title":"Index","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"Pages = [\"api.md\"]","category":"page"},{"location":"api/#General","page":"API","title":"General","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"Mill.emptyismissing\nMill.emptyismissing!\nMill.bagcount\nMill.bagcount!\nMill.string_start_code\nMill.string_start_code!\nMill.string_end_code\nMill.string_end_code!","category":"page"},{"location":"api/#Mill.emptyismissing","page":"API","title":"Mill.emptyismissing","text":"Mill.emptyismissing()\n\nGet the current value of the emptyismissing parameter.\n\nSee also: Mill.emptyismissing!,  Empty bags (docs).\n\n\n\n\n\n","category":"function"},{"location":"api/#Mill.emptyismissing!","page":"API","title":"Mill.emptyismissing!","text":"Mill.emptyismissing!(::Bool)\n\nSet the new value to the emptyismissing parameter.\n\nSee also: Mill.emptyismissing,  Empty bags (docs).\n\n\n\n\n\n","category":"function"},{"location":"api/#Mill.bagcount","page":"API","title":"Mill.bagcount","text":"Mill.bagcount()\n\nGet the current value of the bagcount parameter.\n\nSee also: Mill.bagcount!,  Bag count (docs).\n\n\n\n\n\n","category":"function"},{"location":"api/#Mill.bagcount!","page":"API","title":"Mill.bagcount!","text":"Mill.bagcount!(Bool)\n\nSet the new value to the bagcount parameter.\n\nSee also: Mill.bagcount,  Bag count (docs).\n\n\n\n\n\n","category":"function"},{"location":"api/#Mill.string_start_code","page":"API","title":"Mill.string_start_code","text":"Mill.string_start_code()\n\nGet the current value of the string_start_code parameter used as a code point of the abstract string-start character. The default value of the parameter is 0x02, which corresponds to the STX character in ASCII encoding.\n\nSee also: Mill.string_start_code!, Mill.string_end_code,     Mill.string_end_code!, Strings (docs).\n\n\n\n\n\n","category":"function"},{"location":"api/#Mill.string_start_code!","page":"API","title":"Mill.string_start_code!","text":"Mill.string_start_code!(c::UInt8)\n\nSet the new value to the string_start_code parameter used as a code point of the abstract string-start character to c. The default value of the parameter is 0x02, which corresponds to the STX character in ASCII encoding.\n\nSee also: Mill.string_start_code, Mill.string_end_code,     Mill.string_end_code!, Strings (docs).\n\n\n\n\n\n","category":"function"},{"location":"api/#Mill.string_end_code","page":"API","title":"Mill.string_end_code","text":"Mill.string_end_code()\n\nGet the current value of the string_end_code parameter used as a code point of the abstract string-end character. The default value of the parameter is 0x03, which corresponds to the ETX character in ASCII encoding.\n\nSee also: Mill.string_end_code!, Mill.string_start_code,     Mill.string_start_code!, Strings (docs).\n\n\n\n\n\n","category":"function"},{"location":"api/#Mill.string_end_code!","page":"API","title":"Mill.string_end_code!","text":"Mill.string_end_code!(c::UInt8)\n\nSet the new value to the string_end_code parameter used as a code point of the abstract string-end character to c. The default value of the parameter is 0x03, which corresponds to the ETX character in ASCII encoding.\n\nSee also: Mill.string_end_code, Mill.string_start_code,     Mill.string_start_code!, Strings (docs).\n\n\n\n\n\n","category":"function"},{"location":"api/#Bags","page":"API","title":"Bags","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"AbstractBags\nAlignedBags\nAlignedBags()\nAlignedBags(::UnitRange{<:Integer}...)\nAlignedBags(::Vector{<:Integer})\nScatteredBags\nScatteredBags()\nScatteredBags(::Vector{<:Integer})\nlength2bags\nbags\nremapbags\nadjustbags","category":"page"},{"location":"api/#Mill.AbstractBags","page":"API","title":"Mill.AbstractBags","text":"AbstractBags{T}\n\nSupertype for structures storing indices of type T of bags' instances in BagNodes.\n\n\n\n\n\n","category":"type"},{"location":"api/#Mill.AlignedBags","page":"API","title":"Mill.AlignedBags","text":"AlignedBags{T <: Integer} <: AbstractBags{T}\n\nAlignedBags struct stores indices of bags' instances in one or more UnitRange{T}s. This is only possible if instances of every bag are stored in one contiguous block.\n\nSee also: ScatteredBags.\n\n\n\n\n\n","category":"type"},{"location":"api/#Mill.AlignedBags-Tuple{}","page":"API","title":"Mill.AlignedBags","text":"AlignedBags()\n\nConstruct new AlignedBags struct containing no bags.\n\nExamples\n\njulia> AlignedBags()\nAlignedBags{Int64}(UnitRange{Int64}[])\n\n\n\n\n\n","category":"method"},{"location":"api/#Mill.AlignedBags-Tuple{Vararg{UnitRange{var\"#s23\"} where var\"#s23\"<:Integer,N} where N}","page":"API","title":"Mill.AlignedBags","text":"AlignedBags(bags::UnitRange{<:Integer}...)\n\nConstruct new AlignedBags struct from bags in arguments.\n\nExamples\n\njulia> AlignedBags(1:3, 4:8)\nAlignedBags{Int64}(UnitRange{Int64}[1:3, 4:8])\n\n\n\n\n\n","category":"method"},{"location":"api/#Mill.AlignedBags-Tuple{Array{var\"#s23\",1} where var\"#s23\"<:Integer}","page":"API","title":"Mill.AlignedBags","text":"AlignedBags(k::Vector{<:Integer})\n\nConstruct new AlignedBags struct from Vector k specifying the index of the bag each instance belongs to. Throws ArgumentError if this is not possible.\n\nExamples\n\njulia> AlignedBags([2, 2, 1, 1, 1, 3])\nAlignedBags{Int64}(UnitRange{Int64}[1:2, 3:5, 6:6])\n\n\n\n\n\n","category":"method"},{"location":"api/#Mill.ScatteredBags","page":"API","title":"Mill.ScatteredBags","text":"ScatteredBags{T <: Integer} <: AbstractBags{T}\n\nScatteredBags struct stores indices of bags' instances that are not necessarily contiguous.\n\nSee also: AlignedBags.\n\n\n\n\n\n","category":"type"},{"location":"api/#Mill.ScatteredBags-Tuple{}","page":"API","title":"Mill.ScatteredBags","text":"ScatteredBags()\n\nConstruct new ScatteredBags struct containing no bags.\n\nExamples\n\njulia> ScatteredBags()\nScatteredBags{Int64}(Array{Int64,1}[])\n\n\n\n\n\n","category":"method"},{"location":"api/#Mill.ScatteredBags-Tuple{Array{var\"#s23\",1} where var\"#s23\"<:Integer}","page":"API","title":"Mill.ScatteredBags","text":"ScatteredBags(k::Vector{<:Integer})\n\nConstruct new ScatteredBags struct from Vector k specifying the index of the bag each instance belongs to.\n\nExamples\n\njulia> ScatteredBags([2, 2, 1, 1, 1, 3])\nScatteredBags{Int64}([[3, 4, 5], [1, 2], [6]])\n\n\n\n\n\n","category":"method"},{"location":"api/#Mill.length2bags","page":"API","title":"Mill.length2bags","text":"length2bags(ls::Vector{<:Integer})\n\nConvert lengths of bags given in ls to AlignedBags with contiguous blocks.\n\nExamples\n\njulia> length2bags([1, 3, 2])\nAlignedBags{Int64}(UnitRange{Int64}[1:1, 2:4, 5:6])\n\nSee also: AlignedBags.\n\n\n\n\n\n","category":"function"},{"location":"api/#Mill.bags","page":"API","title":"Mill.bags","text":"bags(k::Vector{<:Integer})\nbags(k::Vector{T}) where T <: UnitRange{<:Integer}\nbags(b::AbstractBags)\n\nConstruct an AbstractBags structure that is most suitable for the input (AlignedBags if possible, ScatteredBags otherwise).\n\nExamples\n\njulia> bags([2, 2, 3, 1])\nAlignedBags{Int64}(UnitRange{Int64}[1:2, 3:3, 4:4])\n\njulia> bags([2, 3, 1, 2])\nScatteredBags{Int64}([[3], [1, 4], [2]])\n\njulia> bags([1:3, 4:5])\nAlignedBags{Int64}(UnitRange{Int64}[1:3, 4:5])\n\njulia> bags(ScatteredBags())\nScatteredBags{Int64}(Array{Int64,1}[])\n\nSee also: AlignedBags, ScatteredBags.\n\n\n\n\n\n","category":"function"},{"location":"api/#Mill.remapbags","page":"API","title":"Mill.remapbags","text":"remapbags(b::AbstractBags, idcs::VecOrRange{<:Integer}) -> (rb, I)\n\nSelect a subset of bags in b corresponding to indices idcs and remap instance indices appropriately. Return new bags rb as well as a Vector of remapped instances I.\n\nExamples\n\njulia> remapbags(AlignedBags([1:1, 2:3, 4:5]), [1, 3])\n(AlignedBags{Int64}(UnitRange{Int64}[1:1, 2:3]), [1, 4, 5])\n\njulia> remapbags(ScatteredBags([[1,3], [2], Int[]]), [2])\n(ScatteredBags{Int64}([[1]]), [2])\n\n\n\n\n\n","category":"function"},{"location":"api/#Mill.adjustbags","page":"API","title":"Mill.adjustbags","text":"adjustbags(b::AlignedBags, mask::AbstractVector{Bool})\n\nRemove indices of instances brom bags b and remap the remaining instances accordingly.\n\nExamples\n\njulia> adjustbags(AlignedBags([1:2, 0:-1, 3:4]), [false, false, true, true])\nAlignedBags{Int64}(UnitRange{Int64}[0:-1, 0:-1, 1:2])\n\n\n\n\n\n","category":"function"},{"location":"api/#Data-nodes","page":"API","title":"Data nodes","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"AbstractNode\nAbstractProductNode\nAbstractBagNode\nMill.data\nMill.metadata","category":"page"},{"location":"api/#Mill.AbstractNode","page":"API","title":"Mill.AbstractNode","text":"AbstractNode\n\nSupertype for any structure representing a data node.\n\n\n\n\n\n","category":"type"},{"location":"api/#Mill.AbstractProductNode","page":"API","title":"Mill.AbstractProductNode","text":"AbstractProductNode <: AbstractNode\n\nSupertype for any structure representing a data node implementing a Cartesian product of data in subtrees.\n\n\n\n\n\n","category":"type"},{"location":"api/#Mill.AbstractBagNode","page":"API","title":"Mill.AbstractBagNode","text":"AbstractBagNode <: AbstractNode\n\nSupertype for any structure representing a data node that implements a multi-instance learning problem.\n\n\n\n\n\n","category":"type"},{"location":"api/#Mill.data","page":"API","title":"Mill.data","text":"Mill.data(n::AbstractNode)\n\nReturn data stored in node n.\n\nExamples\n\njulia> Mill.data(ArrayNode([1 2; 3 4], \"metadata\"))\n2×2 Array{Int64,2}:\n 1  2\n 3  4\n\njulia> Mill.data(BagNode(ArrayNode([1 2; 3 4]), bags([1:3, 4:4]), \"metadata\"))\n2×2 ArrayNode{Array{Int64,2},Nothing}:\n 1  2\n 3  4\n\nSee also: Mill.metadata\n\n\n\n\n\n","category":"function"},{"location":"api/#Mill.metadata","page":"API","title":"Mill.metadata","text":"Mill.metadata(n::AbstractNode)\n\nReturn metadata stored in node n.\n\nExamples\n\njulia> Mill.metadata(ArrayNode([1 2; 3 4], \"metadata\"))\n\"metadata\"\n\njulia> Mill.metadata(BagNode(ArrayNode([1 2; 3 4]), bags([1:3, 4:4]), \"metadata\"))\n\"metadata\"\n\nSee also: Mill.data\n\n\n\n\n\n","category":"function"},{"location":"manual/missing/","page":"Missing data","title":"Missing data","text":"using Mill","category":"page"},{"location":"manual/missing/#Missing-data","page":"Missing data","title":"Missing data","text":"","category":"section"},{"location":"manual/missing/","page":"Missing data","title":"Missing data","text":"One detail that was left out so far is how Mill.jl handles incomplete or missing data. This phenomenon is nowadays ubiquitous in many data sources and occurs due to:","category":"page"},{"location":"manual/missing/","page":"Missing data","title":"Missing data","text":"a high price of obtaining a (part of) observation\ninformation being unreachable due to privacy reasons\na gradual change in the definition of data being gathered\na faulty collection process","category":"page"},{"location":"manual/missing/","page":"Missing data","title":"Missing data","text":"and many other possible reasons. At the same time, it is wasteful to throw away the incomplete observations altogether. Thanks to the hierarchical structure of both samples and models, we can still process samples with missing information fragments at various levels of abstraction. Problems of this type can be categorized into 3 not necessarily separate types:","category":"page"},{"location":"manual/missing/","page":"Missing data","title":"Missing data","text":"Missing parts of raw-data in a leaf ArrayNode\nEmpty bags with no instances in a BagNode\nAnd entire key missing in a ProductNode","category":"page"},{"location":"manual/missing/","page":"Missing data","title":"Missing data","text":"At the moment, Mill.jl is capable of handling the first two cases. The solution always involves an additional vector of parameters (denoted always by ψ) that are used during the model evaluation to substitute the missing values. Parameters ψ can be either fixed or learned during training. Everything is done automatically.","category":"page"},{"location":"manual/missing/#Empty-bags","page":"Missing data","title":"Empty bags","text":"","category":"section"},{"location":"manual/missing/","page":"Missing data","title":"Missing data","text":"It may happen that some bags in the datasets are empty by definition or no associated instances were obtained during data collection. Recall, that an empty bag is specified as empty range 0:-1 in case of AlignedBags and as an empty vector [] when ScatteredBags are used:","category":"page"},{"location":"manual/missing/","page":"Missing data","title":"Missing data","text":"empty_bags_1 = AlignedBags([1:2, 0:-1, 3:5, 0:-1])\nempty_bags_2 = ScatteredBags([[1, 2], [], [3, 4, 5], []])","category":"page"},{"location":"manual/missing/","page":"Missing data","title":"Missing data","text":"To obtain the vector representation for a bag, be it for dircetly predicting some value or using it to represent some higher-level structures, we need to deal with these empty bags. This is done in Bag aggregation. Each AggregationOperator carries a vector of parameters ψ, initialized to zeros upon creation:","category":"page"},{"location":"manual/missing/","page":"Missing data","title":"Missing data","text":"a = SegmentedSumMax(2)","category":"page"},{"location":"manual/missing/","page":"Missing data","title":"Missing data","text":"When we evaluate any BagModel, these values are used to compute output for empty bags instead of the aggregation itself. See the demo below:","category":"page"},{"location":"manual/missing/","page":"Missing data","title":"Missing data","text":"an = ArrayNode(randn(Float32, 2, 5))\nds = BagNode(an, empty_bags_2)\nm = BagModel(identity, a, identity)\nm(ds)","category":"page"},{"location":"manual/missing/","page":"Missing data","title":"Missing data","text":"Vector ψ is learnable and therefore after training will contain a suitable representation of an empty bag for the given problem.","category":"page"},{"location":"manual/missing/","page":"Missing data","title":"Missing data","text":"When a BagNode is entirely empty, it can be constructed with missing instead of a matrix wrapped in an ArrayNode:","category":"page"},{"location":"manual/missing/","page":"Missing data","title":"Missing data","text":"bn1 = BagNode(ArrayNode(rand(3, 4)), [1:4])\nbn2 = BagNode(missing, [0:-1])","category":"page"},{"location":"manual/missing/","page":"Missing data","title":"Missing data","text":"and everything will work as expected. For example, we can concatenate these two:","category":"page"},{"location":"manual/missing/","page":"Missing data","title":"Missing data","text":"x = catobs(bn1, bn2)","category":"page"},{"location":"manual/missing/","page":"Missing data","title":"Missing data","text":"Notice, that the resulting ArrayNode has still the same dimension as ArrayNode inside bn1. The emptiness of bn2 is stored in bags:","category":"page"},{"location":"manual/missing/","page":"Missing data","title":"Missing data","text":"x.bags","category":"page"},{"location":"manual/missing/","page":"Missing data","title":"Missing data","text":"The second element BagNode can be obtained again by indexing:","category":"page"},{"location":"manual/missing/","page":"Missing data","title":"Missing data","text":"bn1 == x[2]","category":"page"},{"location":"manual/missing/","page":"Missing data","title":"Missing data","text":"Even though this approach of using missing for data field in BagNodes is the most accurate from the semantic point of view, it may cause excessive compilation, as the types will be different. Therefore, if this happens in multiple places in the sample tree, it may be better to instead use an empty matrix for type consistency:","category":"page"},{"location":"manual/missing/","page":"Missing data","title":"Missing data","text":"BagNode(ArrayNode(zeros(3, 0)), [0:-1])","category":"page"},{"location":"manual/missing/","page":"Missing data","title":"Missing data","text":"How indexing behaves with respect to this issue depends on a global switch (off by default) and  can be changed with the Mill.emptyismissing! function:","category":"page"},{"location":"manual/missing/","page":"Missing data","title":"Missing data","text":"a = BagNode(ArrayNode(rand(3, 2)), [1:2, 0:-1, 0:-1])\na[2:3].data\nMill.emptyismissing!(true)\na[2:3].data\nmissing","category":"page"},{"location":"manual/missing/","page":"Missing data","title":"Missing data","text":"Mill.emptyismissing!(false)","category":"page"},{"location":"manual/missing/#PostImputingMatrix","page":"Missing data","title":"PostImputingMatrix","text":"","category":"section"},{"location":"manual/missing/","page":"Missing data","title":"Missing data","text":"Storing missing strings in NGramMatrix is straightforward:","category":"page"},{"location":"manual/missing/","page":"Missing data","title":"Missing data","text":"missing_ngrams = NGramMatrix([\"foo\", missing, \"bar\"], 3, 256, 5)","category":"page"},{"location":"manual/missing/","page":"Missing data","title":"Missing data","text":"When some values of categorical variables are missing, Mill.jl defines a new type for representation:","category":"page"},{"location":"manual/missing/","page":"Missing data","title":"Missing data","text":"missing_categorical = maybehotbatch([missing, 2, missing], 1:5)","category":"page"},{"location":"manual/missing/","page":"Missing data","title":"Missing data","text":"MaybeHotMatrix behaves similarly as OneHotMatrix from Flux.jl, but it supports possibly missing values. In case when no values are missing it behaves exactly like OneHotMatrix:","category":"page"},{"location":"manual/missing/","page":"Missing data","title":"Missing data","text":"maybehotbatch([5, 2, 1], 1:5)","category":"page"},{"location":"manual/missing/","page":"Missing data","title":"Missing data","text":"MaybeHotMatrix behaves like AbstractMatrix and supports left multiplication again:","category":"page"},{"location":"manual/missing/","page":"Missing data","title":"Missing data","text":"missing_categorical::AbstractMatrix{Union{Bool, Missing}}","category":"page"},{"location":"manual/missing/","page":"Missing data","title":"Missing data","text":"However, multiplying these matrices with missing data leads into missing data in the output.","category":"page"},{"location":"manual/missing/","page":"Missing data","title":"Missing data","text":"W = rand(2, 5)\nW * missing_ngrams\nW * missing_categorical","category":"page"},{"location":"manual/missing/","page":"Missing data","title":"Missing data","text":"Consequently, gradient can't be computed and any model can't be trained.","category":"page"},{"location":"manual/missing/","page":"Missing data","title":"Missing data","text":"ukn: Model debugging\nFlux gradient call returns an error like Output should be scalar; gradients are not defined for output missing when attempted on missing result. In a similar fashion as having NaNs in a model, this signifies that some missing input is not treated anywhere in the model and it propagates up. Generally speaking, it is recommended to deal with missing values as soon as possible (on the leaf level) so that they do not propagate and cause type instabilities.","category":"page"},{"location":"manual/missing/","page":"Missing data","title":"Missing data","text":"PostImputingMatrix is a solution for this. It can be constructed as follows:","category":"page"},{"location":"manual/missing/","page":"Missing data","title":"Missing data","text":"A = PostImputingMatrix(W)","category":"page"},{"location":"manual/missing/","page":"Missing data","title":"Missing data","text":"Matrix W is stored inside and A creates one vector of parameters ψ of length size(W, 1) on top of that. Suddenly, multiplication automagically works:","category":"page"},{"location":"manual/missing/","page":"Missing data","title":"Missing data","text":"A * missing_ngrams\nA * missing_categorical","category":"page"},{"location":"manual/missing/","page":"Missing data","title":"Missing data","text":"What happens under the hood is that whenever A encounters a missing column in the matrix, it fills in values from ψ after the multiplication is performed (effectively replacing all missing values in the result of multiplying with W, but implemented more efficiently). Vector ψ can be learned during training as well and everything works out of the box.","category":"page"},{"location":"manual/missing/#PreImputingMatrix","page":"Missing data","title":"PreImputingMatrix","text":"","category":"section"},{"location":"manual/missing/","page":"Missing data","title":"Missing data","text":"If we have to deal with inputs where some elements of input matrix are missing:","category":"page"},{"location":"manual/missing/","page":"Missing data","title":"Missing data","text":"X = [missing 1 2; 3 missing missing]","category":"page"},{"location":"manual/missing/","page":"Missing data","title":"Missing data","text":"we can make use of PreImputingMatrix:","category":"page"},{"location":"manual/missing/","page":"Missing data","title":"Missing data","text":"W = rand(1:2, 3, 2)\nA = PreImputingMatrix(W)","category":"page"},{"location":"manual/missing/","page":"Missing data","title":"Missing data","text":"As opposed to PostImputingMatrix, A now stores a vector of values ψ with length size(W, 2). When we use it for multiplication:","category":"page"},{"location":"manual/missing/","page":"Missing data","title":"Missing data","text":"A * X","category":"page"},{"location":"manual/missing/","page":"Missing data","title":"Missing data","text":"what happens is that when we perform a dot product of a row of A and a column of X, we first fill in values from ψ into the column before the multiplication is performed. Again, it is possible to compute gradients with respect to all three of W, ψ and X and therefore learn the appropriate default values in ψ from the data:","category":"page"},{"location":"manual/missing/","page":"Missing data","title":"Missing data","text":"using Flux\n\ngradient((A, X) -> sum(A * X), A, X)","category":"page"},{"location":"manual/missing/#Model-reflection-with-missing-values","page":"Missing data","title":"Model reflection with missing values","text":"","category":"section"},{"location":"manual/missing/","page":"Missing data","title":"Missing data","text":"Model reflection takes missing values and types into account and creates appropriate (sub)models to handle them:","category":"page"},{"location":"manual/missing/","page":"Missing data","title":"Missing data","text":"ds = ProductNode(ArrayNode.((missing_ngrams, missing_categorical, X)))\nm = reflectinmodel(ds)","category":"page"},{"location":"manual/missing/","page":"Missing data","title":"Missing data","text":"Here, [pre_imputing]Dense and [post_imputing]Dense are standard dense layers with a special matrix inside:","category":"page"},{"location":"manual/missing/","page":"Missing data","title":"Missing data","text":"dense = m.ms[1].m; typeof(dense.W)","category":"page"},{"location":"manual/missing/","page":"Missing data","title":"Missing data","text":"Inside Mill.jl we add a special definition Base.show for these types for compact printing.","category":"page"},{"location":"manual/missing/","page":"Missing data","title":"Missing data","text":"The reflectinmodel method use types to determine whether imputing is needed or not. Compare the following:","category":"page"},{"location":"manual/missing/","page":"Missing data","title":"Missing data","text":"reflectinmodel(ArrayNode(randn(2, 3)))\nreflectinmodel(ArrayNode([1.0 2.0 missing; 4.0 missing missing]))\nreflectinmodel(ArrayNode(Matrix{Union{Missing, Float64}}(randn(2, 3))))","category":"page"},{"location":"manual/missing/","page":"Missing data","title":"Missing data","text":"In the last case, the imputing type is returned even though there is no missing element in the matrix. Of course, the same applies to MaybeHot* types and NGramMatrix. This way, we can signify that even though there are no missing values in the available sample, we expect them to appear in the future and want our model compatible. If it is hard to determine this in advance a safe bet is to make all leaves in the model. The performance will not suffer because imputing types are as fast as their non-imputing counterparts on data not containing missing values and the only tradeoff is a slight increase in the number of parameters, some of which may never be used.","category":"page"},{"location":"examples/musk/musk/","page":"Musk","title":"Musk","text":"using Random; Random.seed!(42)\n\nusing Pkg\nold_path = Pkg.project().path\nPkg.activate(pwd())\nPkg.instantiate()","category":"page"},{"location":"examples/musk/musk/#Musk-dataset","page":"Musk","title":"Musk dataset","text":"","category":"section"},{"location":"examples/musk/musk/","page":"Musk","title":"Musk","text":"Musk dataset is a classic MIL problem of the field, introduced in Thomas G. Dietterich , Richard H. Lathrop , Tomás Lozano-Pérez  (1997). Below we demonstrate how to solve this problem using Mill.jl. The full example is also accessible here, as well as a Julia environment to run it.","category":"page"},{"location":"examples/musk/musk/","page":"Musk","title":"Musk","text":"For the demo, we load all dependencies:","category":"page"},{"location":"examples/musk/musk/","page":"Musk","title":"Musk","text":"using FileIO, JLD2, Statistics, Mill, Flux\nusing Flux: throttle, @epochs\nusing Mill: reflectinmodel\nusing Base.Iterators: repeated","category":"page"},{"location":"examples/musk/musk/","page":"Musk","title":"Musk","text":"and then load the dataset and transform it into a Mill.jl structure. The musk.jld2 file contains:","category":"page"},{"location":"examples/musk/musk/","page":"Musk","title":"Musk","text":"a matrix with features fMat:","category":"page"},{"location":"examples/musk/musk/","page":"Musk","title":"Musk","text":"fMat = load(\"musk.jld2\", \"fMat\")          # matrix with instances, each column is one sample","category":"page"},{"location":"examples/musk/musk/","page":"Musk","title":"Musk","text":"the id of sample (bag in MIL terminology) specifying to which each instance (column in fMat) belongs to:","category":"page"},{"location":"examples/musk/musk/","page":"Musk","title":"Musk","text":"bagids = load(\"musk.jld2\", \"bagids\")      # ties instances to bags","category":"page"},{"location":"examples/musk/musk/","page":"Musk","title":"Musk","text":"The resulting BagNode is a structure which holds (i) feature matrix and (ii) ranges identifying which columns in the feature matrix each bag spans. This representation ensures that feed-forward networks do not need to deal with bag boundaries and always process full continuous matrices:","category":"page"},{"location":"examples/musk/musk/","page":"Musk","title":"Musk","text":"ds = BagNode(ArrayNode(fMat), bagids)     # create a BagNode dataset","category":"page"},{"location":"examples/musk/musk/","page":"Musk","title":"Musk","text":"the label of each instance in y.  The label of a bag is a maximum of labels of its instances, i.e. one positive instance in a bag makes it positive:","category":"page"},{"location":"examples/musk/musk/","page":"Musk","title":"Musk","text":"y = load(\"musk.jld2\", \"y\")                # load labels\ny = map(i -> maximum(y[i]) + 1, ds.bags)  # create labels on bags\ny_oh = Flux.onehotbatch(y, 1:2)           # one-hot encoding","category":"page"},{"location":"examples/musk/musk/","page":"Musk","title":"Musk","text":"Once the data are in Mill.jl internal format, we will manually create a model. BagModel is designed to implement a basic multi-instance learning model utilizing two feed-forward networks with an aggregaton operator in between:","category":"page"},{"location":"examples/musk/musk/","page":"Musk","title":"Musk","text":"model = BagModel(\n    ArrayModel(Dense(166, 10, Flux.relu)),                      # model on the level of Flows\n    SegmentedMeanMax(10),                                       # aggregation\n    ArrayModel(Chain(Dense(21, 10, Flux.relu), Dense(10, 2))))  # model on the level of bags","category":"page"},{"location":"examples/musk/musk/","page":"Musk","title":"Musk","text":"Instances are first passed through a single layer with 10 neurons (input dimension is 166) with relu non-linearity, then we use mean and max aggregation functions simultaneously (for some problems, max is better then mean, therefore we use both), and then we use one layer with 10 neurons and relu nonlinearity followed by linear layer with 2 neurons (output dimension).","category":"page"},{"location":"examples/musk/musk/","page":"Musk","title":"Musk","text":"Let's check that forward pass works:","category":"page"},{"location":"examples/musk/musk/","page":"Musk","title":"Musk","text":"model(ds)","category":"page"},{"location":"examples/musk/musk/","page":"Musk","title":"Musk","text":"Since Mill.jl is entirely compatible with Flux.jl, we can use its cross-entropy loss function:","category":"page"},{"location":"examples/musk/musk/","page":"Musk","title":"Musk","text":"loss(ds, y_oh) = Flux.logitcrossentropy(model(ds).data, y_oh)","category":"page"},{"location":"examples/musk/musk/","page":"Musk","title":"Musk","text":"and run simple training procedure using its tooling:","category":"page"},{"location":"examples/musk/musk/","page":"Musk","title":"Musk","text":"evalcb = () -> @show(loss(ds, y_oh))\nopt = Flux.ADAM()\n@epochs 10 Flux.train!(loss, params(model), repeated((ds, y_oh), 1000), opt, cb=throttle(evalcb, 1))","category":"page"},{"location":"examples/musk/musk/","page":"Musk","title":"Musk","text":"We can also calculate training error, which should be not so surprisingly low:","category":"page"},{"location":"examples/musk/musk/","page":"Musk","title":"Musk","text":"mean(mapslices(argmax, model(ds).data, dims=1)' .!= y)","category":"page"},{"location":"examples/musk/musk/","page":"Musk","title":"Musk","text":"Pkg.activate(old_path)","category":"page"},{"location":"manual/nodes/","page":"Nodes","title":"Nodes","text":"using Mill","category":"page"},{"location":"manual/nodes/","page":"Nodes","title":"Nodes","text":"ukw: Tip\nIt is recommended to read the Motivation section first to understand the crucial ideas behind hierarchical multiple instance learning.","category":"page"},{"location":"manual/nodes/#Nodes","page":"Nodes","title":"Nodes","text":"","category":"section"},{"location":"manual/nodes/","page":"Nodes","title":"Nodes","text":"Mill.jl enables representation of arbitrarily complex tree-like hierarchies and appropriate models for these hierarchies. It defines two core abstract types:","category":"page"},{"location":"manual/nodes/","page":"Nodes","title":"Nodes","text":"AbstractNode which stores data on any level of abstraction and its subtypes can be further nested\nAbstractModelNode which helps to define a corresponding model. For each specific implementation of AbstractNode we have one or more specific AbstractModelNode(s) for processing it.","category":"page"},{"location":"manual/nodes/","page":"Nodes","title":"Nodes","text":"Below we will go through implementation of ArrayNode, BagNode and ProductNode together with their corresponding models. It is possible to define data and model nodes for more complex behaviors (see Custom Nodes), however, these three core types are already sufficient for a lot of tasks, for instance, representing any JSON document and using appropriate models to convert it to a vector represention or classify it (see TODO).","category":"page"},{"location":"manual/nodes/#ArrayNode-and-ArrayModel","page":"Nodes","title":"ArrayNode and ArrayModel","text":"","category":"section"},{"location":"manual/nodes/","page":"Nodes","title":"Nodes","text":"ArrayNode thinly wraps an array of features (specifically any subtype of AbstractArray):","category":"page"},{"location":"manual/nodes/","page":"Nodes","title":"Nodes","text":"X = Float32.([1 2 3 4; 5 6 7 8])\nAN = ArrayNode(X)","category":"page"},{"location":"manual/nodes/","page":"Nodes","title":"Nodes","text":"Similarly, ArrayModel wraps any function performing operation over this array. In example below, we wrap a feature matrix X and a Dense model from Flux.jl:","category":"page"},{"location":"manual/nodes/","page":"Nodes","title":"Nodes","text":"using Flux: Dense","category":"page"},{"location":"manual/nodes/","page":"Nodes","title":"Nodes","text":"f = Dense(2, 3)\nAM = ArrayModel(f)","category":"page"},{"location":"manual/nodes/","page":"Nodes","title":"Nodes","text":"We can apply the model now with AM(AN) to get another ArrayNode and verify that the feedforward layer f is really applied:","category":"page"},{"location":"manual/nodes/","page":"Nodes","title":"Nodes","text":"AM(AN)\nf(X) == AM(AN).data","category":"page"},{"location":"manual/nodes/","page":"Nodes","title":"Nodes","text":"ukn: Model outputs\nA convenient property of all Mill.jl models is that after applying them to a corresponding data node we always obtain an ArrayNode as output regardless of the type and complexity of the model. This becomes important later.","category":"page"},{"location":"manual/nodes/","page":"Nodes","title":"Nodes","text":"The most common interpretation of the data inside ArrayNodes is that each column contains features of one sample and therefore the node AN carries size(AN.data, 2) samples. In this sense, ArrayNodes wrap the standard machine learning problem, where each sample is represented with a vector, a matrix or a more general tensor of features. Alternatively, one can obtain a number of samples of any AbstractNode with nobs function from StatsBase.jl package:","category":"page"},{"location":"manual/nodes/","page":"Nodes","title":"Nodes","text":"using StatsBase: nobs\nnobs(AN)","category":"page"},{"location":"manual/nodes/#BagNode","page":"Nodes","title":"BagNode","text":"","category":"section"},{"location":"manual/nodes/","page":"Nodes","title":"Nodes","text":"BagNode is represents the standard multiple instance learning problem, that is, each sample is a bag containing an arbitrary number of instances. In the simplest case, each instance is a vector:","category":"page"},{"location":"manual/nodes/","page":"Nodes","title":"Nodes","text":"BN = BagNode(AN, [1:1, 2:3, 4:4])","category":"page"},{"location":"manual/nodes/","page":"Nodes","title":"Nodes","text":"where for simplicity we used AN from the previous example. Each BagNode carries data and bags fields:","category":"page"},{"location":"manual/nodes/","page":"Nodes","title":"Nodes","text":"BN.data\nBN.bags","category":"page"},{"location":"manual/nodes/","page":"Nodes","title":"Nodes","text":"Here, data can be an arbitrary AbstractNode storing representation of instances (ArrayNode in this case) and bags field contains information, which instances belong to which bag. In this specific case bn stores three bags (samples). The first one consists of a single instance {[1.0, 5.0]} (first column of AN), the second one of two instances {[2.0, 6.0], [3.0, 7.0]}, and the last one of a single instance {[4.0, 8.0]}. We can see that we deal with three top-level samples (bags):","category":"page"},{"location":"manual/nodes/","page":"Nodes","title":"Nodes","text":"nobs(BN)","category":"page"},{"location":"manual/nodes/","page":"Nodes","title":"Nodes","text":"whereas they are formed using four instances:","category":"page"},{"location":"manual/nodes/","page":"Nodes","title":"Nodes","text":"nobs(AN)","category":"page"},{"location":"manual/nodes/","page":"Nodes","title":"Nodes","text":"In Mill.jl, there two ways to store indices of the bag's instances:","category":"page"},{"location":"manual/nodes/","page":"Nodes","title":"Nodes","text":"* in `AlignedBags` structure, which accepts a `Vector` of `UnitRange`s and requires all bag's instances stored continuously:","category":"page"},{"location":"manual/nodes/","page":"Nodes","title":"Nodes","text":"AlignedBags([1:3, 4:4, 5:6])","category":"page"},{"location":"manual/nodes/","page":"Nodes","title":"Nodes","text":"* and in `ScatteredBags` structure, which accepts a `Vector` of `Vectors`s storing not necessarily contiguous indices:","category":"page"},{"location":"manual/nodes/","page":"Nodes","title":"Nodes","text":"ScatteredBags([[3, 2, 1], [4], [6, 5]])","category":"page"},{"location":"manual/nodes/","page":"Nodes","title":"Nodes","text":"The two examples above are semantically equivalent, as bags are unordered collections of instances. An empty bag with no instances is in AlignedBags specified as empty range 0:-1 and in ScatteredBags as an empty vector Int[]. The constructor of BagNode accepts directly one of these two structures and tries to automagically decide the better type in other cases.","category":"page"},{"location":"manual/nodes/#BagModel","page":"Nodes","title":"BagModel","text":"","category":"section"},{"location":"manual/nodes/","page":"Nodes","title":"Nodes","text":"Each BagNode is processed by a BagModel, which contains two (sub)models and an aggregation operator:","category":"page"},{"location":"manual/nodes/","page":"Nodes","title":"Nodes","text":"im = ArrayModel(Dense(2, 3))\na = SegmentedMax(3)\nbm = ArrayModel(Dense(4, 4))\nBM = BagModel(im, a, bm)","category":"page"},{"location":"manual/nodes/","page":"Nodes","title":"Nodes","text":"The first network submodel (called instance model im) is responsible for converting the instance representation to a vector form:","category":"page"},{"location":"manual/nodes/","page":"Nodes","title":"Nodes","text":"y = im(AN)","category":"page"},{"location":"manual/nodes/","page":"Nodes","title":"Nodes","text":"Note that because of the property mentioned above, the output of instance model im will always be an ArrayNode wrapping a matrix. We get four columns, one for each instance. This result is then used in Aggregation (a) which takes vector representation of all instances and produces a single vector per bag:","category":"page"},{"location":"manual/nodes/","page":"Nodes","title":"Nodes","text":"y = a(y, BN.bags)","category":"page"},{"location":"manual/nodes/","page":"Nodes","title":"Nodes","text":"unk: More about aggregation\nTo read more about aggregation operators and find out why there are four rows instead of three after applying the operator, see Bag aggregation section.","category":"page"},{"location":"manual/nodes/","page":"Nodes","title":"Nodes","text":"Finally, y is then passed to a feed forward model (called bag model bm) producing the final output per bag. In our example we therefore get a matrix with three columns:","category":"page"},{"location":"manual/nodes/","page":"Nodes","title":"Nodes","text":"y = bm(y)","category":"page"},{"location":"manual/nodes/","page":"Nodes","title":"Nodes","text":"However, the best way to use a bag model node is to simply apply it, which results into the same output:","category":"page"},{"location":"manual/nodes/","page":"Nodes","title":"Nodes","text":"BM(BN) == y","category":"page"},{"location":"manual/nodes/","page":"Nodes","title":"Nodes","text":"The whole procedure is depicted in the following picture:","category":"page"},{"location":"manual/nodes/","page":"Nodes","title":"Nodes","text":"(Image: )","category":"page"},{"location":"manual/nodes/","page":"Nodes","title":"Nodes","text":"Three instances of the BagNode are represented by red subtrees are first mapped with instance model im, aggregated (aggregation operator here is a concatenation of two different operators a_1 and a_2), and the results of aggregation are transformed with bag model bm.","category":"page"},{"location":"manual/nodes/","page":"Nodes","title":"Nodes","text":"ukn: Musk example\nAnother handy feature of Mill.jl models is that they are completely differentiable and therefore fit in the Flux.jl framework. Nodes for processing arrays and bags are sufficient to solve the classical Musk dataset problem.","category":"page"},{"location":"manual/nodes/#ProductNodes-and-ProductModels","page":"Nodes","title":"ProductNodes and ProductModels","text":"","category":"section"},{"location":"manual/nodes/","page":"Nodes","title":"Nodes","text":"ProductNode can be thought of as a Cartesian Product or a Dictionary. It holds a Tuple or NamedTuple of nodes (not necessarily of the same type). For example, a ProductNode with a BagNode and ArrayNode as children would look like this:","category":"page"},{"location":"manual/nodes/","page":"Nodes","title":"Nodes","text":"PN = ProductNode((a=ArrayNode(Float32.([1 2 3; 4 5 6])), b=BN))","category":"page"},{"location":"manual/nodes/","page":"Nodes","title":"Nodes","text":"Analogically, the ProductModel contains a (Named)Tuple of (sub)models processing each of its children (stored in ms field standing for models), as well as one more (sub)model m:","category":"page"},{"location":"manual/nodes/","page":"Nodes","title":"Nodes","text":"ms = (a=AM, b=BM)\nm = ArrayModel(Dense(7, 2))\nPM = ProductModel(ms, m)","category":"page"},{"location":"manual/nodes/","page":"Nodes","title":"Nodes","text":"Again, since the library is based on the property that the output of each model is an ArrayNode, the product model applies models from ms to appropriate children and vertically concatenates the output, which is then processed by model m. An example of model processing the above sample would be:","category":"page"},{"location":"manual/nodes/","page":"Nodes","title":"Nodes","text":"y = PM.m(vcat(PM[:a](PN[:a]), PM[:b](PN[:b])))","category":"page"},{"location":"manual/nodes/","page":"Nodes","title":"Nodes","text":"which is equivalent to:","category":"page"},{"location":"manual/nodes/","page":"Nodes","title":"Nodes","text":"PM(PN) == y","category":"page"},{"location":"manual/nodes/","page":"Nodes","title":"Nodes","text":"Application of a product model (this time with four subtrees (keys)) can be visualized as follows:","category":"page"},{"location":"manual/nodes/","page":"Nodes","title":"Nodes","text":"(Image: )","category":"page"},{"location":"manual/nodes/","page":"Nodes","title":"Nodes","text":"unk: Indexing in product nodes\nIn general, we recommend to use NamedTuples, because the key can be used for indexing both ProductNodes and ProductModels.","category":"page"},{"location":"manual/leaf_data/","page":"Data in leaves","title":"Data in leaves","text":"using Mill","category":"page"},{"location":"manual/leaf_data/#Data-in-leaves","page":"Data in leaves","title":"Data in leaves","text":"","category":"section"},{"location":"manual/leaf_data/","page":"Data in leaves","title":"Data in leaves","text":"In Mill.jl tree-like data representations, there are always some raw data on the leaf level, whereas on higher levels instances are grouped into bags (BagNodes), and different sets are joined together with Cartesion products (ProductNodes) and thus more abstract concepts are created. In this section we look into several examples how the lowest-level data can be represented.","category":"page"},{"location":"manual/leaf_data/","page":"Data in leaves","title":"Data in leaves","text":"For this purpose, let's assume that we would like to identify infected computers in a network from their HTTP traffic. Since one computer can make an arbitrary number of connections during the observation period, modelling the computer as a bag of connections seems like the most natural approach:","category":"page"},{"location":"manual/leaf_data/","page":"Data in leaves","title":"Data in leaves","text":"connections = AlignedBags([1:2, 3:3, 4:7, 8:8, 9:10])","category":"page"},{"location":"manual/leaf_data/","page":"Data in leaves","title":"Data in leaves","text":"Thus, each of the ten connections becomes an instance in one of the bags. How to represent this instance? Each HTTP flow has properties that can be expressed as standard numerical features, categorical features or strings of characters.","category":"page"},{"location":"manual/leaf_data/#Numerical-features","page":"Data in leaves","title":"Numerical features","text":"","category":"section"},{"location":"manual/leaf_data/","page":"Data in leaves","title":"Data in leaves","text":"We have already shown how to represent standard numerical features in previous parts of this manual. It is as simple as wrapping a type that behaves like a matrix into an ArrayNode:","category":"page"},{"location":"manual/leaf_data/","page":"Data in leaves","title":"Data in leaves","text":"content_lengths = [4446, 1957, 4310, 11604, 17019, 13947, 13784, 15495, 3888, 11853]\ndates = [1435420950, 1376190532, 1316869962, 1302775198, 1555598383,\n         1562237892, 1473173059, 1325242539, 1508048391, 1532722821]\nnumerical_node = ArrayNode([content_lengths'; dates'])","category":"page"},{"location":"manual/leaf_data/","page":"Data in leaves","title":"Data in leaves","text":"We use Content-Length  and Date request headers, the latter converted to Unix timestamp.","category":"page"},{"location":"manual/leaf_data/#Categorical-features","page":"Data in leaves","title":"Categorical features","text":"","category":"section"},{"location":"manual/leaf_data/","page":"Data in leaves","title":"Data in leaves","text":"For categorical variables, we proceed in the same way, but we use one-hot encoding implemented in Flux.jl. This way, we can encode for example a verb of the request:","category":"page"},{"location":"manual/leaf_data/","page":"Data in leaves","title":"Data in leaves","text":"using Flux\n\nALL_VERBS = [\"GET\", \"HEAD\", \"POST\", \"PUT\", \"DELETE\"] # etc...\nverbs = [\"GET\", \"GET\", \"POST\", \"HEAD\", \"HEAD\", \"HEAD\", \"HEAD\", \"PUT\", \"DELETE\", \"PUT\"]\nverb_node = ArrayNode(Flux.onehotbatch(verbs, ALL_VERBS))","category":"page"},{"location":"manual/leaf_data/","page":"Data in leaves","title":"Data in leaves","text":"or Content-Encoding header:","category":"page"},{"location":"manual/leaf_data/","page":"Data in leaves","title":"Data in leaves","text":"ALL_ENCODINGS = [\"bzip2\", \"gzip\", \"xz\", \"identity\"] # etc...\nencodings = [\"xz\", \"gzip\", \"bzip2\", \"xz\", \"identity\", \"bzip2\", \"identity\", \"identity\", \"xz\", \"xz\"]\nencoding_node = ArrayNode(Flux.onehotbatch(encodings, ALL_ENCODINGS))","category":"page"},{"location":"manual/leaf_data/","page":"Data in leaves","title":"Data in leaves","text":"Because Flux.OneHotMatrix supports multiplication it is possible to wrap it into an ArrayNode.","category":"page"},{"location":"manual/leaf_data/#Strings","page":"Data in leaves","title":"Strings","text":"","category":"section"},{"location":"manual/leaf_data/","page":"Data in leaves","title":"Data in leaves","text":"The last example we will consider are string features. This could for example be the Host header:","category":"page"},{"location":"manual/leaf_data/","page":"Data in leaves","title":"Data in leaves","text":"hosts = [\n    \"www.foo.com\",\n    \"www.foo.com\",\n    \"www.baz.com\",\n    \"www.foo.com\",\n    \"www.baz.com\",\n    \"www.foo.com\",\n    \"www.baz.com\",\n    \"www.baz.com\",\n    \"www.bar.com\",\n    \"www.baz.com\",\n]","category":"page"},{"location":"manual/leaf_data/","page":"Data in leaves","title":"Data in leaves","text":"Mill.jl offers ngram histogram-based representation for strings. To get started, we pass the vector of strings into the constructor of NGramMatrix:","category":"page"},{"location":"manual/leaf_data/","page":"Data in leaves","title":"Data in leaves","text":"hosts_ngrams = NGramMatrix(hosts, 3, 256, 7)","category":"page"},{"location":"manual/leaf_data/","page":"Data in leaves","title":"Data in leaves","text":"Each string gets processed into ngrams (trigram in this case as specified in the first parameter). Then, each character is transformed into an integer via the codeunits function and the whole trigram is interpreted as a three digit number using a base b specified in the second parameter. Here, we use a base of 256, which is the most reasonable choice for ascii URLs. For example, for foo trigram, we obtain:","category":"page"},{"location":"manual/leaf_data/","page":"Data in leaves","title":"Data in leaves","text":"c = codeunits(\"foo\")\nc[1] * 256^2 + c[2] * 256 + c[3]","category":"page"},{"location":"manual/leaf_data/","page":"Data in leaves","title":"Data in leaves","text":"The last step is taking the modulo of this result with respect to some prime modulo m, in this case 7 (last parameter in the constructor), leaving us with 3 as a result. Therefore, for this trigram foo, we would add 1 to the third row[1]. We can convert this NGramMatrix into a sparse array and then to the standard array:","category":"page"},{"location":"manual/leaf_data/","page":"Data in leaves","title":"Data in leaves","text":"[1]: One appropriate value for modulo m for real problems is 2053","category":"page"},{"location":"manual/leaf_data/","page":"Data in leaves","title":"Data in leaves","text":"using SparseArrays\nhosts_dense = hosts_ngrams |> SparseMatrixCSC |> Matrix","category":"page"},{"location":"manual/leaf_data/","page":"Data in leaves","title":"Data in leaves","text":"Again, we get one column for each string, and the matrix has the same number of rows as modulo m. For each string s, we get length(s) + n - 1 ngrams:","category":"page"},{"location":"manual/leaf_data/","page":"Data in leaves","title":"Data in leaves","text":"sum(hosts_dense; dims=1)","category":"page"},{"location":"manual/leaf_data/","page":"Data in leaves","title":"Data in leaves","text":"This is because we use special abstract characters (or tokens) for the start and the end of the string. If we denote these ^ and $, respectively, from string \"foo\", we get trigrams ^^f, ^fo, foo, oo$, o$$. Note that these special characters are purely abstract whereas ^ and $ used only for illustration purposes here are characters like any other. Both string start and string end special characters have a unique mapping to integers, which can be obtained as well as set:","category":"page"},{"location":"manual/leaf_data/","page":"Data in leaves","title":"Data in leaves","text":"Mill.string_start_code()\nMill.string_end_code()\nMill.string_start_code!(42)\nMill.string_start_code()","category":"page"},{"location":"manual/leaf_data/","page":"Data in leaves","title":"Data in leaves","text":"NGramMatrix behaves like a matrix, implements an efficient left-multiplication and thus can be used in ArrayNode:","category":"page"},{"location":"manual/leaf_data/","page":"Data in leaves","title":"Data in leaves","text":"hosts_ngrams::AbstractMatrix{Int64}\nhost_node = ArrayNode(hosts_ngrams)","category":"page"},{"location":"manual/leaf_data/","page":"Data in leaves","title":"Data in leaves","text":"Adding custom nodes section shows one more slightly more complex way of processing strings, specifically Unix paths.","category":"page"},{"location":"manual/leaf_data/#Putting-it-all-together","page":"Data in leaves","title":"Putting it all together","text":"","category":"section"},{"location":"manual/leaf_data/","page":"Data in leaves","title":"Data in leaves","text":"Now, we can finally put wrap everything into one ProductNode:","category":"page"},{"location":"manual/leaf_data/","page":"Data in leaves","title":"Data in leaves","text":"ds = ProductNode((\n    numerical=numerical_node,\n    verb=verb_node,\n    encoding=encoding_node,\n    hosts=host_node\n))","category":"page"},{"location":"manual/leaf_data/","page":"Data in leaves","title":"Data in leaves","text":"create a model for training and compute some gradients:","category":"page"},{"location":"manual/leaf_data/","page":"Data in leaves","title":"Data in leaves","text":"m = reflectinmodel(ds)\ngradient(() -> sum(m(ds).data), Flux.params(m))","category":"page"},{"location":"manual/leaf_data/","page":"Data in leaves","title":"Data in leaves","text":"ukn: Numerical features\nTo put all numerical features into one ArrayNode is a design choice. We could as well introduce more keys in the final ProductNode. The model treats these two cases differently (see Nodes section).","category":"page"},{"location":"manual/leaf_data/","page":"Data in leaves","title":"Data in leaves","text":"This dummy example illustrates the versatility of Mill.jl. With little to no preprocessing we are able to process complex hierarchical structures and avoid manually designing feature extraction procedures. For a more involved study on processing Internet traffic with Mill.jl, see for example Tomáš Pevný , Marek Dědič  (2020).","category":"page"},{"location":"manual/reflectin/","page":"Model reflection","title":"Model reflection","text":"using Mill","category":"page"},{"location":"manual/reflectin/#Model-Reflection","page":"Model reflection","title":"Model Reflection","text":"","category":"section"},{"location":"manual/reflectin/","page":"Model reflection","title":"Model reflection","text":"Since constructions of large models can be a tedious and error-prone process, Mill.jl provides reflectinmodel function that helps to automate it. The simplest definition accepts only one argument, a sample ds, and returns a compatible model:","category":"page"},{"location":"manual/reflectin/","page":"Model reflection","title":"Model reflection","text":"ds = BagNode(ProductNode((BagNode(ArrayNode(randn(4, 10)),\n                                  [1:2, 3:4, 5:5, 6:7, 8:10]),\n                          ArrayNode(randn(3, 5)),\n                          BagNode(BagNode(ArrayNode(randn(2, 30)),\n                                          [i:i+1 for i in 1:2:30]),\n                                  [1:3, 4:6, 7:9, 10:12, 13:15]),\n                          ArrayNode(randn(2, 5)))),\n             [1:1, 2:3, 4:5]);\nprinttree(ds)\n\nm = reflectinmodel(ds);\nprinttree(m)\n\nm(ds)","category":"page"},{"location":"manual/reflectin/","page":"Model reflection","title":"Model reflection","text":"The sample ds serves here as a specimen needed to specify a structure of the problem and calculate dimensions.","category":"page"},{"location":"manual/reflectin/#Optional-arguments","page":"Model reflection","title":"Optional arguments","text":"","category":"section"},{"location":"manual/reflectin/","page":"Model reflection","title":"Model reflection","text":"To have better control over the topology, reflectinmodel accepts up to two more optional arguments and four keyword arguments:","category":"page"},{"location":"manual/reflectin/","page":"Model reflection","title":"Model reflection","text":"The first optional argument expects a function that returns a layer (or a set of layers) given input dimension d (defaults to d -> Flux.Dense(d, 10)).\nThe second optional argument is a function returning aggregation function for BagModel nodes (defaults to d -> SegmentedMean(d)).","category":"page"},{"location":"manual/reflectin/","page":"Model reflection","title":"Model reflection","text":"Compare the following example to the previous one:","category":"page"},{"location":"manual/reflectin/","page":"Model reflection","title":"Model reflection","text":"using Flux","category":"page"},{"location":"manual/reflectin/","page":"Model reflection","title":"Model reflection","text":"m = reflectinmodel(ds, d -> Dense(d, 5, relu), d -> SegmentedMax(d));\nprinttree(m)\n\nm(ds)","category":"page"},{"location":"manual/reflectin/#Keyword-arguments","page":"Model reflection","title":"Keyword arguments","text":"","category":"section"},{"location":"manual/reflectin/","page":"Model reflection","title":"Model reflection","text":"The reflectinmodel allows even further customization. To index into the sample (or model), we can use printtree(ds; trav=true) from HierarchicalUtils.jl that prints the sample together with identifiers of individual nodes:","category":"page"},{"location":"manual/reflectin/","page":"Model reflection","title":"Model reflection","text":"using HierarchicalUtils","category":"page"},{"location":"manual/reflectin/","page":"Model reflection","title":"Model reflection","text":"printtree(ds; trav=true)","category":"page"},{"location":"manual/reflectin/","page":"Model reflection","title":"Model reflection","text":"These identifiers can be used to override the default construction functions. Note that the output, i.e. the last feed-forward network of the whole model is always tagged with an empty string \"\", which simplifies putting linear layer with an appropriate output dimension on the end. Dictionaries with these overrides can be passed in as keyword arguments:","category":"page"},{"location":"manual/reflectin/","page":"Model reflection","title":"Model reflection","text":"fsm overrides constructions of feed-forward models\nfsa overrides construction of aggregation functions.","category":"page"},{"location":"manual/reflectin/","page":"Model reflection","title":"Model reflection","text":"For example to specify just the last feed forward neural network:","category":"page"},{"location":"manual/reflectin/","page":"Model reflection","title":"Model reflection","text":"reflectinmodel(ds, d -> Dense(d, 5, relu), d -> SegmentedMeanMax(d);\n    fsm = Dict(\"\" => d -> Chain(Dense(d, 20, relu), Dense(20, 12)))) |> printtree","category":"page"},{"location":"manual/reflectin/","page":"Model reflection","title":"Model reflection","text":"Both keyword arguments in action:","category":"page"},{"location":"manual/reflectin/","page":"Model reflection","title":"Model reflection","text":"reflectinmodel(ds, d -> Dense(d, 5, relu), d -> SegmentedMeanMax(d);\n    fsm = Dict(\"\" => d -> Chain(Dense(d, 20, relu), Dense(20, 12))),\n    fsa = Dict(\"Y\" => d -> SegmentedMean(d), \"g\" => d -> SegmentedPNorm(d))) |> printtree","category":"page"},{"location":"citation/#Citation","page":"Citation","title":"Citation","text":"","category":"section"},{"location":"citation/","page":"Citation","title":"Citation","text":"For citing, please use the following entry:","category":"page"},{"location":"citation/","page":"Citation","title":"Citation","text":"@misc{Mill2018,\n    author = {Tomáš Pevný and Šimon Mandlík},\n    title = {{Mill.jl framework: a flexible library for (hierarchical) multi-instance learning}},\n    year = 2018,\n    howpublished = \"\\url{https://github.com/pevnak/Mill.jl}\"\n}","category":"page"},{"location":"manual/aggregation/","page":"Bag aggregation","title":"Bag aggregation","text":"using Mill","category":"page"},{"location":"manual/aggregation/#Bag-aggregation","page":"Bag aggregation","title":"Bag aggregation","text":"","category":"section"},{"location":"manual/aggregation/","page":"Bag aggregation","title":"Bag aggregation","text":"A wrapper type Aggregation and all subtypes of AggregationOperator it wraps are structures that are responsible for mapping of vector representations of multiple instances into a single vector. They all operate element-wise and independently of dimension and thus the output has the same size as representations on the input, unless the Concatenation of multiple operators is used or Bag Count is enabled.","category":"page"},{"location":"manual/aggregation/","page":"Bag aggregation","title":"Bag aggregation","text":"Some setup:","category":"page"},{"location":"manual/aggregation/","page":"Bag aggregation","title":"Bag aggregation","text":"d = 2\nX = Float32.([1 2 3 4; 8 7 6 5])\nn = ArrayNode(X)\nbags = AlignedBags([1:1, 2:3, 4:4])\n\nMill.bagcount!(false)","category":"page"},{"location":"manual/aggregation/","page":"Bag aggregation","title":"Bag aggregation","text":"Different choice of operator, or their combinations, are suitable for different problems. Nevertheless, because the input is interpreted as an unordered bag of instances, every operator is invariant to permutation and also does not scale when increasing size of the bag.","category":"page"},{"location":"manual/aggregation/#Non-parametric-aggregation","page":"Bag aggregation","title":"Non-parametric aggregation","text":"","category":"section"},{"location":"manual/aggregation/","page":"Bag aggregation","title":"Bag aggregation","text":"SegmentedMax is the most straightforward operator defined in one dimension as follows:","category":"page"},{"location":"manual/aggregation/","page":"Bag aggregation","title":"Bag aggregation","text":"a_max(x_1 ldots x_k) = max_i = 1 ldots k x_i","category":"page"},{"location":"manual/aggregation/","page":"Bag aggregation","title":"Bag aggregation","text":"where x_1 ldots x_k are all instances of the given bag. In Mill.jl, the operator is constructed this way:","category":"page"},{"location":"manual/aggregation/","page":"Bag aggregation","title":"Bag aggregation","text":"a_max = SegmentedMax(d)","category":"page"},{"location":"manual/aggregation/","page":"Bag aggregation","title":"Bag aggregation","text":"ukn: Dimension\nThe dimension of input is required so that the default parameters ψ can be properly instantiated (see Missing data for details).","category":"page"},{"location":"manual/aggregation/","page":"Bag aggregation","title":"Bag aggregation","text":"The application is straightforward and can be performed on both raw AbstractArrays or ArrayNodes:","category":"page"},{"location":"manual/aggregation/","page":"Bag aggregation","title":"Bag aggregation","text":"a_max(X, bags)\na_max(n, bags)","category":"page"},{"location":"manual/aggregation/","page":"Bag aggregation","title":"Bag aggregation","text":"Since we have three bags, we have three columns in the output, each storing the maximal element over all instances of the given bag.","category":"page"},{"location":"manual/aggregation/#SegmentedMean","page":"Bag aggregation","title":"SegmentedMean","text":"","category":"section"},{"location":"manual/aggregation/","page":"Bag aggregation","title":"Bag aggregation","text":"SegmentedMean is defined as:","category":"page"},{"location":"manual/aggregation/","page":"Bag aggregation","title":"Bag aggregation","text":"a_operatornamemean(x_1 ldots x_k) = frac1k sum_i = 1^k x_i","category":"page"},{"location":"manual/aggregation/","page":"Bag aggregation","title":"Bag aggregation","text":"and used the same way:","category":"page"},{"location":"manual/aggregation/","page":"Bag aggregation","title":"Bag aggregation","text":"a_mean = SegmentedMean(d)\na_mean(X, bags)\na_mean(n, bags)","category":"page"},{"location":"manual/aggregation/","page":"Bag aggregation","title":"Bag aggregation","text":"ukn: Sufficiency of the mean operator\nIn theory, SegmentedMean is sufficient for approximation (Tomáš Pevný , Vojtěch Kovařík  (2019)), but in practice, a combination of multiple operators performes better.","category":"page"},{"location":"manual/aggregation/","page":"Bag aggregation","title":"Bag aggregation","text":"The max aggregation is suitable for cases when one instance in the bag may give evidence strong enough to predict the label. On the other side of the spectrum lies the mean aggregation function, which detects well trends identifiable globally over the whole bag.","category":"page"},{"location":"manual/aggregation/#Parametric-aggregation","page":"Bag aggregation","title":"Parametric aggregation","text":"","category":"section"},{"location":"manual/aggregation/","page":"Bag aggregation","title":"Bag aggregation","text":"Whereas non-parametric aggregations do not use any parameter, parametric aggregations represent an entire class of functions parametrized by one or more real vectors of parameters, which can be even learned during training.","category":"page"},{"location":"manual/aggregation/#SegmentedLSE","page":"Bag aggregation","title":"SegmentedLSE","text":"","category":"section"},{"location":"manual/aggregation/","page":"Bag aggregation","title":"Bag aggregation","text":"SegmentedLSE (log-sum-exp) aggregation (Oren Z. Kraus , Lei Jimmy Ba , Brendan Frey  (2015)) is parametrized by a vector of positive numbers bmr in (mathbbR^+)^d m that specifies one real parameter for computation in each output dimension:","category":"page"},{"location":"manual/aggregation/","page":"Bag aggregation","title":"Bag aggregation","text":"a_operatornamelse(x_1 ldots x_k r) = frac1rlog left(frac1k sum_i = 1^k exp(rcdot x_i)right)","category":"page"},{"location":"manual/aggregation/","page":"Bag aggregation","title":"Bag aggregation","text":"With different values of r, LSE behaves differently and in fact both max and mean operators are limiting cases of LSE. If r is very small, the output approaches simple mean, and on the other hand, if r is a large number, LSE becomes a smooth approximation of the max function. Naively implementing the definition above may lead to numerical instabilities, however, the Mill.jl implementation is numerically stable.","category":"page"},{"location":"manual/aggregation/","page":"Bag aggregation","title":"Bag aggregation","text":"a_lse = SegmentedLSE(d)\na_lse(X, bags)","category":"page"},{"location":"manual/aggregation/#SegmentedPNorm","page":"Bag aggregation","title":"SegmentedPNorm","text":"","category":"section"},{"location":"manual/aggregation/","page":"Bag aggregation","title":"Bag aggregation","text":"(Normalized) p-norm operator (Caglar Gulcehre , Kyunghyun Cho , Razvan Pascanu , Yoshua Bengio  (2014)) is parametrized by a vector of real numbers bmp in (mathbbR^+)^d, where forall i in 1 ldots m  colon p_i geq 1, and another vector bmc in (mathbbR^+)^d. It is computed with formula:","category":"page"},{"location":"manual/aggregation/","page":"Bag aggregation","title":"Bag aggregation","text":"a_operatornamepnorm(x_1 ldots x_k p c) = left(frac1k sum_i = 1^k vert x_i - c vert ^ p right)^frac1p","category":"page"},{"location":"manual/aggregation/","page":"Bag aggregation","title":"Bag aggregation","text":"Again, the Mill.jl implementation is stable.","category":"page"},{"location":"manual/aggregation/","page":"Bag aggregation","title":"Bag aggregation","text":"a_pnorm = SegmentedPNorm(d)\na_pnorm(X, bags)","category":"page"},{"location":"manual/aggregation/","page":"Bag aggregation","title":"Bag aggregation","text":"Because all parameter constraints are included implicitly (field \\rho in both types is a real number that undergoes appropriate transformation before being used), both parametric operators are easy to use and do not require any special treatment. Replacing the definition of aggregation operators while constructing a model (either manually or with reflectinmodel) is enough.","category":"page"},{"location":"manual/aggregation/#Concatenation","page":"Bag aggregation","title":"Concatenation","text":"","category":"section"},{"location":"manual/aggregation/","page":"Bag aggregation","title":"Bag aggregation","text":"To use a concatenation of two or more operators, one can use an Aggregation constructor:","category":"page"},{"location":"manual/aggregation/","page":"Bag aggregation","title":"Bag aggregation","text":"a = Aggregation(a_mean, a_max)\na(X, bags)","category":"page"},{"location":"manual/aggregation/","page":"Bag aggregation","title":"Bag aggregation","text":"For the most common combinations, Mill.jl provides some convenience definitions:","category":"page"},{"location":"manual/aggregation/","page":"Bag aggregation","title":"Bag aggregation","text":"SegmentedMeanMax(d)","category":"page"},{"location":"manual/aggregation/#Weighted-aggregation","page":"Bag aggregation","title":"Weighted aggregation","text":"","category":"section"},{"location":"manual/aggregation/","page":"Bag aggregation","title":"Bag aggregation","text":"Sometimes, different instances in the bag are not equally important and contribute to output to a different extent. For instance, this may come in handy when performing importance sampling over very large bags. SegmentedMean and SegmentedPNorm have definitions taking weights into account:","category":"page"},{"location":"manual/aggregation/","page":"Bag aggregation","title":"Bag aggregation","text":"a_operatornamemean((x_i w_i)_i=1^k) = frac1sum_i=1^k w_i sum_i = 1^k w_i cdot x_i","category":"page"},{"location":"manual/aggregation/","page":"Bag aggregation","title":"Bag aggregation","text":"a_operatornamepnorm(x_i w_i_i=1^k p c) = left(frac1sum_i=1^k w_i sum_i = 1^k w_icdotvert x_i - c vert ^ p right)^frac1p","category":"page"},{"location":"manual/aggregation/","page":"Bag aggregation","title":"Bag aggregation","text":"This is done in Mill.jl by passing an additional parameter:","category":"page"},{"location":"manual/aggregation/","page":"Bag aggregation","title":"Bag aggregation","text":"w = Float32.([1.0, 0.2, 0.8, 0.5])\na_mean(X, bags, w)\na_pnorm(X, bags, w)","category":"page"},{"location":"manual/aggregation/","page":"Bag aggregation","title":"Bag aggregation","text":"For SegmentedMax (and SegmentedLSE) it is possible to pass in weights, but they are ignored during computation:","category":"page"},{"location":"manual/aggregation/","page":"Bag aggregation","title":"Bag aggregation","text":"a_max(X, bags, w) == a_max(X, bags)","category":"page"},{"location":"manual/aggregation/#WeightedBagNode","page":"Bag aggregation","title":"WeightedBagNode","text":"","category":"section"},{"location":"manual/aggregation/","page":"Bag aggregation","title":"Bag aggregation","text":"WeightedBagNode is used to store instance weights into a dataset. It accepts weights in the constructor:","category":"page"},{"location":"manual/aggregation/","page":"Bag aggregation","title":"Bag aggregation","text":"wbn = WeightedBagNode(n, bags, w)","category":"page"},{"location":"manual/aggregation/","page":"Bag aggregation","title":"Bag aggregation","text":"and passes them to aggregation operators:","category":"page"},{"location":"manual/aggregation/","page":"Bag aggregation","title":"Bag aggregation","text":"m = reflectinmodel(wbn)\nm(wbn)","category":"page"},{"location":"manual/aggregation/","page":"Bag aggregation","title":"Bag aggregation","text":"Otherwise, WeightedBagNode behaves exactly like the standard BagNode.","category":"page"},{"location":"manual/aggregation/#Bag-count","page":"Bag aggregation","title":"Bag count","text":"","category":"section"},{"location":"manual/aggregation/","page":"Bag aggregation","title":"Bag aggregation","text":"For some problems, it may be beneficial to use the size of the bag directly and feed it to subsequent layers. Whether this is the case is controlled by Mill.bagcount!(::Bool) function. It is on by default, however, it was disabled at the beginning of this section for demonstration purposes. Let's turn it back on:","category":"page"},{"location":"manual/aggregation/","page":"Bag aggregation","title":"Bag aggregation","text":"Mill.bagcount!(true)","category":"page"},{"location":"manual/aggregation/","page":"Bag aggregation","title":"Bag aggregation","text":"In the aggregation phase, bag count appends one more element which stores the bag size to the output after all operators are applied. Furthermore, in Mill.jl, we opted to perform a mapping x mapsto log(x) + 1 on top of that:","category":"page"},{"location":"manual/aggregation/","page":"Bag aggregation","title":"Bag aggregation","text":"a_mean(X, bags)","category":"page"},{"location":"manual/aggregation/","page":"Bag aggregation","title":"Bag aggregation","text":"The matrix now has three rows, the last one storing the size of the bag.","category":"page"},{"location":"manual/aggregation/","page":"Bag aggregation","title":"Bag aggregation","text":"When the bag count is on, one needs to have a model accepting corresponding sizes:","category":"page"},{"location":"manual/aggregation/","page":"Bag aggregation","title":"Bag aggregation","text":"bn = BagNode(n, bags)\nbm = reflectinmodel(bn)","category":"page"},{"location":"manual/aggregation/","page":"Bag aggregation","title":"Bag aggregation","text":"Note that the bm (sub)model field of the BagNode has size of (11, 10), 10 for aggregation output and 1 for sizes of bags.","category":"page"},{"location":"manual/aggregation/","page":"Bag aggregation","title":"Bag aggregation","text":"bm(bn)","category":"page"},{"location":"manual/aggregation/","page":"Bag aggregation","title":"Bag aggregation","text":"Model reflection takes bag count toggle into account. If we disable it again, bm (sub)model has size (10, 10):","category":"page"},{"location":"manual/aggregation/","page":"Bag aggregation","title":"Bag aggregation","text":"Mill.bagcount!(false)\nbm = reflectinmodel(bn)","category":"page"},{"location":"manual/aggregation/","page":"Bag aggregation","title":"Bag aggregation","text":"Mill.bagcount!(true)","category":"page"},{"location":"manual/aggregation/#Default-aggregation-values","page":"Bag aggregation","title":"Default aggregation values","text":"","category":"section"},{"location":"manual/aggregation/","page":"Bag aggregation","title":"Bag aggregation","text":"When all aggregation operators are printed, one may notice that all of them store one additional vector ψ. This is a vector of default parameters, initialized to all zeros, that are used for empty bags:","category":"page"},{"location":"manual/aggregation/","page":"Bag aggregation","title":"Bag aggregation","text":"bags = AlignedBags([1:1, 0:-1, 2:3, 0:-1, 4:4])\na_mean(X, bags)","category":"page"},{"location":"manual/aggregation/","page":"Bag aggregation","title":"Bag aggregation","text":"See Missing data page for more information.","category":"page"},{"location":"#Home","page":"Home","title":"Home","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Mill.jl is a library built on top of Flux.jl aimed to flexibly prototype hierarchical multi-instance learning models as described in Tomáš Pevný , Petr Somol  (2017) and  Tomáš Pevný , Petr Somol  (2016). It is developed to be:","category":"page"},{"location":"","page":"Home","title":"Home","text":"flexible and versatile\nas general as possible\nfast \nand dependent on only handful of other packages","category":"page"},{"location":"#Installation","page":"Home","title":"Installation","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Run the following in REPL:","category":"page"},{"location":"","page":"Home","title":"Home","text":"] add Mill","category":"page"},{"location":"","page":"Home","title":"Home","text":"Go to","category":"page"},{"location":"","page":"Home","title":"Home","text":"Motivation for a brief introduction into the philosophy of Mill.jl\nArchitecture of Mill \nExamples\nHelper tools\nReferences\nTODO finish this and add contents","category":"page"}]
}
