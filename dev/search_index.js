var documenterSearchIndex = {"docs":
[{"location":"manual/more_on_nodes/","page":"More on nodes","title":"More on nodes","text":"using Mill","category":"page"},{"location":"manual/more_on_nodes/#More-on-nodes","page":"More on nodes","title":"More on nodes","text":"","category":"section"},{"location":"manual/more_on_nodes/#Node-nesting","page":"More on nodes","title":"Node nesting","text":"","category":"section"},{"location":"manual/more_on_nodes/","page":"More on nodes","title":"More on nodes","text":"The main advantage of the Mill.jl library is that it allows to arbitrarily nest and cross-product BagModels, as described in Theorem 5 in Tomáš Pevný, Vojtěch Kovařík (2019). In other words, instances themselves may be represented in much more complex way than in the BagNode and BagModel example.","category":"page"},{"location":"manual/more_on_nodes/","page":"More on nodes","title":"More on nodes","text":"Let's start the demonstration by nesting two MIL problems. The outer MIL model contains three samples (outer-level bags), whose instances are (inner-level) bags themselves. The first outer-level bag contains one inner-level bag problem with two inner-level instances, the second outer-level bag contains two inner-level bags with total of three inner-level instances, and finally the third outer-level bag contains two inner bags with four instances:","category":"page"},{"location":"manual/more_on_nodes/","page":"More on nodes","title":"More on nodes","text":"ds = BagNode(BagNode(ArrayNode(randn(4, 10)),\n                     [1:2, 3:4, 5:5, 6:7, 8:10]),\n             [1:1, 2:3, 4:5])","category":"page"},{"location":"manual/more_on_nodes/","page":"More on nodes","title":"More on nodes","text":"Here is one example of a model, which is appropriate for this hierarchy:","category":"page"},{"location":"manual/more_on_nodes/","page":"More on nodes","title":"More on nodes","text":"using Flux: Dense, Chain, relu","category":"page"},{"location":"manual/more_on_nodes/","page":"More on nodes","title":"More on nodes","text":"m = BagModel(\n        BagModel(\n            ArrayModel(Dense(4, 3, relu)),\n            SegmentedMeanMax(3),\n            ArrayModel(Dense(6, 3, relu))),\n        SegmentedMeanMax(3),\n        ArrayModel(Chain(Dense(6, 3, relu), Dense(3, 2))))","category":"page"},{"location":"manual/more_on_nodes/","page":"More on nodes","title":"More on nodes","text":"and can be directly applied to obtain a result:","category":"page"},{"location":"manual/more_on_nodes/","page":"More on nodes","title":"More on nodes","text":"m(ds)","category":"page"},{"location":"manual/more_on_nodes/","page":"More on nodes","title":"More on nodes","text":"Here we again make use of the property that even if each instance is represented with an arbitrarily complex structure, we always obtain a vector representation after applying instance model im, regardless of the complexity of im and Mill.data(ds):","category":"page"},{"location":"manual/more_on_nodes/","page":"More on nodes","title":"More on nodes","text":"m.im(Mill.data(ds))","category":"page"},{"location":"manual/more_on_nodes/","page":"More on nodes","title":"More on nodes","text":"In one final example we demonstrate a complex model consisting of all types of nodes introduced so far:","category":"page"},{"location":"manual/more_on_nodes/","page":"More on nodes","title":"More on nodes","text":"ds = BagNode(ProductNode((BagNode(ArrayNode(randn(4, 10)),\n                                  [1:2, 3:4, 5:5, 6:7, 8:10]),\n                          ArrayNode(randn(3, 5)),\n                          BagNode(BagNode(ArrayNode(randn(2, 30)),\n                                          [i:i+1 for i in 1:2:30]),\n                                  [1:3, 4:6, 7:9, 10:12, 13:15]),\n                          ArrayNode(randn(2, 5)))),\n             [1:1, 2:3, 4:5])","category":"page"},{"location":"manual/more_on_nodes/","page":"More on nodes","title":"More on nodes","text":"As data and model trees tend to be complex, Mill limits the printing. To inspect the whole tree, use printtree:","category":"page"},{"location":"manual/more_on_nodes/","page":"More on nodes","title":"More on nodes","text":"printtree(ds)","category":"page"},{"location":"manual/more_on_nodes/","page":"More on nodes","title":"More on nodes","text":"Instead of defining a model manually, we can also make use of Model reflection, another Mill functionality, which simplifies model creation:","category":"page"},{"location":"manual/more_on_nodes/","page":"More on nodes","title":"More on nodes","text":"m = reflectinmodel(ds, d -> Dense(d, 2), SegmentedMean)\nm(ds)","category":"page"},{"location":"manual/more_on_nodes/#Node-conveniences","page":"More on nodes","title":"Node conveniences","text":"","category":"section"},{"location":"manual/more_on_nodes/","page":"More on nodes","title":"More on nodes","text":"To make the handling of data and model hierarchies easier, Mill provides several tools. Let's setup some data:","category":"page"},{"location":"manual/more_on_nodes/","page":"More on nodes","title":"More on nodes","text":"AN = ArrayNode(Float32.([1 2 3 4; 5 6 7 8]))\nAM = reflectinmodel(AN)\nBN = BagNode(AN, [1:1, 2:3, 4:4])\nBM = reflectinmodel(BN)\nPN = ProductNode(a=ArrayNode(Float32.([1 2 3; 4 5 6])), b=BN)\nPM = reflectinmodel(PN)","category":"page"},{"location":"manual/more_on_nodes/#Function:-nobs","page":"More on nodes","title":"Function: nobs","text":"","category":"section"},{"location":"manual/more_on_nodes/","page":"More on nodes","title":"More on nodes","text":"nobs function from StatsBase.jl returns a number of samples from the current level point of view. This number usually increases as we go down the tree when BagNodes are involved, as each bag may contain more than one instance.","category":"page"},{"location":"manual/more_on_nodes/","page":"More on nodes","title":"More on nodes","text":"using StatsBase: nobs\nnobs(AN)\nnobs(BN)\nnobs(PN)","category":"page"},{"location":"manual/more_on_nodes/#Indexing-and-Slicing","page":"More on nodes","title":"Indexing and Slicing","text":"","category":"section"},{"location":"manual/more_on_nodes/","page":"More on nodes","title":"More on nodes","text":"Indexing in [Mill] operates on the level of observations:","category":"page"},{"location":"manual/more_on_nodes/","page":"More on nodes","title":"More on nodes","text":"AN[1]\nnobs(ans)\nBN[2]\nnobs(ans)\nPN[3]\nnobs(ans)\nAN[[1, 4]]\nnobs(ans)\nBN[1:2]\nnobs(ans)\nPN[[2, 3]]\nnobs(ans)\nPN[Int[]]\nnobs(ans)","category":"page"},{"location":"manual/more_on_nodes/","page":"More on nodes","title":"More on nodes","text":"This may be useful for creating minibatches and their permutations.","category":"page"},{"location":"manual/more_on_nodes/","page":"More on nodes","title":"More on nodes","text":"Note that apart from the perhaps apparent recurrent effect, this operation requires other implicit actions, such as properly recomputing bag indices:","category":"page"},{"location":"manual/more_on_nodes/","page":"More on nodes","title":"More on nodes","text":"BN.bags\nBN[[1, 3]].bags","category":"page"},{"location":"manual/more_on_nodes/#Function:-[catobs](@ref)","page":"More on nodes","title":"Function: catobs","text":"","category":"section"},{"location":"manual/more_on_nodes/","page":"More on nodes","title":"More on nodes","text":"catobs function concatenates several samples (datasets) together:","category":"page"},{"location":"manual/more_on_nodes/","page":"More on nodes","title":"More on nodes","text":"catobs(AN[1], AN[4])\ncatobs(BN[3], BN[[2, 1]])\ncatobs(PN[[1, 2]], PN[3:3]) == PN","category":"page"},{"location":"manual/more_on_nodes/","page":"More on nodes","title":"More on nodes","text":"Again, the effect is recurrent and everything is appropriately recomputed:","category":"page"},{"location":"manual/more_on_nodes/","page":"More on nodes","title":"More on nodes","text":"BN.bags\ncatobs(BN[3], BN[[1]]).bags","category":"page"},{"location":"manual/more_on_nodes/","page":"More on nodes","title":"More on nodes","text":"This operation is an analogy to what is usually done in the classical setting. If every observation is represented as a vector of features, each (mini)batch of samples is first concatenated into one matrix and the whole matrix is run through the neural network using fast matrix multiplication procedures. The same reasoning applies here, but instead of Base.cat, catobs is needed.","category":"page"},{"location":"manual/more_on_nodes/","page":"More on nodes","title":"More on nodes","text":"Equipped with everything mentioned above there are two different ways to construct minibatches from data. First option, applicable mainly to smaller datasets, is to load all avaiable data into memory, store it as one big data node containing all observations, and use Indexing and Slicing to obtain minibatches. Such approach is demonstrated in the Musk example. The other option is to read all observations into memory separately (or load them on demand) and construct minibatches with catobs.","category":"page"},{"location":"manual/more_on_nodes/","page":"More on nodes","title":"More on nodes","text":"ukn: More tips\nFor more tips for handling datasets and models, see External tools.","category":"page"},{"location":"manual/more_on_nodes/#Metadata","page":"More on nodes","title":"Metadata","text":"","category":"section"},{"location":"manual/more_on_nodes/","page":"More on nodes","title":"More on nodes","text":"Each AbstractMillNode can also carry arbitrary metadata (defaulting to nothing). Metadata is provided upon construction of the node and accessed metadata by Mill.metadata:","category":"page"},{"location":"manual/more_on_nodes/","page":"More on nodes","title":"More on nodes","text":"n1 = ArrayNode(randn(2, 2), [\"metadata\"])\nMill.metadata(n1)\nn2 = ProductNode(tuple(n1), [1 3; 2 4])\nMill.metadata(n2)","category":"page"},{"location":"references/#References","page":"References","title":"References","text":"","category":"section"},{"location":"references/","page":"References","title":"References","text":"","category":"page"},{"location":"api/special_arrays/#Special-arrays","page":"Special Arrays","title":"Special arrays","text":"","category":"section"},{"location":"api/special_arrays/#Index","page":"Special Arrays","title":"Index","text":"","category":"section"},{"location":"api/special_arrays/","page":"Special Arrays","title":"Special Arrays","text":"Pages = [\"special_arrays.md\"]","category":"page"},{"location":"api/special_arrays/#API","page":"Special Arrays","title":"API","text":"","category":"section"},{"location":"api/special_arrays/","page":"Special Arrays","title":"Special Arrays","text":"MaybeHotVector\nmaybehot\nMaybeHotMatrix\nmaybehotbatch\nmaybecold\n\nNGramIterator\nNGramIterator(::AbstractString, ::Any, ::Any, ::Any)\nngrams\nngrams!\ncountngrams\ncountngrams!\nNGramMatrix\nNGramMatrix(::Missing)\n\nPostImputingMatrix\nPostImputingMatrix(::AbstractMatrix)\npostimputing_dense\n\nPreImputingMatrix\nPreImputingMatrix(::AbstractMatrix)\npreimputing_dense","category":"page"},{"location":"api/special_arrays/#Mill.MaybeHotVector","page":"Special Arrays","title":"Mill.MaybeHotVector","text":"MaybeHotVector{T, U, V} <: AbstractVector{V}\n\nA vector-like structure for representing one-hot encoded variables. Like Flux.OneHotVector but supports missing values.\n\nConstruct with the maybehot function.\n\nSee also: MaybeHotMatrix, maybehotbatch.\n\n\n\n\n\n","category":"type"},{"location":"api/special_arrays/#Mill.maybehot","page":"Special Arrays","title":"Mill.maybehot","text":"maybehot(l, labels)\n\nReturn a MaybeHotVector where the first occurence of l in labels is set to 1 and all other elements are set to 0.\n\nExamples\n\njulia> maybehot(:b, [:a, :b, :c])\n3-element MaybeHotVector with eltype Bool:\n ⋅\n 1\n ⋅\n\njulia> maybehot(missing, 1:3)\n3-element MaybeHotVector with eltype Missing:\n missing\n missing\n missing\n\nSee also: maybehotbatch, MaybeHotVector, MaybeHotMatrix.\n\n\n\n\n\n","category":"function"},{"location":"api/special_arrays/#Mill.MaybeHotMatrix","page":"Special Arrays","title":"Mill.MaybeHotMatrix","text":"MaybeHotMatrix{T, U, V} <: AbstractMatrix{V}\n\nA matrix-like structure for representing one-hot encoded variables. Like Flux.OneHotMatrix but supports missing values.\n\nConstruct with the maybehotbatch function.\n\nSee also: MaybeHotVector, maybehot.\n\n\n\n\n\n","category":"type"},{"location":"api/special_arrays/#Mill.maybehotbatch","page":"Special Arrays","title":"Mill.maybehotbatch","text":"maybehotbatch(ls, labels)\n\nReturn a MaybeHotMatrix in which each column corresponds to one element of ls containing 1 at its first occurence in labels with all other elements set to 0.\n\nExamples\n\njulia> maybehotbatch([:c, :a], [:a, :b, :c])\n3×2 MaybeHotMatrix with eltype Bool:\n ⋅  1\n ⋅  ⋅\n 1  ⋅\n\njulia> maybehotbatch([missing, 2], 1:3)\n3×2 MaybeHotMatrix with eltype Union{Missing, Bool}:\n missing    ⋅\n missing   true\n missing    ⋅\n\nSee also: maybehot, MaybeHotMatrix, MaybeHotVector.\n\n\n\n\n\n","category":"function"},{"location":"api/special_arrays/#Mill.maybecold","page":"Special Arrays","title":"Mill.maybecold","text":"maybecold(y, labels=1:size(y,1))\n\nSimilar to Flux.onecold but when y contains missing values, missing is in the result as well.\n\nTherefore, it is roughly the inverse operation of maybehot or maybehotbatch.\n\nExamples\n\njulia> maybehot(:b, [:a, :b, :c])\n3-element MaybeHotVector with eltype Bool:\n ⋅\n 1\n ⋅\n\njulia> maybecold(ans, [:a, :b, :c])\n:b\n\njulia> maybehot(missing, 1:3)\n3-element MaybeHotVector with eltype Missing:\n missing\n missing\n missing\n\njulia> maybecold(ans)\nmissing\n\njulia> maybecold(maybehotbatch([missing, 2], 1:3))\n2-element Vector{Union{Missing, Int64}}:\n  missing\n 2\n\nSee also: Flux.onecold, maybehot, maybehotbatch.\n\n\n\n\n\n","category":"function"},{"location":"api/special_arrays/#Mill.NGramIterator","page":"Special Arrays","title":"Mill.NGramIterator","text":"NGramIterator{T}\n\nIterates over ngram codes of collection of integers s using Mill.string_start_code() and Mill.string_end_code() for padding. NGram codes are computed as in positional number systems, where items of s are digits, b is the base, and m is modulo.\n\nIn order to reduce collisions when mixing ngrams of different order one should avoid zeros and negative integers in s and should set base b to the expected number of unique tokens in s.\n\nSee also: NGramMatrix, ngrams, ngrams!, countngrams,     countngrams!.\n\n\n\n\n\n","category":"type"},{"location":"api/special_arrays/#Mill.NGramIterator-Tuple{AbstractString, Any, Any, Any}","page":"Special Arrays","title":"Mill.NGramIterator","text":"NGramIterator(s, n=3, b=256, m=typemax(Int))\n\nConstruct an NGramIterator. If s is an AbstractString it is first converted to integers with Base.codeunits.\n\nExamples\n\njulia> NGramIterator(\"deadbeef\", 3, 256, 17) |> collect\n10-element Vector{Int64}:\n  2\n 16\n  9\n  9\n  6\n 10\n 11\n 15\n  2\n  6\n\njulia> NGramIterator(collect(1:9), 3, 10, 1009) |> collect\n11-element Vector{Int64}:\n 221\n 212\n 123\n 234\n 345\n 456\n 567\n 678\n 789\n 893\n 933\n\njulia> Mill.string_start_code()\n0x02\n\njulia> Mill.string_end_code()\n0x03\n\nSee also: NGramMatrix, ngrams, ngrams!, countngrams,     countngrams!.\n\n\n\n\n\n","category":"method"},{"location":"api/special_arrays/#Mill.ngrams","page":"Special Arrays","title":"Mill.ngrams","text":"ngrams(o, x, n=3, b=256)\n\nReturn codes of n grams of x using base b.\n\nExamples\n\njulia> ngrams(\"foo\", 3, 256)\n5-element Vector{Int64}:\n  131686\n  157295\n 6713199\n 7302915\n 7275267\n\nSee also: ngrams!, countngrams,     countngrams!, NGramMatrix, NGramIterator.\n\n\n\n\n\n","category":"function"},{"location":"api/special_arrays/#Mill.ngrams!","page":"Special Arrays","title":"Mill.ngrams!","text":"ngrams!(o, x, n=3, b=256)\n\nStore codes of n grams of x using base b to o.\n\nExamples\n\njulia> o = zeros(Int, 5)\n5-element Vector{Int64}:\n 0\n 0\n 0\n 0\n 0\n\njulia> ngrams!(o, \"foo\", 3, 256)\n5-element Vector{Int64}:\n  131686\n  157295\n 6713199\n 7302915\n 7275267\n\nSee also: ngrams, countngrams,     countngrams!, NGramMatrix, NGramIterator.\n\n\n\n\n\n","category":"function"},{"location":"api/special_arrays/#Mill.countngrams","page":"Special Arrays","title":"Mill.countngrams","text":"countngrams(o, x, n, b, m)\n\nCount the number of of n grams of x using base b and modulo m into a vector of length m in case x is a single sequence or into a matrix with m rows if x is an iterable of sequences.\n\nExamples\n\njulia> countngrams(\"foo\", 3, 256, 5)\n5-element Vector{Int64}:\n 2\n 1\n 1\n 0\n 1\n\njulia> countngrams([\"foo\", \"bar\"], 3, 256, 5)\n5×2 Matrix{Int64}:\n 2  1\n 1  0\n 1  2\n 0  0\n 1  2\n\nSee also: countngrams!, ngrams, ngrams!,     NGramMatrix, NGramIterator.\n\n\n\n\n\n","category":"function"},{"location":"api/special_arrays/#Mill.countngrams!","page":"Special Arrays","title":"Mill.countngrams!","text":"countngrams!(o, x, n, b, m=length(o))\n\nCount the number of of n grams of x using base b and modulo m and store the result to o.\n\nExamples\n\njulia> o = zeros(Int, 5)\n5-element Vector{Int64}:\n 0\n 0\n 0\n 0\n 0\n\njulia> countngrams!(o, \"foo\", 3, 256)\n5-element Vector{Int64}:\n 2\n 1\n 1\n 0\n 1\n\nSee also: countngrams, ngrams, ngrams!,     NGramMatrix, NGramIterator.\n\n\n\n\n\n","category":"function"},{"location":"api/special_arrays/#Mill.NGramMatrix","page":"Special Arrays","title":"Mill.NGramMatrix","text":"NGramMatrix{T, U, V} <: AbstractMatrix{U}\n\nA matrix-like structure for lazily representing sequences like strings as ngrams of cardinality n using b as a base for calculations and m as the modulo. Therefore, the matrix has m rows and one column for representing each sequence. Missing sequences are supported.\n\nSee also: NGramIterator, ngrams, ngrams!, countngrams,     countngrams!.\n\n\n\n\n\n","category":"type"},{"location":"api/special_arrays/#Mill.NGramMatrix-Tuple{Missing}","page":"Special Arrays","title":"Mill.NGramMatrix","text":"NGramMatrix(s, n=3, b=256, m=2053)\n\nConstruct an NGramMatrix. s can either be a single sequence or any AbstractVector.\n\nExamples\n\njulia> NGramMatrix([1,2,3])\n2053×1 NGramMatrix{Vector{Int64}, Vector{Vector{Int64}}, Int64}:\n [1, 2, 3]\n\njulia> NGramMatrix([\"a\", missing, \"c\"], 2, 128)\n2053×3 NGramMatrix{Union{Missing, String}, Vector{Union{Missing, String}}, Union{Missing, Int64}}:\n \"a\"\n missing\n \"c\"\n\nSee also: NGramIterator, ngrams, ngrams!, countngrams,     countngrams!.\n\n\n\n\n\n","category":"method"},{"location":"api/special_arrays/#Mill.PostImputingMatrix","page":"Special Arrays","title":"Mill.PostImputingMatrix","text":"PostImputingMatrix{T <: Number, U <: AbstractMatrix{T}, V <: AbstractVector{T}} <: AbstractMatrix{T}\n\nA parametrized matrix that fills in a default vector of parameters whenever a \"missing\" column is encountered during multiplication.\n\nSupports multiplication with NGramMatrix, MaybeHotMatrix and MaybeHotVector. For any other AbstractMatrix it falls back to standard multiplication.\n\nExamples\n\njulia> A = PostImputingMatrix(ones(2, 2), -ones(2))\n2×2 PostImputingMatrix{Float64, Matrix{Float64}, Vector{Float64}}:\nW:\n 1.0  1.0\n 1.0  1.0\n\nψ:\n -1.0\n -1.0\n\njulia> A * maybehotbatch([1, missing], 1:2)\n2×2 Matrix{Float64}:\n 1.0  -1.0\n 1.0  -1.0\n\nSee also: PreImputingMatrix.\n\n\n\n\n\n","category":"type"},{"location":"api/special_arrays/#Mill.PostImputingMatrix-Tuple{AbstractMatrix}","page":"Special Arrays","title":"Mill.PostImputingMatrix","text":"PostImputingMatrix(W::AbstractMatrix{T}, ψ=zeros(T, size(W, 1))) where T\n\nConstruct a PostImputingMatrix with multiplication parameters W and default parameters ψ.\n\nExamples\n\njulia> PostImputingMatrix([1 2; 3 4])\n2×2 PostImputingMatrix{Int64, Matrix{Int64}, Vector{Int64}}:\nW:\n 1  2\n 3  4\n\nψ:\n 0\n 0\n\nSee also: PreImputingMatrix.\n\n\n\n\n\n","category":"method"},{"location":"api/special_arrays/#Mill.postimputing_dense","page":"Special Arrays","title":"Mill.postimputing_dense","text":"postimputing_dense(d_in, d_out, σ)\n\nLike Flux.Dense, but use a PostImputingMatrix instead of a standard matrix.\n\nExamples\n\njulia> d = postimputing_dense(3, 2)\n[postimputing]Dense(3, 2) \t# 3 arrays, 10 params, 160 bytes\n\njulia> typeof(d.weight)\nPostImputingMatrix{Float32, Matrix{Float32}, Vector{Float32}}\n\njulia> typeof(d.bias)\nVector{Float32} (alias for Array{Float32, 1})\n\nSee also: PostImputingMatrix, preimputing_dense, PreImputingMatrix.\n\n\n\n\n\n","category":"function"},{"location":"api/special_arrays/#Mill.PreImputingMatrix","page":"Special Arrays","title":"Mill.PreImputingMatrix","text":"PreImputingMatrix{T <: Number, U <: AbstractMatrix{T}, V <: AbstractVector{T}} <: AbstractMatrix{T}\n\nA parametrized matrix that fills in elements from a default vector of parameters whenever a missing element is encountered during multiplication.\n\nExamples\n\njulia> A = PreImputingMatrix(ones(2, 2), -ones(2))\n2×2 PreImputingMatrix{Float64, Matrix{Float64}, Vector{Float64}}:\nW:\n 1.0  1.0\n 1.0  1.0\n\nψ:\n -1.0  -1.0\n\njulia> A * [0 1; missing -1]\n2×2 Matrix{Float64}:\n -1.0  0.0\n -1.0  0.0\n\nSee also: PreImputingMatrix.\n\n\n\n\n\n","category":"type"},{"location":"api/special_arrays/#Mill.PreImputingMatrix-Tuple{AbstractMatrix}","page":"Special Arrays","title":"Mill.PreImputingMatrix","text":"PreImputingMatrix(W::AbstractMatrix{T}, ψ=zeros(T, size(W, 2))) where T\n\nConstruct a PreImputingMatrix with multiplication parameters W and default parameters ψ.\n\nExamples\n\njulia> PreImputingMatrix([1 2; 3 4])\n2×2 PreImputingMatrix{Int64, Matrix{Int64}, Vector{Int64}}:\nW:\n 1  2\n 3  4\n\nψ:\n 0  0\n\nSee also: PostImputingMatrix.\n\n\n\n\n\n","category":"method"},{"location":"api/special_arrays/#Mill.preimputing_dense","page":"Special Arrays","title":"Mill.preimputing_dense","text":"preimputing_dense(in, out, σ)\n\nLike Flux.Dense, but use a PreImputingMatrix instead of a standard matrix.\n\nExamples\n\njulia> d = preimputing_dense(2, 3)\n[preimputing]Dense(2, 3) \t# 3 arrays, 11 params, 164 bytes\n\njulia> typeof(d.weight)\nPreImputingMatrix{Float32, Matrix{Float32}, Vector{Float32}}\n\njulia> typeof(d.bias)\nVector{Float32} (alias for Array{Float32, 1})\n\nSee also: PreImputingMatrix, postimputing_dense, PostImputingMatrix.\n\n\n\n\n\n","category":"function"},{"location":"api/aggregation/#Aggregation","page":"Aggregation","title":"Aggregation","text":"","category":"section"},{"location":"api/aggregation/#Index","page":"Aggregation","title":"Index","text":"","category":"section"},{"location":"api/aggregation/","page":"Aggregation","title":"Aggregation","text":"Pages = [\"aggregation.md\"]","category":"page"},{"location":"api/aggregation/#API","page":"Aggregation","title":"API","text":"","category":"section"},{"location":"api/aggregation/","page":"Aggregation","title":"Aggregation","text":"AbstractAggregation\nAggregationStack\n\nSegmentedSum\nSegmentedMax\nSegmentedMean\nSegmentedPNorm\nSegmentedLSE\n\nSegmentedMeanMax\nSegmentedPNormLSE\n\nBagCount","category":"page"},{"location":"api/aggregation/#Mill.AbstractAggregation","page":"Aggregation","title":"Mill.AbstractAggregation","text":"AbstractAggregation\n\nSupertype for any aggregation operator.\n\nSee also: AggregationStack, SegmentedSum, SegmentedMax,     SegmentedMean, SegmentedPNorm, SegmentedLSE.\n\n\n\n\n\n","category":"type"},{"location":"api/aggregation/#Mill.AggregationStack","page":"Aggregation","title":"Mill.AggregationStack","text":"AggregationStack{T <: Tuple{Vararg{AbstractAggregation}}} <: AbstractAggregation\n\nA container that implements a concatenation of one or more AbstractAggregations.\n\nConstruct with e.g. AggregationStack(SegmentedMean([t::Type, ]d)) for single operators and with e.g. SegmentedPNormLSE([t::Type, ]d) for concatenations. With these calls all parameters inside operators are initialized randomly as Float32 arrays, unless type t is further specified. Another option is to vcat two operators together.\n\nNested stacks are flattened into a single-level structure upon construction, see examples.\n\nIntended to be used as a functor:\n\n(a::AggregationStack)(x, bags[, w])\n\nwhere x is either Missing, AbstractMatrix or ArrayNode, bags is AbstractBags structure and optionally w is an AbstractVector of weights.\n\nExamples\n\njulia> a = AggregationStack(SegmentedMean(2), SegmentedMax(2))\nAggregationStack:\n SegmentedMean(ψ = Float32[0.0, 0.0])\n SegmentedMax(ψ = Float32[0.0, 0.0])\n\njulia> a(Float32[0 1 2; 3 4 5], bags([1:1, 2:3]))\n4×2 Matrix{Float32}:\n 0.0  1.5\n 3.0  4.5\n 0.0  2.0\n 3.0  5.0\n\njulia> a = AggregationStack(SegmentedMean(2), AggregationStack(SegmentedMax(2)))\nAggregationStack:\n SegmentedMean(ψ = Float32[0.0, 0.0])\n SegmentedMax(ψ = Float32[0.0, 0.0])\n\njulia> a = SegmentedMeanMax(Float32, 2)\nAggregationStack:\n SegmentedMean(ψ = Float32[0.0, 0.0])\n SegmentedMax(ψ = Float32[0.0, 0.0])\n\njulia> vcat(SegmentedMean(2), SegmentedMax(2))\nAggregationStack:\n SegmentedMean(ψ = Float32[0.0, 0.0])\n SegmentedMax(ψ = Float32[0.0, 0.0])\n\n\nSee also: AbstractAggregation, SegmentedSum, SegmentedMax,     SegmentedMean, SegmentedPNorm, SegmentedLSE.\n\n\n\n\n\n","category":"type"},{"location":"api/aggregation/#Mill.SegmentedSum","page":"Aggregation","title":"Mill.SegmentedSum","text":"SegmentedSum{V <: AbstractVector{<:Number}} <: AbstractAggregation\n\nAbstractAggregation implementing segmented sum aggregation:\n\nf(x_1 ldots x_k) = sum_i = 1^k x_i\n\nStores a vector of parameters ψ that are filled into the resulting matrix in case an empty bag is encountered.\n\nSee also: AbstractAggregation, AggregationStack,     SegmentedMax, SegmentedMean, SegmentedPNorm, SegmentedLSE.\n\n\n\n\n\n","category":"type"},{"location":"api/aggregation/#Mill.SegmentedMax","page":"Aggregation","title":"Mill.SegmentedMax","text":"SegmentedMax{V <: AbstractVector{<:Number}} <: AbstractAggregation\n\nAbstractAggregation implementing segmented max aggregation:\n\nf(x_1 ldots x_k) = max_i = 1 ldots k x_i\n\nStores a vector of parameters ψ that are filled into the resulting matrix in case an empty bag is encountered.\n\nSee also: AbstractAggregation, AggregationStack,     SegmentedMean, SegmentedSum, SegmentedPNorm, SegmentedLSE.\n\n\n\n\n\n","category":"type"},{"location":"api/aggregation/#Mill.SegmentedMean","page":"Aggregation","title":"Mill.SegmentedMean","text":"SegmentedMean{V <: AbstractVector{<:Number}}} <: AbstractAggregation\n\nAbstractAggregation implementing segmented mean aggregation:\n\nf(x_1 ldots x_k) = frac1k sum_i = 1^k x_i\n\nStores a vector of parameters ψ that are filled into the resulting matrix in case an empty bag is encountered.\n\nSee also: AbstractAggregation, AggregationStack,     SegmentedMax, SegmentedSum, SegmentedPNorm, SegmentedLSE.\n\n\n\n\n\n","category":"type"},{"location":"api/aggregation/#Mill.SegmentedPNorm","page":"Aggregation","title":"Mill.SegmentedPNorm","text":"SegmentedPNorm{V <: AbstractVector{<:AbstractFloat}} <: AbstractAggregation\n\nAbstractAggregation implementing segmented p-norm aggregation:\n\nf(x_1 ldots x_k p c) = left(frac1k sum_i = 1^k vert x_i - c vert ^ p right)^frac1p\n\nStores a vector of parameters ψ that are filled into the resulting matrix in case an empty bag is encountered, and vectors of parameters p and c used during computation.\n\nSee also: AbstractAggregation, AggregationStack,     SegmentedMax, SegmentedMean, SegmentedSum, SegmentedLSE.\n\n\n\n\n\n","category":"type"},{"location":"api/aggregation/#Mill.SegmentedLSE","page":"Aggregation","title":"Mill.SegmentedLSE","text":"SegmentedLSE{V <: AbstractVector{<:AbstractFloat}} <: AbstractAggregation\n\nAbstractAggregation implementing segmented log-sum-exp (LSE) aggregation:\n\nf(x_1 ldots x_k r) = frac1rlog left(frac1k sum_i = 1^k exp(rcdot x_i)right)\n\nStores a vector of parameters ψ that are filled into the resulting matrix in case an empty bag is encountered, and a vector of parameters r used during computation.\n\nSee also: AbstractAggregation, AggregationStack,     SegmentedMax, SegmentedMean, SegmentedSum, SegmentedPNorm.\n\n\n\n\n\n","category":"type"},{"location":"api/aggregation/#Mill.SegmentedMeanMax","page":"Aggregation","title":"Mill.SegmentedMeanMax","text":"SegmentedMeanMax([t::Type, ]d::Int)\n\nConstruct AggregationStack consisting of SegmentedMean and SegmentedMax operators.\n\nExamples\n\njulia> SegmentedMeanMax(4)\nAggregationStack:\n SegmentedMean(ψ = Float32[0.0, 0.0, 0.0, 0.0])\n SegmentedMax(ψ = Float32[0.0, 0.0, 0.0, 0.0])\n\njulia> SegmentedMeanMax(Float64, 2)\nAggregationStack:\n SegmentedMean(ψ = [0.0, 0.0])\n SegmentedMax(ψ = [0.0, 0.0])\n\nSee also: AbstractAggregation, AggregationStack, SegmentedSum,     SegmentedMax, SegmentedMean, SegmentedPNorm, SegmentedLSE.\n\n\n\n\n\n","category":"function"},{"location":"api/aggregation/#Mill.SegmentedPNormLSE","page":"Aggregation","title":"Mill.SegmentedPNormLSE","text":"SegmentedPNormLSE([t::Type, ]d::Int)\n\nConstruct AggregationStack consisting of SegmentedPNorm and SegmentedLSE operators.\n\nSee also: AbstractAggregation, AggregationStack, SegmentedSum,     SegmentedMax, SegmentedMean, SegmentedPNorm, SegmentedLSE.\n\n\n\n\n\n","category":"function"},{"location":"api/aggregation/#Mill.BagCount","page":"Aggregation","title":"Mill.BagCount","text":"BagCount{T <: AbstractAggregation}\n\nA wrapper type that when called applies the AbstractAggregation stored in it, and appends one more element containing bag size after x  log(x + 1) transformation to the result.\n\nUsed as a functor:\n\n(bc::BagCount)(x, bags[, w])\n\nwhere x is either Missing, AbstractMatrix or ArrayNode, bags is AbstractBags structure and optionally w is an AbstractVector of weights.\n\nExamples\n\njulia> x = Float32[0 1 2; 3 4 5]\n2×3 Matrix{Float32}:\n 0.0  1.0  2.0\n 3.0  4.0  5.0\n\njulia> b = bags([1:1, 2:3])\nAlignedBags{Int64}(UnitRange{Int64}[1:1, 2:3])\n\njulia> a = vcat(SegmentedMean(2), SegmentedMax(2))\nAggregationStack:\n SegmentedMean(ψ = Float32[0.0, 0.0])\n SegmentedMax(ψ = Float32[0.0, 0.0])\n\njulia> a(x, b)\n4×2 Matrix{Float32}:\n 0.0  1.5\n 3.0  4.5\n 0.0  2.0\n 3.0  5.0\n\njulia> BagCount(a)(x, b)\n5×2 Matrix{Float32}:\n 0.0       1.5\n 3.0       4.5\n 0.0       2.0\n 3.0       5.0\n 0.693147  1.09861\n\nSee also: AbstractAggregation, AggregationStack, SegmentedSum,     SegmentedMax, SegmentedMean, SegmentedPNorm, SegmentedLSE.\n\n\n\n\n\n","category":"type"},{"location":"tools/hierarchical/","page":"HierarchicalUtils.jl","title":"HierarchicalUtils.jl","text":"using Mill\nusing StatsBase: nobs","category":"page"},{"location":"tools/hierarchical/#HierarchicalUtils.jl","page":"HierarchicalUtils.jl","title":"HierarchicalUtils.jl","text":"","category":"section"},{"location":"tools/hierarchical/","page":"HierarchicalUtils.jl","title":"HierarchicalUtils.jl","text":"Mill.jl uses HierarchicalUtils.jl which brings a lot of additional features.","category":"page"},{"location":"tools/hierarchical/","page":"HierarchicalUtils.jl","title":"HierarchicalUtils.jl","text":"using HierarchicalUtils","category":"page"},{"location":"tools/hierarchical/#Printing","page":"HierarchicalUtils.jl","title":"Printing","text":"","category":"section"},{"location":"tools/hierarchical/","page":"HierarchicalUtils.jl","title":"HierarchicalUtils.jl","text":"For instance, Base.show with text/plain MIME calls HierarchicalUtils.printtree:","category":"page"},{"location":"tools/hierarchical/","page":"HierarchicalUtils.jl","title":"HierarchicalUtils.jl","text":"ds = BagNode(ProductNode((BagNode(ArrayNode(randn(4, 10)),\n                                  [1:2, 3:4, 5:5, 6:7, 8:10]),\n                          ArrayNode(randn(3, 5)),\n                          BagNode(BagNode(ArrayNode(randn(2, 30)),\n                                          [i:i+1 for i in 1:2:30]),\n                                  [1:3, 4:6, 7:9, 10:12, 13:15]),\n                          ArrayNode(randn(2, 5)))),\n             [1:1, 2:3, 4:5])\nprinttree(ds; htrunc=3)","category":"page"},{"location":"tools/hierarchical/","page":"HierarchicalUtils.jl","title":"HierarchicalUtils.jl","text":"This can be used to print a non-truncated version of a model:","category":"page"},{"location":"tools/hierarchical/","page":"HierarchicalUtils.jl","title":"HierarchicalUtils.jl","text":"printtree(ds)","category":"page"},{"location":"tools/hierarchical/#Traversal-encoding","page":"HierarchicalUtils.jl","title":"Traversal encoding","text":"","category":"section"},{"location":"tools/hierarchical/","page":"HierarchicalUtils.jl","title":"HierarchicalUtils.jl","text":"Callling with trav=true enables convenient traversal functionality with string indexing:","category":"page"},{"location":"tools/hierarchical/","page":"HierarchicalUtils.jl","title":"HierarchicalUtils.jl","text":"m = reflectinmodel(ds)\nprinttree(m; trav=true)","category":"page"},{"location":"tools/hierarchical/","page":"HierarchicalUtils.jl","title":"HierarchicalUtils.jl","text":"This way any node in the model tree is swiftly accessible, which may come in handy when inspecting model parameters or simply deleting/replacing/inserting nodes to tree (for instance when constructing adversarial samples). All tree nodes are accessible by indexing with the traversal code:.","category":"page"},{"location":"tools/hierarchical/","page":"HierarchicalUtils.jl","title":"HierarchicalUtils.jl","text":"m[\"Y\"]","category":"page"},{"location":"tools/hierarchical/","page":"HierarchicalUtils.jl","title":"HierarchicalUtils.jl","text":"The following two approaches give the same result:","category":"page"},{"location":"tools/hierarchical/","page":"HierarchicalUtils.jl","title":"HierarchicalUtils.jl","text":"m[\"Y\"] === m.im.ms[1]","category":"page"},{"location":"tools/hierarchical/#Counting-functions","page":"HierarchicalUtils.jl","title":"Counting functions","text":"","category":"section"},{"location":"tools/hierarchical/","page":"HierarchicalUtils.jl","title":"HierarchicalUtils.jl","text":"Other functions provided by HierarchicalUtils.jl:","category":"page"},{"location":"tools/hierarchical/","page":"HierarchicalUtils.jl","title":"HierarchicalUtils.jl","text":"nnodes(ds)\nnleafs(ds)\nNodeIterator(ds) |> collect\nNodeIterator(ds, m) |> collect\nLeafIterator(ds) |> collect\nTypeIterator(BagModel, m) |> collect\nPredicateIterator(x -> nobs(x) ≥ 10, ds) |> collect","category":"page"},{"location":"tools/hierarchical/","page":"HierarchicalUtils.jl","title":"HierarchicalUtils.jl","text":"For the complete showcase of possibilites, refer to HierarchicalUtils.jl and this notebook","category":"page"},{"location":"examples/gnn/","page":"GNNs in 16 lines","title":"GNNs in 16 lines","text":"using Mill\n\nusing Graphs, GraphRecipes, Plots, Random\nRandom.seed!(1)\n\ng = SimpleGraph(10)\nfor e in [(1, 2), (1, 3), (1, 4),\n          (2, 4), (2, 5),\n          (3, 4), (3, 5), (3, 6), (3, 8), (3, 10),\n          (4, 5), (4, 6), (4, 9),\n          (5, 7), (5, 8),\n          (6, 5), (6, 7), (6, 8),\n          (7, 8), (7, 10),\n          (8, 9)\n]\n    add_edge!(g, e...)\nend\n\ngp = graphplot(adjacency_matrix(g); linecolor = :darkgrey,\n                                    nodecolor=:lightgrey,\n                                    fontsize=11,\n                                    markersize=0.2, nodeshape=:circle,\n                                    background_color=:transparent,\n                                    markercolor = range(colorant\"yellow\", stop=colorant\"blue\", length=nv(g))\n                                    )\nsavefig(gp, \"graph.svg\")","category":"page"},{"location":"examples/gnn/#GNN-in-16-lines","page":"GNNs in 16 lines","title":"GNN in 16 lines","text":"","category":"section"},{"location":"examples/gnn/","page":"GNNs in 16 lines","title":"GNNs in 16 lines","text":"As has been mentioned in Šimon Mandlík, Tomáš Pevný (2020), multiple instance learning is an essential piece for implementing message passing inference over graphs, the main concept behind spatial Graph Neural Networks (GNNs). It is straightforward and quick to achieve this with Mill.jl. We begin with some dependencies:","category":"page"},{"location":"examples/gnn/","page":"GNNs in 16 lines","title":"GNNs in 16 lines","text":"using Mill, Flux, Graphs, Statistics","category":"page"},{"location":"examples/gnn/","page":"GNNs in 16 lines","title":"GNNs in 16 lines","text":"Let's assume a graph g, represented as a SimpleGraph from Graphs.jl","category":"page"},{"location":"examples/gnn/","page":"GNNs in 16 lines","title":"GNNs in 16 lines","text":"g = SimpleGraph(10)\nfor e in [(1, 2), (1, 3), (1, 4),\n          (2, 4), (2, 5),\n          (3, 4), (3, 5), (3, 6), (3, 8), (3, 10),\n          (4, 5), (4, 6), (4, 9),\n          (5, 7), (5, 8),\n          (6, 5), (6, 7), (6, 8),\n          (7, 8), (7, 10),\n          (8, 9)\n]\n    add_edge!(g, e...)\nend","category":"page"},{"location":"examples/gnn/","page":"GNNs in 16 lines","title":"GNNs in 16 lines","text":"(Image: )","category":"page"},{"location":"examples/gnn/","page":"GNNs in 16 lines","title":"GNNs in 16 lines","text":"Furthermore, let's assume that each vertex is described by three features stored in a matrix X:","category":"page"},{"location":"examples/gnn/","page":"GNNs in 16 lines","title":"GNNs in 16 lines","text":"X = ArrayNode(randn(Float32, 3, 10))","category":"page"},{"location":"examples/gnn/","page":"GNNs in 16 lines","title":"GNNs in 16 lines","text":"We use ScatteredBags from Mill to encode neighbors of each vertex. In other words, each vertex is described by a bag of its neighbors. This information is conveniently stored in fadjlist field of g, therefore the bags can be constructed as:","category":"page"},{"location":"examples/gnn/","page":"GNNs in 16 lines","title":"GNNs in 16 lines","text":"b = ScatteredBags(g.fadjlist)","category":"page"},{"location":"examples/gnn/","page":"GNNs in 16 lines","title":"GNNs in 16 lines","text":"Finally, we create two models. First model called lift will pre-process the description of vertices to some latent space for message passing, and the second one will realize the message passing itself, which we will call mp:","category":"page"},{"location":"examples/gnn/","page":"GNNs in 16 lines","title":"GNNs in 16 lines","text":"lift = reflectinmodel(X, d -> Dense(d, 5), SegmentedMean)\nU = lift(X)\nmp = reflectinmodel(BagNode(U, b), d -> Dense(d, 5), SegmentedMean)","category":"page"},{"location":"examples/gnn/","page":"GNNs in 16 lines","title":"GNNs in 16 lines","text":"Notice that BagNode(U, b) now essentially encodes vertex features as well as the adjacency matrix. This also means that one step of message passing algorithm can be realized as:","category":"page"},{"location":"examples/gnn/","page":"GNNs in 16 lines","title":"GNNs in 16 lines","text":"Y = mp(BagNode(U, b))","category":"page"},{"location":"examples/gnn/","page":"GNNs in 16 lines","title":"GNNs in 16 lines","text":"and it is differentiable, which can be verified by executing:","category":"page"},{"location":"examples/gnn/","page":"GNNs in 16 lines","title":"GNNs in 16 lines","text":"gradient(() -> sum(sin.(mp(BagNode(U, b)) |> Mill.data)), Flux.params(mp))","category":"page"},{"location":"examples/gnn/","page":"GNNs in 16 lines","title":"GNNs in 16 lines","text":"If we put everything together, the GNN implementation is implemented in the following 16 lines:","category":"page"},{"location":"examples/gnn/","page":"GNNs in 16 lines","title":"GNNs in 16 lines","text":"struct GNN{L,M, R}\n    lift::L\n    mp::M\n    m::R\nend\n\nFlux.@functor GNN\n\nfunction mpstep(m::GNN, U::ArrayNode, bags, n)\n    n == 0 && return(U)\n    mpstep(m, m.mp(BagNode(U, bags)), bags, n - 1)\nend\n\nfunction (m::GNN)(g, X, n)\n    U = m.lift(X)\n    bags = Mill.ScatteredBags(g.fadjlist)\n    o = mpstep(m, U, bags, n)\n    m.m(vcat(mean(Mill.data(o), dims = 2), maximum(Mill.data(o), dims = 2)))\nend\n\nnothing # hide","category":"page"},{"location":"examples/gnn/","page":"GNNs in 16 lines","title":"GNNs in 16 lines","text":"As it is the case with whole Mill, even this graph neural network is properly integrated with Flux.jl ecosystem and suports automatic differentiation:","category":"page"},{"location":"examples/gnn/","page":"GNNs in 16 lines","title":"GNNs in 16 lines","text":"zd = 4\nf(d) = Chain(Dense(d, zd, relu), Dense(zd, zd))\nagg = SegmentedMeanMax\ngnn = GNN(reflectinmodel(X, f, agg),\n          BagModel(f(zd), agg(zd), f(2zd)),\n          f(2zd))\nnothing # hide","category":"page"},{"location":"examples/gnn/","page":"GNNs in 16 lines","title":"GNNs in 16 lines","text":"gnn(g, X, 5)\ngradient(() -> gnn(g, X, 5) |> sum, Flux.params(gnn))","category":"page"},{"location":"examples/gnn/","page":"GNNs in 16 lines","title":"GNNs in 16 lines","text":"The above implementation is surprisingly general, as it supports an arbitrarily rich description of vertices. For simplicity, we used only vectors in X, however, any Mill hierarchy is applicable.","category":"page"},{"location":"examples/gnn/","page":"GNNs in 16 lines","title":"GNNs in 16 lines","text":"To put different weights on edges, one can use Weighted aggregation.","category":"page"},{"location":"api/utilities/#Utilities","page":"Utilities","title":"Utilities","text":"","category":"section"},{"location":"api/utilities/#Index","page":"Utilities","title":"Index","text":"","category":"section"},{"location":"api/utilities/","page":"Utilities","title":"Utilities","text":"Pages = [\"utilities.md\"]","category":"page"},{"location":"api/utilities/#API","page":"Utilities","title":"API","text":"","category":"section"},{"location":"api/utilities/","page":"Utilities","title":"Utilities","text":"sparsify\n\nlist_lens\nfind_lens\nfindnonempty_lens\npred_lens\n\ncode2lens\nlens2code\n\nmodel_lens\ndata_lens\n\nreplacein","category":"page"},{"location":"api/utilities/#Mill.sparsify","page":"Utilities","title":"Mill.sparsify","text":"sparsify(x, nnzrate)\n\nReplace AbstractMatrix x with SparseMatrixCSC if at most nnzrate fraction of elements is non-zero.\n\njulia> n = ArrayNode([0 0; 0 0])\n2×2 ArrayNode{Matrix{Int64}, Nothing}:\n 0  0\n 0  0\n\njulia> Mill.mapdata(i -> sparsify(i, 0.05), n)\n2×2 ArrayNode{SparseMatrixCSC{Int64, Int64}, Nothing}:\n ⋅  ⋅\n ⋅  ⋅\n\nSee also: Mill.mapdata.\n\n\n\n\n\n","category":"function"},{"location":"api/utilities/#Mill.list_lens","page":"Utilities","title":"Mill.list_lens","text":"list_lens(n)\n\nReturn a Vector of Setfield.Lenses for accessing all nodes/fields in n.\n\nExamples\n\njulia> n = ProductNode((BagNode(missing, bags([0:-1, 0:-1])), ArrayNode([1 2; 3 4])))\nProductNode \t# 2 obs, 16 bytes\n  ├── BagNode \t# 2 obs, 88 bytes\n  │     └── ∅\n  └── ArrayNode(2×2 Array with Int64 elements) \t# 2 obs, 80 bytes\n\njulia> list_lens(n)\n9-element Vector{Lens}:\n (@lens _)\n (@lens _.data[1])\n (@lens _.data[1].data)\n (@lens _.data[1].bags)\n (@lens _.data[1].metadata)\n (@lens _.data[2])\n (@lens _.data[2].data)\n (@lens _.data[2].metadata)\n (@lens _.metadata)\n\nSee also: pred_lens, find_lens, findnonempty_lens.\n\n\n\n\n\n","category":"function"},{"location":"api/utilities/#Mill.find_lens","page":"Utilities","title":"Mill.find_lens","text":"find_lens(n, x)\n\nReturn a Vector of Setfield.Lenses for accessing all nodes/fields in n that return true when compared to x using Base.===.\n\nExamples\n\njulia> n = ProductNode((BagNode(missing, bags([0:-1, 0:-1])), ArrayNode([1 2; 3 4])))\nProductNode \t# 2 obs, 16 bytes\n  ├── BagNode \t# 2 obs, 88 bytes\n  │     └── ∅\n  └── ArrayNode(2×2 Array with Int64 elements) \t# 2 obs, 80 bytes\n\njulia> find_lens(n, n.data[1])\n1-element Vector{Setfield.ComposedLens{Setfield.PropertyLens{:data}, Setfield.IndexLens{Tuple{Int64}}}}:\n (@lens _.data[1])\n\nSee also: pred_lens, list_lens, findnonempty_lens.\n\n\n\n\n\n","category":"function"},{"location":"api/utilities/#Mill.findnonempty_lens","page":"Utilities","title":"Mill.findnonempty_lens","text":"findnonempty_lens(n)\n\nReturn a Vector of Setfield.Lenses for accessing all nodes/fields in n that have at least one observation.\n\nExamples\n\njulia> n = ProductNode((BagNode(missing, bags([0:-1, 0:-1])), ArrayNode([1 2; 3 4])))\nProductNode \t# 2 obs, 16 bytes\n  ├── BagNode \t# 2 obs, 88 bytes\n  │     └── ∅\n  └── ArrayNode(2×2 Array with Int64 elements) \t# 2 obs, 80 bytes\n\njulia> findnonempty_lens(n)\n3-element Vector{Lens}:\n (@lens _)\n (@lens _.data[1])\n (@lens _.data[2])\n\nSee also: pred_lens, list_lens, find_lens.\n\n\n\n\n\n","category":"function"},{"location":"api/utilities/#Mill.pred_lens","page":"Utilities","title":"Mill.pred_lens","text":"pred_lens(p, n)\n\nReturn a Vector of Setfield.Lenses for accessing all nodes/fields in n conforming to predicate p.\n\nExamples\n\njulia> n = ProductNode((BagNode(missing, bags([0:-1, 0:-1])), ArrayNode([1 2; 3 4])))\nProductNode \t# 2 obs, 16 bytes\n  ├── BagNode \t# 2 obs, 88 bytes\n  │     └── ∅\n  └── ArrayNode(2×2 Array with Int64 elements) \t# 2 obs, 80 bytes\n\njulia> pred_lens(x -> x isa ArrayNode, n)\n1-element Vector{Setfield.ComposedLens{Setfield.PropertyLens{:data}, Setfield.IndexLens{Tuple{Int64}}}}:\n (@lens _.data[2])\n\nSee also: list_lens, find_lens, findnonempty_lens.\n\n\n\n\n\n","category":"function"},{"location":"api/utilities/#Mill.code2lens","page":"Utilities","title":"Mill.code2lens","text":"code2lens(n, c)\n\nConvert code c from HierarchicalUtils.jl traversal to a Vector of Setfield.Lens such that they access each node in tree egal to n.\n\nExamples\n\njulia> n = ProductNode((BagNode(missing, bags([0:-1, 0:-1])), ArrayNode([1 2; 3 4])));\n\njulia> printtree(n; trav=true)\nProductNode [\"\"] \t# 2 obs, 16 bytes\n  ├── BagNode [\"E\"] \t# 2 obs, 88 bytes\n  │     └── ∅ [\"M\"]\n  └── ArrayNode(2×2 Array with Int64 elements) [\"U\"] \t# 2 obs, 80 bytes\n\njulia> code2lens(n, \"U\")\n1-element Vector{Setfield.ComposedLens{Setfield.PropertyLens{:data}, Setfield.IndexLens{Tuple{Int64}}}}:\n (@lens _.data[2])\n\nSee also: lens2code.\n\n\n\n\n\n","category":"function"},{"location":"api/utilities/#Mill.lens2code","page":"Utilities","title":"Mill.lens2code","text":"lens2code(n, l)\n\nConvert Setfield.Lens l to a Vector of codes from HierarchicalUtils.jl traversal such that they access each node in tree egal to n.\n\nExamples\n\njulia> n = ProductNode((BagNode(missing, bags([0:-1, 0:-1])), ArrayNode([1 2; 3 4])));\n\njulia> printtree(n; trav=true)\nProductNode [\"\"] \t# 2 obs, 16 bytes\n  ├── BagNode [\"E\"] \t# 2 obs, 88 bytes\n  │     └── ∅ [\"M\"]\n  └── ArrayNode(2×2 Array with Int64 elements) [\"U\"] \t# 2 obs, 80 bytes\n\njulia> lens2code(n, (@lens _.data[2]))\n1-element Vector{String}:\n \"U\"\n\n\nSee also: code2lens.\n\n\n\n\n\n","category":"function"},{"location":"api/utilities/#Mill.model_lens","page":"Utilities","title":"Mill.model_lens","text":"model_lens(m, l)\n\nConvert Setfield.Lens l for a data node to a new lens for accessing the same location in model m.\n\nExamples\n\njulia> n = ProductNode((BagNode(ArrayNode(randn(2, 2)), bags([0:-1, 0:-1])), ArrayNode([1 2; 3 4])))\nProductNode \t# 2 obs, 24 bytes\n  ├── BagNode \t# 2 obs, 96 bytes\n  │     └── ArrayNode(2×2 Array with Float64 elements) \t# 2 obs, 80 bytes\n  └── ArrayNode(2×2 Array with Int64 elements) \t# 2 obs, 80 bytes\n\njulia> m = reflectinmodel(n)\nProductModel ↦ ArrayModel(Dense(20, 10)) \t# 2 arrays, 210 params, 920 bytes\n  ├── BagModel ↦ BagCount([SegmentedMean(10); SegmentedMax(10)]) ↦ ArrayModel(Dense(21, 10)) \t# 4 arrays, 240 params, 1.094 KiB\n  │     └── ArrayModel(Dense(2, 10)) \t# 2 arrays, 30 params, 200 bytes\n  └── ArrayModel(Dense(2, 10)) \t# 2 arrays, 30 params, 200 bytes\n\njulia> model_lens(m, (@lens _.data[2]))\n(@lens _.ms[2])\n\nSee also: data_lens.\n\n\n\n\n\n","category":"function"},{"location":"api/utilities/#Mill.data_lens","page":"Utilities","title":"Mill.data_lens","text":"data_lens(n, l)\n\nConvert Setfield.Lens l for a model node to a new lens for accessing the same location in data node n.\n\nExamples\n\njulia> n = ProductNode((BagNode(ArrayNode(randn(2, 2)), bags([0:-1, 0:-1])), ArrayNode([1 2; 3 4])))\nProductNode \t# 2 obs, 24 bytes\n  ├── BagNode \t# 2 obs, 96 bytes\n  │     └── ArrayNode(2×2 Array with Float64 elements) \t# 2 obs, 80 bytes\n  └── ArrayNode(2×2 Array with Int64 elements) \t# 2 obs, 80 bytes\n\njulia> m = reflectinmodel(n)\nProductModel ↦ ArrayModel(Dense(20, 10)) \t# 2 arrays, 210 params, 920 bytes\n  ├── BagModel ↦ BagCount([SegmentedMean(10); SegmentedMax(10)]) ↦ ArrayModel(Dense(21, 10)) \t# 4 arrays, 240 params, 1.094 KiB\n  │     └── ArrayModel(Dense(2, 10)) \t# 2 arrays, 30 params, 200 bytes\n  └── ArrayModel(Dense(2, 10)) \t# 2 arrays, 30 params, 200 bytes\n\njulia> data_lens(n, (@lens _.ms[2]))\n(@lens _.data[2])\n\nSee also: data_lens.\n\n\n\n\n\n","category":"function"},{"location":"api/utilities/#Mill.replacein","page":"Utilities","title":"Mill.replacein","text":"replacein(n, old, new)\n\nReplace in data node or model n each occurence of old by new.\n\nShort description\n\nExamples\n\njulia> n = ProductNode((BagNode(ArrayNode(randn(2, 2)), bags([0:-1, 0:-1])), ArrayNode([1 2; 3 4])))\nProductNode \t# 2 obs, 24 bytes\n  ├── BagNode \t# 2 obs, 96 bytes\n  │     └── ArrayNode(2×2 Array with Float64 elements) \t# 2 obs, 80 bytes\n  └── ArrayNode(2×2 Array with Int64 elements) \t# 2 obs, 80 bytes\n\njulia> replacein(n, n.data[1], ArrayNode(maybehotbatch([1, 2], 1:2)))\nProductNode \t# 2 obs, 24 bytes\n  ├── ArrayNode(2×2 MaybeHotMatrix with Bool elements) \t# 2 obs, 80 bytes\n  └── ArrayNode(2×2 Array with Int64 elements) \t# 2 obs, 80 bytes\n\n\n\n\n\n","category":"function"},{"location":"manual/custom/","page":"Custom nodes","title":"Custom nodes","text":"using Mill\nusing Flux","category":"page"},{"location":"manual/custom/#Custom-nodes","page":"Custom nodes","title":"Custom nodes","text":"","category":"section"},{"location":"manual/custom/","page":"Custom nodes","title":"Custom nodes","text":"Mill.jl data nodes are lightweight wrappers around data, such as Array, DataFrame, and others. It is of course possible to define a custom data (and model) nodes. A useful abstraction for implementing custom data nodes suitable for most cases  is LazyNode, which you can easily use to extend the functionality of Mill.","category":"page"},{"location":"manual/custom/#Unix-path-example","page":"Custom nodes","title":"Unix path example","text":"","category":"section"},{"location":"manual/custom/","page":"Custom nodes","title":"Custom nodes","text":"Let's define a custom node type for representing path names in Unix and one custom model type for processing it. LazyNode serves as a bolierplate for simple extension of Mill ecosystem. We start by by defining an example of such node:","category":"page"},{"location":"manual/custom/","page":"Custom nodes","title":"Custom nodes","text":"ds = LazyNode{:Path}([\"/var/lib/blob_files/myfile.blob\"])","category":"page"},{"location":"manual/custom/","page":"Custom nodes","title":"Custom nodes","text":"Entirely new type is not needed, because we can dispatch on the first type parameter. Specifically, :Path \"tag\" in this case defines a special kind of LazyNode. Consequently, we can define multiple variations of custom LazyNode without any conflicts in dispatch.","category":"page"},{"location":"manual/custom/","page":"Custom nodes","title":"Custom nodes","text":"As a next step, we extend the Mill.unpack2mill function, which always takes one LazyNode and produces an arbitrary Mill structure. We will represent individual file and directory names (as obtained by splitpath) using an NGramMatrix representation and, for simplicity, the whole path as a bag of individual names:","category":"page"},{"location":"manual/custom/","page":"Custom nodes","title":"Custom nodes","text":"function Mill.unpack2mill(ds::LazyNode{:Path})\n    ss = splitpath.(ds.data)\n    x = NGramMatrix(reduce(vcat, ss), 3)\n    BagNode(ArrayNode(x), Mill.length2bags(length.(ss)))\nend","category":"page"},{"location":"manual/custom/","page":"Custom nodes","title":"Custom nodes","text":"Mill.unpack2mill(ds)","category":"page"},{"location":"manual/custom/","page":"Custom nodes","title":"Custom nodes","text":"Also, note that the node keeps an array of strings instead of just one string. This is because we want our node to be able to hold multiple observations than one. Methods such as catobs work as expected:","category":"page"},{"location":"manual/custom/","page":"Custom nodes","title":"Custom nodes","text":"ds1 = LazyNode{:Path}([\"/var/lib/blob_files/myfile.blob\"])\nds2 = LazyNode{:Path}([\"/var/lib/python\"])\nnothing # hide","category":"page"},{"location":"manual/custom/","page":"Custom nodes","title":"Custom nodes","text":"ds = catobs(ds1, ds2)","category":"page"},{"location":"manual/custom/","page":"Custom nodes","title":"Custom nodes","text":"The Mill.unpack2mill function is called lazily during the inference by a LazyModel counterpart.","category":"page"},{"location":"manual/custom/","page":"Custom nodes","title":"Custom nodes","text":"Model reflection works too:","category":"page"},{"location":"manual/custom/","page":"Custom nodes","title":"Custom nodes","text":"pm = reflectinmodel(ds, d -> Dense(d, 3))","category":"page"},{"location":"manual/custom/","page":"Custom nodes","title":"Custom nodes","text":"We can use the obtained model to perform inference as we would do with any other model.","category":"page"},{"location":"manual/custom/","page":"Custom nodes","title":"Custom nodes","text":"pm(ds).data","category":"page"},{"location":"manual/custom/#Adding-custom-nodes-without-[LazyNode](@ref)","page":"Custom nodes","title":"Adding custom nodes without LazyNode","text":"","category":"section"},{"location":"manual/custom/","page":"Custom nodes","title":"Custom nodes","text":"The solution using LazyNode is sufficient in most scenarios. For other cases, it is recommended to equip custom nodes with the following functionality:","category":"page"},{"location":"manual/custom/","page":"Custom nodes","title":"Custom nodes","text":"allow nesting (if needed)\nimplement Mill.subset and optionally Base.getindex to obtain subsets of observations. Mill already defines Mill.subset for common datatypes, which can be used.\nallow concatenation of nodes with catobs. Optionally, implement reduce(catobs, ...) as well to avoid excessive compilations if a number of arguments will vary a lot\ndefine a specialized method for StatsBase.nobs\nregister the custom node with HierarchicalUtils.jl to obtain pretty printing, iterators and other functionality","category":"page"},{"location":"manual/custom/","page":"Custom nodes","title":"Custom nodes","text":"Here is an example of a custom node with the same functionality as in the Unix path example section:","category":"page"},{"location":"manual/custom/","page":"Custom nodes","title":"Custom nodes","text":"using Mill\n\nimport Base: getindex, show\nimport Mill: catobs, data, metadata, VecOrRange, AbstractMillNode, reflectinmodel\nimport Flux\nimport StatsBase: nobs\nimport HierarchicalUtils: NodeType, LeafNode\n\nstruct PathNode{S <: AbstractString, C} <: AbstractMillNode\n    data::Vector{S}\n    metadata::C\nend\n\nPathNode(data::Vector{S}) where {S <: AbstractString} = PathNode(data, nothing)\nBase.show(io::IO, n::PathNode) = print(io, \"PathNode ($(nobs(n)) obs)\")\n\nBase.ndims(n::PathNode) = Colon()\nnobs(n::PathNode) = length(n.data)\ncatobs(ns::PathNode) = PathNode(vcat(data.(ns)...), catobs(metadata.(as)...))\nBase.getindex(n::PathNode, i::VecOrRange{<:Int}) = PathNode(subset(data(x), i),\n                                                            subset(metadata(x), i))\nNodeType(::Type{<:PathNode}) = LeafNode()\nnothing # hide","category":"page"},{"location":"manual/custom/","page":"Custom nodes","title":"Custom nodes","text":"We also have to define a corresponding model node type which will be a counterpart processing the data:","category":"page"},{"location":"manual/custom/","page":"Custom nodes","title":"Custom nodes","text":"The solution using LazyNode is sufficient in most scenarios. For other cases, it is recommended to equip custom nodes with the following functionality:","category":"page"},{"location":"manual/custom/","page":"Custom nodes","title":"Custom nodes","text":"Flux.@functor PathModel show(io::IO, n::PathModel) = print(io, \"PathModel\") NodeType(::Type{<:PathModel}) = LeafNode()","category":"page"},{"location":"manual/custom/","page":"Custom nodes","title":"Custom nodes","text":"path2mill(ds::PathNode) = path2mill(ds.data) path2mill(ss::Vector{<:AbstractString}) = reduce(catobs, map(path2mill, ss)) function path2mill(s::String)     ss = splitpath(s)     BagNode(ArrayNode(NGramMatrix(ss, 3)), AlignedBags([1:length(ss)])) end","category":"page"},{"location":"manual/custom/","page":"Custom nodes","title":"Custom nodes","text":"(m::PathModel)(x::PathNode) = m.m(m.path2mill(x))","category":"page"},{"location":"manual/custom/","page":"Custom nodes","title":"Custom nodes","text":"function reflectinmodel(ds::PathNode, args...)     pm = reflectinmodel(path2mill(ds), args...)     PathModel(pm, path2mill) end nothing # hide","category":"page"},{"location":"manual/custom/","page":"Custom nodes","title":"Custom nodes","text":"\nExample of usage:\n","category":"page"},{"location":"manual/custom/","page":"Custom nodes","title":"Custom nodes","text":"@repl custom ds = PathNode([\"/etc/passwd\", \"/home/tonda/.bashrc\"]) pm = reflectinmodel(ds, d -> Dense(d, 3)) pm(ds).data ```","category":"page"},{"location":"api/data_nodes/#Data-nodes","page":"Data nodes","title":"Data nodes","text":"","category":"section"},{"location":"api/data_nodes/#Index","page":"Data nodes","title":"Index","text":"","category":"section"},{"location":"api/data_nodes/","page":"Data nodes","title":"Data nodes","text":"Pages = [\"data_nodes.md\"]","category":"page"},{"location":"api/data_nodes/#API","page":"Data nodes","title":"API","text":"","category":"section"},{"location":"api/data_nodes/","page":"Data nodes","title":"Data nodes","text":"AbstractMillNode\nAbstractProductNode\nAbstractBagNode\n\nArrayNode\nArrayNode(::AbstractArray)\n\nBagNode\nBagNode(::AbstractMillNode, ::AbstractVector, m)\n\nWeightedBagNode\nWeightedBagNode(::AbstractMillNode, ::AbstractVector, ::Vector, m)\n\nProductNode\nProductNode(; ns...)\n\nLazyNode\nLazyNode(::Symbol, ::Any)\n\nMill.unpack2mill\n\nMill.data\nMill.metadata\ndatasummary\ndropmeta\ncatobs\nMill.subset\nMill.mapdata\nremoveinstances\n","category":"page"},{"location":"api/data_nodes/#Mill.AbstractMillNode","page":"Data nodes","title":"Mill.AbstractMillNode","text":"AbstractMillNode\n\nSupertype for any structure representing a data node.\n\n\n\n\n\n","category":"type"},{"location":"api/data_nodes/#Mill.AbstractProductNode","page":"Data nodes","title":"Mill.AbstractProductNode","text":"AbstractProductNode <: AbstractMillNode\n\nSupertype for any structure representing a data node implementing a Cartesian product of data in subtrees.\n\n\n\n\n\n","category":"type"},{"location":"api/data_nodes/#Mill.AbstractBagNode","page":"Data nodes","title":"Mill.AbstractBagNode","text":"AbstractBagNode <: AbstractMillNode\n\nSupertype for any data node structure representing a multi-instance learning problem.\n\n\n\n\n\n","category":"type"},{"location":"api/data_nodes/#Mill.ArrayNode","page":"Data nodes","title":"Mill.ArrayNode","text":"ArrayNode{A <: AbstractArray, C} <: AbstractMillNode\n\nData node for storing array-like data of type A and metadata of type C. The convention is that samples are stored along the last axis, e.g. in columns of a matrix.\n\nSee also: AbstractMillNode, ArrayModel.\n\n\n\n\n\n","category":"type"},{"location":"api/data_nodes/#Mill.ArrayNode-Tuple{AbstractArray}","page":"Data nodes","title":"Mill.ArrayNode","text":"ArrayNode(d::AbstractArray, m=nothing)\n\nConstruct a new ArrayNode with data d and metadata m.\n\nExamples\n\njulia> a = ArrayNode([1 2; 3 4; 5 6])\n3×2 ArrayNode{Matrix{Int64}, Nothing}:\n 1  2\n 3  4\n 5  6\n\nSee also: AbstractMillNode, ArrayModel.\n\n\n\n\n\n","category":"method"},{"location":"api/data_nodes/#Mill.BagNode","page":"Data nodes","title":"Mill.BagNode","text":"BagNode{T <: Union{AbstractMillNode, Missing}, B <: AbstractBags, C} <: AbstractBagNode\n\nData node that represents a multi-instance learning problem. Contains instances stored in a subtree of type T, bag indices of type B and optional metadata of type C.\n\nSee also: WeightedBagNode, AbstractBagNode,     AbstractMillNode, BagModel.\n\n\n\n\n\n","category":"type"},{"location":"api/data_nodes/#Mill.BagNode-Tuple{AbstractMillNode, AbstractVector, Any}","page":"Data nodes","title":"Mill.BagNode","text":"BagNode(d::Union{AbstractMillNode, Missing}, b::AbstractBags, m=nothing)\nBagNode(d::Union{AbstractMillNode, Missing}, b::AbstractVector, m=nothing)\n\nConstruct a new BagNode with data d, bags b, and metadata m. If b is an AbstractVector, Mill.bags is applied first.\n\nExamples\n\njulia> BagNode(ArrayNode(maybehotbatch([1, missing, 2], 1:2)), AlignedBags([1:1, 2:3]))\nBagNode \t# 2 obs, 104 bytes\n  └── ArrayNode(2×3 MaybeHotMatrix with Union{Missing, Bool} elements) \t# 3 obs, 87 bytes\n\njulia> BagNode(ArrayNode(randn(2, 5)), [1, 2, 2, 1, 1])\nBagNode \t# 2 obs, 200 bytes\n  └── ArrayNode(2×5 Array with Float64 elements) \t# 5 obs, 128 bytes\n\nSee also: WeightedBagNode, AbstractBagNode,     AbstractMillNode, BagModel.\n\n\n\n\n\n","category":"method"},{"location":"api/data_nodes/#Mill.WeightedBagNode","page":"Data nodes","title":"Mill.WeightedBagNode","text":"WeightedBagNode{T <: Union{AbstractMillNode, Missing}, B <: AbstractBags, W, C} <: AbstractBagNode\n\nStructure like BagNode but allows to specify weights of type W of each instance.\n\nSee also: BagNode, AbstractBagNode, AbstractMillNode, BagModel.\n\n\n\n\n\n","category":"type"},{"location":"api/data_nodes/#Mill.WeightedBagNode-Tuple{AbstractMillNode, AbstractVector, Vector, Any}","page":"Data nodes","title":"Mill.WeightedBagNode","text":"WeightedBagNode(d::Union{AbstractMillNode, Missing}, b::AbstractBags, w::Vector, m=nothing)\nWeightedBagNode(d::Union{AbstractMillNode, Missing}, b::AbstractVector, w::Vector, m=nothing)\n\nConstruct a new WeightedBagNode with data d, bags b, weights w and metadata m. If b is an AbstractVector, Mill.bags is applied first.\n\nExamples\n\njulia> BagNode(ArrayNode(NGramMatrix([\"s1\", \"s2\"])), bags([1:2, 0:-1]), [0.2, 0.8])\nBagNode \t# 2 obs, 184 bytes\n  └── ArrayNode(2053×2 NGramMatrix with Int64 elements) \t# 2 obs, 140 bytes\n\njulia> BagNode(ArrayNode(zeros(2, 2)), [1, 2], [1, 2])\nBagNode \t# 2 obs, 160 bytes\n  └── ArrayNode(2×2 Array with Float64 elements) \t# 2 obs, 80 bytes\n\nSee also: BagNode, AbstractBagNode, AbstractMillNode, BagModel.\n\n\n\n\n\n","category":"method"},{"location":"api/data_nodes/#Mill.ProductNode","page":"Data nodes","title":"Mill.ProductNode","text":"ProductNode{T, C} <: AbstractProductNode\n\nData node representing a Cartesian product of several spaces each represented by subtree stored in iterable of type T. May store metadata of type C.\n\nSee also: AbstractProductNode, AbstractMillNode, ProductModel.\n\n\n\n\n\n","category":"type"},{"location":"api/data_nodes/#Mill.ProductNode-Tuple{}","page":"Data nodes","title":"Mill.ProductNode","text":"ProductNode(ds, m=nothing)\n\nConstruct a new ProductNode with data ds, and metadata m.\n\nds should be a Tuple or NamedTuple and all its elements must contain the same number of observations. If  ds is an AbstractMillNode a one-element Tuple is created.\n\nExamples\n\njulia> ProductNode((ArrayNode(zeros(2, 2)), ArrayNode(Flux.onehotbatch([1, 2], 1:2))))\nProductNode \t# 2 obs, 16 bytes\n  ├── ArrayNode(2×2 Array with Float64 elements) \t# 2 obs, 80 bytes\n  └── ArrayNode(2×2 OneHotArray with Bool elements) \t# 2 obs, 64 bytes\n\njulia> ProductNode(x1 = ArrayNode(NGramMatrix([\"Hello\", \"world\"])),\n                   x2 = BagNode(ArrayNode([1 2; 3 4]), [1:3, 4:4]))\nProductNode \t# 2 obs, 48 bytes\n  ├── x1: ArrayNode(2053×2 NGramMatrix with Int64 elements) \t# 2 obs, 146 bytes\n  └── x2: BagNode \t# 2 obs, 96 bytes\n            └── ArrayNode(2×2 Array with Int64 elements) \t# 2 obs, 80 bytes\n\njulia> ProductNode(ArrayNode([1 2 3]))\nProductNode \t# 3 obs, 8 bytes\n  └── ArrayNode(1×3 Array with Int64 elements) \t# 3 obs, 72 bytes\n\njulia> ProductNode((ArrayNode([1 2; 3 4]), ArrayNode([1; 3])))\nERROR: AssertionError: All subtrees must have an equal amount of instances!\n[...]\n\nSee also: AbstractProductNode, AbstractMillNode, ProductModel.\n\n\n\n\n\n","category":"method"},{"location":"api/data_nodes/#Mill.LazyNode","page":"Data nodes","title":"Mill.LazyNode","text":"LazyNode{Name, D, C} <: AbstractMillNode\n\nData node storing data of type D in a lazy manner and optional metadata of type C.\n\nSource of data or its type is specified in Name.\n\nSee also: AbstractMillNode, LazyModel, Mill.unpack2mill.\n\n\n\n\n\n","category":"type"},{"location":"api/data_nodes/#Mill.LazyNode-Tuple{Symbol, Any}","page":"Data nodes","title":"Mill.LazyNode","text":"LazyNode([Name::Symbol], d, m=nothing)\nLazyNode{Name}(d, m=nothing)\n\nConstruct a new LazyNode with name Name, data d, and metadata m.\n\nExamples\n\njulia> LazyNode(:Codons, [\"GGGCGGCGA\", \"CCTCGCGGG\"])\nLazyNode{:Codons, Vector{String}, Nothing}:\n \"GGGCGGCGA\"\n \"CCTCGCGGG\"\n\nSee also: AbstractMillNode, LazyModel, Mill.unpack2mill.\n\n\n\n\n\n","category":"method"},{"location":"api/data_nodes/#Mill.unpack2mill","page":"Data nodes","title":"Mill.unpack2mill","text":"Mill.unpack2mill(x::LazyNode)\n\nReturn a representation of LazyNode x using Mill.jl structures. Every custom LazyNode should have a special method as it is used in LazyModel.\n\nExamples\n\nfunction Mill.unpack2mill(ds::LazyNode{:Sentence})\n    s = split.(ds.data, \" \")\n    x = NGramMatrix(reduce(vcat, s))\n    BagNode(ArrayNode(x), Mill.length2bags(length.(s)))\nend\n\njulia> LazyNode{:Sentence}([\"foo bar\", \"baz\"]) |> Mill.unpack2mill\nBagNode \t# 2 obs, 120 bytes\n  └── ArrayNode(2053×3 NGramMatrix with Int64 elements) \t# 3 obs, 274 bytes\n\nSee also: LazyNode, LazyModel.\n\n\n\n\n\n","category":"function"},{"location":"api/data_nodes/#Mill.data","page":"Data nodes","title":"Mill.data","text":"Mill.data(n::AbstractMillNode)\n\nReturn data stored in node n.\n\nExamples\n\njulia> Mill.data(ArrayNode([1 2; 3 4], \"metadata\"))\n2×2 Matrix{Int64}:\n 1  2\n 3  4\n\njulia> Mill.data(BagNode(ArrayNode([1 2; 3 4]), bags([1:3, 4:4]), \"metadata\"))\n2×2 ArrayNode{Matrix{Int64}, Nothing}:\n 1  2\n 3  4\n\nSee also: Mill.metadata\n\n\n\n\n\n","category":"function"},{"location":"api/data_nodes/#Mill.metadata","page":"Data nodes","title":"Mill.metadata","text":"Mill.metadata(n::AbstractMillNode)\n\nReturn metadata stored in node n.\n\nExamples\n\njulia> Mill.metadata(ArrayNode([1 2; 3 4], \"metadata\"))\n\"metadata\"\n\njulia> Mill.metadata(BagNode(ArrayNode([1 2; 3 4]), bags([1:3, 4:4]), \"metadata\"))\n\"metadata\"\n\nSee also: Mill.data\n\n\n\n\n\n","category":"function"},{"location":"api/data_nodes/#Mill.datasummary","page":"Data nodes","title":"Mill.datasummary","text":"datasummary(n::AbstractMillNode)\n\nPrint summary of parameters of node n.\n\nExamples\n\njulia> n = ProductNode(ArrayNode(randn(2, 3)))\nProductNode \t# 3 obs, 8 bytes\n  └── ArrayNode(2×3 Array with Float64 elements) \t# 3 obs, 96 bytes\n\njulia> datasummary(n)\n\"Data summary: 3 obs, 112 bytes.\"\n\nSee also: modelsummary.\n\n\n\n\n\n","category":"function"},{"location":"api/data_nodes/#Mill.dropmeta","page":"Data nodes","title":"Mill.dropmeta","text":"dropmeta(n:AbstractMillNode)\n\nDrop metadata stored in data node n (recursively).\n\nExamples\n\njulia> n1 = ArrayNode(NGramMatrix([\"foo\", \"bar\"]), [\"metafoo\", \"metabar\"])\n2053×2 ArrayNode{NGramMatrix{String, Vector{String}, Int64}, Vector{String}}:\n \"foo\"\n \"bar\"\n\njulia> n2 = dropmeta(n1)\n2053×2 ArrayNode{NGramMatrix{String, Vector{String}, Int64}, Nothing}:\n \"foo\"\n \"bar\"\n\njulia> isnothing(Mill.metadata(n2))\ntrue\n\nSee also: Mill.metadata.\n\n\n\n\n\n","category":"function"},{"location":"api/data_nodes/#Mill.catobs","page":"Data nodes","title":"Mill.catobs","text":"catobs(ns...)\n\nMerge multiple nodes storing samples (observations) into one suitably promoting in the process if possible.\n\nSimilar to Base.cat but concatenates along the abstract \"axis\" where samples are stored.\n\nIn case of repeated calls with varying number of arguments or argument types, use reduce(catobs, [ns...]) to save compilation time.\n\nExamples\n\njulia> catobs(ArrayNode(zeros(2, 2)), ArrayNode([1 2; 3 4]))\n2×4 ArrayNode{Matrix{Float64}, Nothing}:\n 0.0  0.0  1.0  2.0\n 0.0  0.0  3.0  4.0\n\njulia> n = ProductNode(t1=ArrayNode(randn(2, 3)), t2=BagNode(ArrayNode(randn(3, 8)), bags([1:3, 4:5, 6:8])))\nProductNode \t# 3 obs, 24 bytes\n  ├── t1: ArrayNode(2×3 Array with Float64 elements) \t# 3 obs, 96 bytes\n  └── t2: BagNode \t# 3 obs, 112 bytes\n            └── ArrayNode(3×8 Array with Float64 elements) \t# 8 obs, 240 bytes\n\njulia> catobs(n[1], n[3])\nProductNode \t# 2 obs, 24 bytes\n  ├── t1: ArrayNode(2×2 Array with Float64 elements) \t# 2 obs, 80 bytes\n  └── t2: BagNode \t# 2 obs, 96 bytes\n            └── ArrayNode(3×6 Array with Float64 elements) \t# 6 obs, 192 bytes\n\nSee also: Mill.subset.\n\n\n\n\n\n","category":"function"},{"location":"api/data_nodes/#Mill.subset","page":"Data nodes","title":"Mill.subset","text":"subset(n, i)\n\nExtract a subset i of samples (observations) stored in node n.\n\nSimilar to Base.getindex or MLDataPattern.getobs but defined for all Mill.jl compatible data as well.\n\nExamples\n\njulia> Mill.subset(ArrayNode(NGramMatrix([\"Hello\", \"world\"])), 2)\n2053×1 ArrayNode{NGramMatrix{String, Vector{String}, Int64}, Nothing}:\n \"world\"\n\njulia> Mill.subset(BagNode(ArrayNode(randn(2, 8)), [1:2, 3:3, 4:7, 8:8]), 1:3)\nBagNode \t# 3 obs, 112 bytes\n  └── ArrayNode(2×7 Array with Float64 elements) \t# 7 obs, 160 bytes\n\nSee also: catobs.\n\n\n\n\n\n","category":"function"},{"location":"api/data_nodes/#Mill.mapdata","page":"Data nodes","title":"Mill.mapdata","text":"mapdata(f, x)\n\nRecursively apply f to data in all leaves of x.\n\nExamples\n\njulia> n1 = ProductNode(a=ArrayNode(zeros(2,2)), b=ArrayNode(ones(2,2)))\nProductNode \t# 2 obs, 16 bytes\n  ├── a: ArrayNode(2×2 Array with Float64 elements) \t# 2 obs, 80 bytes\n  └── b: ArrayNode(2×2 Array with Float64 elements) \t# 2 obs, 80 bytes\n\njulia> n2 = Mill.mapdata(x -> x .+ 1, n1)\nProductNode \t# 2 obs, 16 bytes\n  ├── a: ArrayNode(2×2 Array with Float64 elements) \t# 2 obs, 80 bytes\n  └── b: ArrayNode(2×2 Array with Float64 elements) \t# 2 obs, 80 bytes\n\njulia> Mill.data(n2).a\n2×2 ArrayNode{Matrix{Float64}, Nothing}:\n 1.0  1.0\n 1.0  1.0\n\njulia> Mill.data(n2).b\n2×2 ArrayNode{Matrix{Float64}, Nothing}:\n 2.0  2.0\n 2.0  2.0\n\n\n\n\n\n","category":"function"},{"location":"api/data_nodes/#Mill.removeinstances","page":"Data nodes","title":"Mill.removeinstances","text":"removeinstances(n::AbstractBagNode, mask)\n\nRemove instances from n using mask and remap bag indices accordingly.\n\nExamples\n\njulia> b1 = BagNode(ArrayNode([1 2 3; 4 5 6]), bags([1:2, 0:-1, 3:3]))\nBagNode \t# 3 obs, 112 bytes\n  └── ArrayNode(2×3 Array with Int64 elements) \t# 3 obs, 96 bytes\n\njulia> b2 = removeinstances(b1, [false, true, true])\nBagNode \t# 3 obs, 112 bytes\n  └── ArrayNode(2×2 Array with Int64 elements) \t# 2 obs, 80 bytes\n\njulia> b2.data\n2×2 ArrayNode{Matrix{Int64}, Nothing}:\n 2  3\n 5  6\n\njulia> b2.bags\nAlignedBags{Int64}(UnitRange{Int64}[1:1, 0:-1, 2:2])\n\n\n\n\n\n","category":"function"},{"location":"motivation/#Motivation","page":"Motivation","title":"Motivation","text":"","category":"section"},{"location":"motivation/","page":"Motivation","title":"Motivation","text":"In this section, we provide a short introduction into (hierarchical) multi instance learning. A much more detailed overview of this subject can be found in Simon Mandlik, Matej Racinsky, Viliam Lisy, Tomas Pevny (2021) and Šimon Mandlík, Tomáš Pevný (2020).","category":"page"},{"location":"motivation/#What-is-a-Multiple-instance-learning-problem?","page":"Motivation","title":"What is a Multiple instance learning problem?","text":"","category":"section"},{"location":"motivation/","page":"Motivation","title":"Motivation","text":"In Multiple Instance Learning (MIL), also Multi-Instance Learning, the sample bmx is a set of vectors (or matrices) x_1ldotsx_l, where x_i in mathbbR^d. As a result, order does not matter, which makes MIL problems different from sequences. In MIL parlance, sample bmx is also called a bag and its elements x_1 ldots x_2 instances. MIL problems have been introduced in Thomas G. Dietterich, Richard H. Lathrop, Tomás Lozano-Pérez (1997), and extended and generalized in a series of works Tomáš Pevný, Petr Somol (2017), Tomáš Pevný, Petr Somol (2016), Tomáš Pevný, Vojtěch Kovařík (2019). The most comprehensive introduction known to authors is Šimon Mandlík, Tomáš Pevný (2020).","category":"page"},{"location":"motivation/","page":"Motivation","title":"Motivation","text":"Why are MIL problems relevant? Since the seminal paper from Ronald A Fisher (1936), the majority of machine learning problems deals with problems like the one shown below:[1]","category":"page"},{"location":"motivation/","page":"Motivation","title":"Motivation","text":"[1]: Iris flower data set","category":"page"},{"location":"motivation/","page":"Motivation","title":"Motivation","text":"(Image: )","category":"page"},{"location":"motivation/","page":"Motivation","title":"Motivation","text":"where the input sample bmx is a vector (or generally speaking any tensor) of a fixed dimension containing various measurements of the specimen.","category":"page"},{"location":"motivation/","page":"Motivation","title":"Motivation","text":"Most of the time, a skilled botanist is able to identify a specimen not by making use of any measuring device, but by visual or tactile inspection of its stem, leaves and blooms. For different species, different parts of the flower may need to be examined for indicators. At the same time, many species may have nearly identical-looking leaves or blooms, therefore, one needs to step back, consider the whole picture, and appropriately combine lower-level observations into high-level conclusions about the given specimen.","category":"page"},{"location":"motivation/","page":"Motivation","title":"Motivation","text":"If we want to use such more elaborate description of the Iris flower using fixed size structures, we will have a hard time, because every specimen can have a different amounts of leaves or blooms (or they may be completely missing). This means that to use the usual fixed dimension paradigm, we have to either somehow select a single leaf (blossom) and extract features from them, or design procedures for aggregating such features over whole sets, so that the output has fixed dimension. This is clearly undesirable. Mill.jl a framework that seamlessly deals with these challenges in data representation.","category":"page"},{"location":"motivation/#Hierarchical-Multiple-Instance-Learning","page":"Motivation","title":"Hierarchical Multiple Instance Learning","text":"","category":"section"},{"location":"motivation/","page":"Motivation","title":"Motivation","text":"In Hierarchical Multiple Instance Learning (HMIL) the input may consists of not only sets, but also sets of sets and Cartesian Products of these structures. Returning to the previous Iris flower example, a specimen can be represented like this for HMIL:","category":"page"},{"location":"motivation/","page":"Motivation","title":"Motivation","text":"(Image: )","category":"page"},{"location":"motivation/","page":"Motivation","title":"Motivation","text":"The only stem is represented by vector bmx_s encoding its distinctive properties such as shape, color, structure or texture. Next, we inspect all blooms. Each of the blooms may have distinctive discriminative signs, therefore, we describe all three in vectors bmx_b_1 bmx_b_2 bmx_b_3, one vector for each bloom, and group them to a set. Finally, bmx_u represents the only flower which has not blossomed. Likewise, we could describe all leaves of the specimen if any were present. Here we assume that each specimen of the considered species has only one stem, but may have multiple flowers or leaves. Hence, all blooms and buds are represented as unordered sets of vectors as opposed to stem representation, which consists of only one vector.","category":"page"},{"location":"motivation/","page":"Motivation","title":"Motivation","text":"How does MIL models cope with variability in numbers of flowers and leaves? Each MIL model consists of two feed-forward neural networks with an element-wise aggregation operator like mean (or maximum) sandwiched between them. Denoting those feed-forward networks (FFNs) as f_1 and f_2, the output of the model applied to a bag is calculated for example as f_2 left(frac1lsum_i=1^l f_1(x_i) right) if we use mean as an aggregation function.","category":"page"},{"location":"motivation/","page":"Motivation","title":"Motivation","text":"The HMIL model corresponding to the Iris example above would comprise two FFNs and an aggregation to convert set of leafs to a single vector, and another two FFNs and an aggregation to convert set of blossoms to a single vector. These two outputs would be concatenated with a description of a stem, which would be fed to yet another FFN providing the final output. Since the whole scheme is differentiable, we can compute gradients and use any available gradient-based method to optimize the whole model at once using only labels on the level of output[2].","category":"page"},{"location":"motivation/","page":"Motivation","title":"Motivation","text":"[2]: Some methods for MIL problems require instance-level labels as well, which are not always available.","category":"page"},{"location":"motivation/","page":"Motivation","title":"Motivation","text":"The Mill.jl library simplifies implementation of machine learning problems using (H)MIL representation. In theory, it can represent any problem that can be represented in JSONs. That is why we have created a separate tool, JsonGrinder.jl, which helps with processing JSON documents for learning.","category":"page"},{"location":"motivation/","page":"Motivation","title":"Motivation","text":"In Tomáš Pevný, Vojtěch Kovařík (2019), authors have further extended the Universal approximation theorem to MIL problems, their Cartesian products, and nested MIL problems, i.e. a case where instances of one bag are in fact bags again.","category":"page"},{"location":"motivation/#Relation-to-Graph-Neural-Networks","page":"Motivation","title":"Relation to Graph Neural Networks","text":"","category":"section"},{"location":"motivation/","page":"Motivation","title":"Motivation","text":"HMIL problems can be seen as a special subset of general graphs. They differ in two important ways:","category":"page"},{"location":"motivation/","page":"Motivation","title":"Motivation","text":"In general graphs, vertices are of a small number of semantic type, whereas in HMIL problems, the number of semantic types of vertices is much higher (it is helpful to think about HMIL problems as about those for which JSON is a natural representation).\nThe computational graph of HMIL is a tree, which introduces assumption that there exist an efficient inference. Contrary, in general graphs (with loops) there is no efficient inference and one has to resort to message passing (Loopy belief propagation).\nOne update message in loopy belief propagation can be viewed as a MIL problem, as it has to produce a vector based on infomation inthe neighborhood, which can contain an arbitrary number of vertices.","category":"page"},{"location":"motivation/#Difference-to-sequences","page":"Motivation","title":"Difference to sequences","text":"","category":"section"},{"location":"motivation/","page":"Motivation","title":"Motivation","text":"The major difference is that instances in bag are not ordered in any way. This means that if a sequence (abc) should be treated as a set, then the output of a function f should be the same for any permutation, i.e. f(abc) = f(cba) = f(bac) = ldots.","category":"page"},{"location":"motivation/","page":"Motivation","title":"Motivation","text":"This property has a dramatic implication on the computational complexity. Sequences are typically modeled using Recurrent Neural Networks (RNNs), where the output is calculated roughly as f(abc) = g(a g(b g(c))). During optimization, a gradient of g needs to be calculated recursively, giving raise to infamous vanishing / exploding gradient problems. In constrast, (H)MIL models calculate the output as f(frac13(g(a) + g(b) + g(c))) (slightly abusing notation again), which means that the gradient of g can be calculated in parallel and not recurrently. ","category":"page"},{"location":"examples/dag/","page":"DAGs","title":"DAGs","text":"using Mill\n\nusing Graphs, GraphRecipes, Plots, Random\nRandom.seed!(4)\n\ng = SimpleDiGraph(8)\nfor e in [(1, 2), (1, 3), (2, 4), (2, 5), (3, 5),\n          (3, 6), (5, 7), (6, 5), (6, 8), (7, 8)]\n    add_edge!(g, e...)\nend\n\ngp = graphplot(adjacency_matrix(g); linecolor = :darkgrey,\n                                    nodecolor=:lightgrey,\n                                    fontsize=11,\n                                    markersize=0.2, nodeshape=:circle,\n                                    background_color=:transparent,\n                                    names=range('a', length=nv(g))\n                                    )\nsavefig(gp, \"dag.svg\")","category":"page"},{"location":"examples/dag/#DAGs","page":"DAGs","title":"DAGs","text":"","category":"section"},{"location":"examples/dag/","page":"DAGs","title":"DAGs","text":"using Mill, Flux, Zygote, ChainRulesCore, Graphs","category":"page"},{"location":"examples/dag/","page":"DAGs","title":"DAGs","text":"Imagine a data/knowledge base represented in a form of a directed acyclic graph (DAG), where a vertex would be modelled based on its parents (and their parents), but not on its descendants. We will make one assumption (common in graphical models) that two children are independent given their parent or, in other words, once we have access to the data or inferred values of the parent, we do not have to inspect its other children.","category":"page"},{"location":"examples/dag/","page":"DAGs","title":"DAGs","text":"For example, in the graph below, when we infer some value for vertex e, we ignore vertices d, g, and h:","category":"page"},{"location":"examples/dag/","page":"DAGs","title":"DAGs","text":"(Image: )","category":"page"},{"location":"examples/dag/","page":"DAGs","title":"DAGs","text":"Firstly, we define a new type for our graph that would be able to store a structure of the graph together with vertex features:","category":"page"},{"location":"examples/dag/","page":"DAGs","title":"DAGs","text":"struct DagGraph{G <: SimpleDiGraph,T}\n    g::G\n    vertex_features::T\nend\n\nnothing # hide","category":"page"},{"location":"examples/dag/","page":"DAGs","title":"DAGs","text":"Then, we specify a model type:","category":"page"},{"location":"examples/dag/","page":"DAGs","title":"DAGs","text":"struct DagModel{M}\n    m::M\n    od::Int\nend\n\nFlux.@functor DagModel\n\nnothing # hide","category":"page"},{"location":"examples/dag/","page":"DAGs","title":"DAGs","text":"Here, m is a function realizing one step of the message passing procedure, and od is output dimension.","category":"page"},{"location":"examples/dag/","page":"DAGs","title":"DAGs","text":"In the course of calculating the value of vertex e, it may happen that for some vertices we will have to compute their value multiple times (for example for c, once when reached from e and once from f). Ideally, we would like to have a cache of already calculated values, which is difficult to do when autodifferentiating with Zygote as it does not support setindex operation. However, since the cache is assigned only once this can be realized through Zygote.buffer. We begin by initializing the cache:","category":"page"},{"location":"examples/dag/","page":"DAGs","title":"DAGs","text":"initcache(g, k) = [Zygote.Buffer(zeros(Float32, k, 1)) for _ in 1:nv(g)]\nChainRulesCore.@non_differentiable initcache(g, k)\nnothing # hide","category":"page"},{"location":"examples/dag/","page":"DAGs","title":"DAGs","text":"To get the already computed value of a vertex when using the model, we just delegate the question to cache as ","category":"page"},{"location":"examples/dag/","page":"DAGs","title":"DAGs","text":"function (model::DagModel)(g::DagGraph, i)\n    cache = initcache(g.g, model.od)\n    ArrayNode(getfromcache!(cache, g, model, i))\nend\n\n(model::DagModel)(g::SimpleDiGraph, vertex_features, i) = model(DagGraph(g, vertex_features), i)\n\nnothing # hide","category":"page"},{"location":"examples/dag/","page":"DAGs","title":"DAGs","text":"which means that the getfromcache! will do all the heavy lifting. It turns out that this function just has to check if the value in cache has been already calculated. If not, it will calculate the value (applying model on millvertex!) and freeze the calculated item in cache:","category":"page"},{"location":"examples/dag/","page":"DAGs","title":"DAGs","text":"function getfromcache!(cache, g::DagGraph, model::DagModel, i::Int)\n    cache[i].freeze && return(copy(cache[i]))\n    ds = millvertex!(cache, g, model, i)\n    cache[i][:] = model.m(ds) |> Mill.data\n    return(copy(cache[i]))\nend\n\nfunction getfromcache!(cache, g::DagGraph, model::DagModel, ii::Vector{Int}) \n    reduce(catobs, [getfromcache!(cache, g, model, i) for i in ii])\nend\n\nnothing # hide","category":"page"},{"location":"examples/dag/","page":"DAGs","title":"DAGs","text":"And what does millvertex! function do? It just takes the representation of ancestors (from cache) and put them together:","category":"page"},{"location":"examples/dag/","page":"DAGs","title":"DAGs","text":"function millvertex!(cache, g::DagGraph, model::DagModel, i)\n    ProductNode(neighbours = millneighbors!(cache, g, model, i), \n                vertex = vertex_features[i]\n    )\nend\nnothing # hide","category":"page"},{"location":"examples/dag/","page":"DAGs","title":"DAGs","text":"The last missing piece is millneighbors! definition:","category":"page"},{"location":"examples/dag/","page":"DAGs","title":"DAGs","text":"function millneighbors!(cache, g::DagGraph, model::DagModel, ii::Vector{Int})\n    isempty(ii) && return(BagNode(missing, [0:-1]))\n    xs = [getfromcache!(cache, g, model, i) for i in  ii]\n    BagNode(ArrayNode(reduce(catobs, xs)), [1:length(xs)])\nend\n\nmillneighbors!(cache, g::DagGraph, model::DagModel, i::Int) = millneighbors!(cache, g, model, inneighbors(g.g, i))\nChainRulesCore.@non_differentiable inneighbors(g, i)\nnothing # hide","category":"page"},{"location":"examples/dag/","page":"DAGs","title":"DAGs","text":"Note that this recursive approach is not the most efficient way to implement this. It would be better to spent a little time with graphs to identify sets of vertices that can be processed in parallel and for which all ancestors are known. But this was a fun little exercise.","category":"page"},{"location":"examples/jsons/","page":"Processing JSONs","title":"Processing JSONs","text":"<p align=\"center\">\n    <a href=\"https://github.com/CTUAvastLab/JsonGrinder.jl\">\n        <img src=\"https://raw.githubusercontent.com/CTUAvastLab/JsonGrinder.jl/master/docs/src/assets/logo.svg\" alt=\"JsonGrinder.jl logo\"/>\n    </a>\n</p>","category":"page"},{"location":"examples/jsons/#Processing-JSONs","page":"Processing JSONs","title":"Processing JSONs","text":"","category":"section"},{"location":"examples/jsons/","page":"Processing JSONs","title":"Processing JSONs","text":"Processing JSONs is actually one of the main motivations for building Mill.jl. As a matter of fact, with Mill one is now able to process a set of valid JSON documents that follow the same meta schema. JsonGrinder.jl is a library that helps with infering the schema and other steps in the pipeline. For some examples, please refer to its documentation.","category":"page"},{"location":"manual/missing/","page":"Missing data","title":"Missing data","text":"using Mill, Flux","category":"page"},{"location":"manual/missing/#Missing-data","page":"Missing data","title":"Missing data","text":"","category":"section"},{"location":"manual/missing/","page":"Missing data","title":"Missing data","text":"One detail that was left out so far is how Mill.jl handles incomplete or missing data. This phenomenon is nowadays ubiquitous in many data sources and occurs due to:","category":"page"},{"location":"manual/missing/","page":"Missing data","title":"Missing data","text":"a high price of obtaining a (part of) observation\ninformation being unreachable due to privacy reasons\na gradual change in the definition of data being gathered\na faulty collection process","category":"page"},{"location":"manual/missing/","page":"Missing data","title":"Missing data","text":"and many other possible reasons. At the same time, it is wasteful to throw away the incomplete observations altogether. Thanks to the hierarchical structure of both samples and models, we can still process samples with missing information fragments at various levels of abstraction. Problems of this type can be categorized into 3 not necessarily separate types:","category":"page"},{"location":"manual/missing/","page":"Missing data","title":"Missing data","text":"Missing parts of raw-data in a leaf ArrayNode\nEmpty bags with no instances in a BagNode\nAnd entire key missing in a ProductNode","category":"page"},{"location":"manual/missing/","page":"Missing data","title":"Missing data","text":"At the moment, Mill is capable of handling the first two cases. The solution always involves an additional vector of parameters (denoted always by ψ) that are used during the model evaluation to substitute the missing values. Parameters ψ can be either fixed or learned during training. Everything is done automatically.","category":"page"},{"location":"manual/missing/#Empty-bags","page":"Missing data","title":"Empty bags","text":"","category":"section"},{"location":"manual/missing/","page":"Missing data","title":"Missing data","text":"It may happen that some bags in the datasets are empty by definition or no associated instances were obtained during data collection. Recall, that an empty bag is specified as empty range 0:-1 in case of AlignedBags and as an empty vector Int[] when ScatteredBags are used:","category":"page"},{"location":"manual/missing/","page":"Missing data","title":"Missing data","text":"empty_bags_1 = AlignedBags([1:2, 0:-1, 3:5, 0:-1])\nempty_bags_2 = ScatteredBags([[1, 2], Int[], [3, 4, 5], Int[]])","category":"page"},{"location":"manual/missing/","page":"Missing data","title":"Missing data","text":"To obtain the vector representation for a bag, be it for dircetly predicting some value or using it to represent some higher-level structures, we need to deal with these empty bags. This is done in Bag aggregation. Each AbstractAggregation operator carries a vector of parameters ψ, initialized to zeros upon creation:","category":"page"},{"location":"manual/missing/","page":"Missing data","title":"Missing data","text":"a = SegmentedSumMax(2)","category":"page"},{"location":"manual/missing/","page":"Missing data","title":"Missing data","text":"When we evaluate any BagModel, these values are used to compute output for empty bags instead of the aggregation itself. See the demo below:","category":"page"},{"location":"manual/missing/","page":"Missing data","title":"Missing data","text":"an = ArrayNode(randn(Float32, 2, 5))\nds = BagNode(an, empty_bags_2)\nm = BagModel(identity, a, identity)\nm(ds)","category":"page"},{"location":"manual/missing/","page":"Missing data","title":"Missing data","text":"Vector ψ is learnable and therefore after training will contain a suitable representation of an empty bag for the given problem.","category":"page"},{"location":"manual/missing/","page":"Missing data","title":"Missing data","text":"When a BagNode is entirely empty, it can be constructed with missing instead of a matrix wrapped in an ArrayNode:","category":"page"},{"location":"manual/missing/","page":"Missing data","title":"Missing data","text":"bn1 = BagNode(ArrayNode(rand(3, 4)), [1:4])\nbn2 = BagNode(missing, [0:-1])","category":"page"},{"location":"manual/missing/","page":"Missing data","title":"Missing data","text":"and everything will work as expected. For example, we can concatenate these two:","category":"page"},{"location":"manual/missing/","page":"Missing data","title":"Missing data","text":"x = catobs(bn1, bn2)","category":"page"},{"location":"manual/missing/","page":"Missing data","title":"Missing data","text":"Notice, that the resulting ArrayNode has still the same dimension as ArrayNode inside bn1. The emptiness of bn2 is stored in bags:","category":"page"},{"location":"manual/missing/","page":"Missing data","title":"Missing data","text":"x.bags","category":"page"},{"location":"manual/missing/","page":"Missing data","title":"Missing data","text":"The second element BagNode can be obtained again by indexing:","category":"page"},{"location":"manual/missing/","page":"Missing data","title":"Missing data","text":"bn1 == x[2]","category":"page"},{"location":"manual/missing/","page":"Missing data","title":"Missing data","text":"Even though this approach of using missing for data field in BagNodes is the most accurate from the semantic point of view, it may cause excessive compilation, as the types will be different. Therefore, if this happens in multiple places in the sample tree, it may be better to instead use an empty matrix for type consistency:","category":"page"},{"location":"manual/missing/","page":"Missing data","title":"Missing data","text":"BagNode(ArrayNode(zeros(3, 0)), [0:-1])","category":"page"},{"location":"manual/missing/","page":"Missing data","title":"Missing data","text":"How indexing behaves with respect to this issue depends on a global switch (off by default) and  can be changed with the Mill.emptyismissing! function:","category":"page"},{"location":"manual/missing/","page":"Missing data","title":"Missing data","text":"a = BagNode(ArrayNode(rand(3, 2)), [1:2, 0:-1, 0:-1])\na[2:3] |> Mill.data\nMill.emptyismissing!(true)\na[2:3] |> Mill.data\nmissing","category":"page"},{"location":"manual/missing/","page":"Missing data","title":"Missing data","text":"Mill.emptyismissing!(false)","category":"page"},{"location":"manual/missing/#Post-imputing","page":"Missing data","title":"Post imputing","text":"","category":"section"},{"location":"manual/missing/","page":"Missing data","title":"Missing data","text":"Storing missing strings in NGramMatrix is straightforward:","category":"page"},{"location":"manual/missing/","page":"Missing data","title":"Missing data","text":"missing_ngrams = NGramMatrix([\"foo\", missing, \"bar\"], 3, 256, 5)","category":"page"},{"location":"manual/missing/","page":"Missing data","title":"Missing data","text":"When some values of categorical variables are missing, Mill defines a new type for representation:","category":"page"},{"location":"manual/missing/","page":"Missing data","title":"Missing data","text":"missing_categorical = maybehotbatch([missing, 2, missing], 1:5)","category":"page"},{"location":"manual/missing/","page":"Missing data","title":"Missing data","text":"MaybeHotMatrix behaves similarly as OneHotMatrix from Flux.jl, but it supports possibly missing values. In case when no values are missing it looks exactly like OneHotMatrix:","category":"page"},{"location":"manual/missing/","page":"Missing data","title":"Missing data","text":"maybehotbatch([5, 2, 1], 1:5)","category":"page"},{"location":"manual/missing/","page":"Missing data","title":"Missing data","text":"MaybeHotMatrix behaves like AbstractMatrix and supports left multiplication again:","category":"page"},{"location":"manual/missing/","page":"Missing data","title":"Missing data","text":"missing_categorical::AbstractMatrix{Union{Bool, Missing}}","category":"page"},{"location":"manual/missing/","page":"Missing data","title":"Missing data","text":"However, multiplying these matrices with missing data leads into missing data in the output.","category":"page"},{"location":"manual/missing/","page":"Missing data","title":"Missing data","text":"W = rand(2, 5)\nW * missing_ngrams\nW * missing_categorical","category":"page"},{"location":"manual/missing/","page":"Missing data","title":"Missing data","text":"Consequently, gradient can't be computed and any model can't be trained.","category":"page"},{"location":"manual/missing/","page":"Missing data","title":"Missing data","text":"ukn: Model debugging\nFlux gradient call returns an error like Output should be scalar; gradients are not defined for output missing when attempted on missing result. In a similar fashion as having NaNs in a model, this signifies that some missing input is not treated anywhere in the model and it propagates up. Generally speaking, it is recommended to deal with missing values as soon as possible (on the leaf level) so that they do not propagate and cause type instabilities.","category":"page"},{"location":"manual/missing/","page":"Missing data","title":"Missing data","text":"PostImputingMatrix is a solution for this. It can be constructed as follows:","category":"page"},{"location":"manual/missing/","page":"Missing data","title":"Missing data","text":"A = PostImputingMatrix(W)","category":"page"},{"location":"manual/missing/","page":"Missing data","title":"Missing data","text":"Matrix W is stored inside and A creates one vector of parameters ψ of length size(W, 1) on top of that. Suddenly, multiplication automagically works:","category":"page"},{"location":"manual/missing/","page":"Missing data","title":"Missing data","text":"A * missing_ngrams\nA * missing_categorical","category":"page"},{"location":"manual/missing/","page":"Missing data","title":"Missing data","text":"What happens under the hood is that whenever A encounters a missing column in the matrix, it fills in values from ψ after the multiplication is performed (effectively replacing all missing values in the result of multiplying with W, but implemented more efficiently). Vector ψ can be learned during training as well and everything works out of the box.","category":"page"},{"location":"manual/missing/#Pre-imputing","page":"Missing data","title":"Pre imputing","text":"","category":"section"},{"location":"manual/missing/","page":"Missing data","title":"Missing data","text":"If we have to deal with inputs where some elements of input matrix are missing:","category":"page"},{"location":"manual/missing/","page":"Missing data","title":"Missing data","text":"X = [missing 1 2; 3 missing missing]","category":"page"},{"location":"manual/missing/","page":"Missing data","title":"Missing data","text":"we can make use of PreImputingMatrix:","category":"page"},{"location":"manual/missing/","page":"Missing data","title":"Missing data","text":"W = rand(1:2, 3, 2)\nA = PreImputingMatrix(W)","category":"page"},{"location":"manual/missing/","page":"Missing data","title":"Missing data","text":"As opposed to PostImputingMatrix, A now stores a vector of values ψ with length size(W, 2). When we use it for multiplication:","category":"page"},{"location":"manual/missing/","page":"Missing data","title":"Missing data","text":"A * X","category":"page"},{"location":"manual/missing/","page":"Missing data","title":"Missing data","text":"what happens is that when we perform a dot product of a row of A and a column of X, we first fill in values from ψ into the column before the multiplication is performed. Again, it is possible to compute gradients with respect to all three of W, ψ and X and therefore learn the appropriate default values in ψ from the data:","category":"page"},{"location":"manual/missing/","page":"Missing data","title":"Missing data","text":"gradient((A, X) -> sum(A * X), A, X)","category":"page"},{"location":"manual/missing/#Model-reflection-with-missing-values","page":"Missing data","title":"Model reflection with missing values","text":"","category":"section"},{"location":"manual/missing/","page":"Missing data","title":"Missing data","text":"Model reflection takes missing values and types into account and creates appropriate (sub)models to handle them:","category":"page"},{"location":"manual/missing/","page":"Missing data","title":"Missing data","text":"ds = ProductNode(ArrayNode.((missing_ngrams, missing_categorical, X)))\nm = reflectinmodel(ds)","category":"page"},{"location":"manual/missing/","page":"Missing data","title":"Missing data","text":"Here, [pre_imputing]Dense and [post_imputing]Dense are standard dense layers with a special matrix inside:","category":"page"},{"location":"manual/missing/","page":"Missing data","title":"Missing data","text":"dense = m.ms[1].m; typeof(dense.W)","category":"page"},{"location":"manual/missing/","page":"Missing data","title":"Missing data","text":"Inside Mill we add a special definition Base.show for these types for compact printing.","category":"page"},{"location":"manual/missing/","page":"Missing data","title":"Missing data","text":"The reflectinmodel method use types to determine whether imputing is needed or not. Compare the following:","category":"page"},{"location":"manual/missing/","page":"Missing data","title":"Missing data","text":"reflectinmodel(ArrayNode(randn(2, 3)))\nreflectinmodel(ArrayNode([1.0 2.0 missing; 4.0 missing missing]))\nreflectinmodel(ArrayNode(Matrix{Union{Missing, Float64}}(randn(2, 3))))","category":"page"},{"location":"manual/missing/","page":"Missing data","title":"Missing data","text":"In the last case, the imputing type is returned even though there is no missing element in the matrix. Of course, the same applies to MaybeHotVector, MaybeHotMatrix and NGramMatrix. This way, we can signify that even though there are no missing values in the available sample, we expect them to appear in the future and want our model compatible. If it is hard to determine this in advance a safe bet is to make all leaves in the model. The performance will not suffer because imputing types are as fast as their non-imputing counterparts on data not containing missing values and the only tradeoff is a slight increase in the number of parameters, some of which may never be used.","category":"page"},{"location":"api/model_nodes/#Model-nodes","page":"Model nodes","title":"Model nodes","text":"","category":"section"},{"location":"api/model_nodes/#Index","page":"Model nodes","title":"Index","text":"","category":"section"},{"location":"api/model_nodes/","page":"Model nodes","title":"Model nodes","text":"Pages = [\"model_nodes.md\"]","category":"page"},{"location":"api/model_nodes/#API","page":"Model nodes","title":"API","text":"","category":"section"},{"location":"api/model_nodes/","page":"Model nodes","title":"Model nodes","text":"AbstractMillModel\n\nArrayModel\nidentity_model\nIdentityModel\n\nBagModel\nBagModel(::AbstractMillModel, ::Union{AbstractAggregation, BagCount}, ::ArrayModel)\n\nProductModel\nProductModel(::Tuple{AbstractMillModel}, ::ArrayModel)\n\nLazyModel\nLazyModel(::Symbol, ::AbstractMillModel)\n\nreflectinmodel\nmodelsummary","category":"page"},{"location":"api/model_nodes/#Mill.AbstractMillModel","page":"Model nodes","title":"Mill.AbstractMillModel","text":"AbstractMillModel\n\nSupertype for any model defined in Mill.jl.\n\n\n\n\n\n","category":"type"},{"location":"api/model_nodes/#Mill.ArrayModel","page":"Model nodes","title":"Mill.ArrayModel","text":"ArrayModel{T} <: AbstractMillModel\n\nA model node for processing ArrayNodes. It applies a (sub)model m stored in it to data of  the ArrayNode.\n\nExamples\n\njulia> Random.seed!(0);\n\njulia> n = ArrayNode(randn(Float32, 2, 2))\n2×2 ArrayNode{Matrix{Float32}, Nothing}:\n 0.679...  -0.353...\n 0.828...  -0.135...\n\njulia> m = ArrayModel(Dense(2, 2))\nArrayModel(Dense(2, 2)) \t# 2 arrays, 6 params, 104 bytes\n\njulia> m(n)\n2×2 ArrayNode{Matrix{Float32}, Nothing}:\n 0.661...  -0.188...\n 0.101...   0.275...\n\nSee also: AbstractMillModel, IdentityModel, identity_model, ArrayNode.\n\n\n\n\n\n","category":"type"},{"location":"api/model_nodes/#Mill.identity_model","page":"Model nodes","title":"Mill.identity_model","text":"identity_model()\n\nReturns an ArrayModel realising the identity transformation.\n\nExamples\n\njulia> identity_model()\nArrayModel(identity)\n\nSee also: ArrayModel, IdentityModel.\n\n\n\n\n\n","category":"function"},{"location":"api/model_nodes/#Mill.IdentityModel","page":"Model nodes","title":"Mill.IdentityModel","text":"IdentityModel\n\nAlias for ArrayModel{typeof(identity)}.\n\nSee also: ArrayModel, identity_model.\n\n\n\n\n\n","category":"type"},{"location":"api/model_nodes/#Mill.BagModel","page":"Model nodes","title":"Mill.BagModel","text":"BagModel{T <: AbstractMillModel, A <: Union{AbstractAggregation, BagCount}, U <: ArrayModel}\n    <: AbstractMillModel\n\nA model node for processing AbstractBagNodes. It first applies its \"instance (sub)model\" im on every instance, then performs elementwise segmented aggregation a and finally applies the final model bm on the aggregated representation of every bag in the data node.\n\nExamples\n\njulia> Random.seed!(0);\n\njulia> n = BagNode(ArrayNode(randn(3, 2)), bags([0:-1, 1:2]))\nBagNode \t# 2 obs, 96 bytes\n  └── ArrayNode(3×2 Array with Float64 elements) \t# 2 obs, 96 bytes\n\njulia> m = BagModel(ArrayModel(Dense(3, 2)), SegmentedMeanMax(2), ArrayModel(Dense(4, 2)))\nBagModel ↦ [SegmentedMean(2); SegmentedMax(2)] ↦ ArrayModel(Dense(4, 2)) \t# 4 arrays, 14 params, 216 bytes\n  └── ArrayModel(Dense(3, 2)) \t# 2 arrays, 8 params, 112 bytes\n\njulia> m(n)\n2×2 ArrayNode{Matrix{Float64}, Nothing}:\n 0.0  -2.049...\n 0.0  -0.906...\n\njulia> m.bm(m.a(m.im(n.data), n.bags))\n2×2 ArrayNode{Matrix{Float64}, Nothing}:\n 0.0  -2.049...\n 0.0  -0.906...\n\nSee also: AbstractMillModel, AbstractAggregation, AbstractBagNode,     BagNode, WeightedBagNode.\n\n\n\n\n\n","category":"type"},{"location":"api/model_nodes/#Mill.BagModel-Tuple{AbstractMillModel, Union{AbstractAggregation, BagCount}, ArrayModel}","page":"Model nodes","title":"Mill.BagModel","text":"BagModel(im, a, bm=identity_model())\n\nConstruct a BagModel from the arguments. im should be AbstractMillModel, a AbstractAggregation or BagCount, and bm ArrayModel.\n\nIt is also possible to pass any function (Flux.Dense, Flux.Chain, identity...) as im or bm. In that case, they are wrapped into an ArrayNode.\n\nExamples\n\njulia> m = BagModel(ArrayModel(Dense(3, 2)), SegmentedMeanMax(2), ArrayModel(Dense(4, 2)))\nBagModel ↦ [SegmentedMean(2); SegmentedMax(2)] ↦ ArrayModel(Dense(4, 2)) \t# 4 arrays, 14 params, 216 bytes\n  └── ArrayModel(Dense(3, 2)) \t# 2 arrays, 8 params, 112 bytes\n\njulia> m = BagModel(Dense(4, 3), BagCount(SegmentedMean(3)))\nBagModel ↦ BagCount(SegmentedMean(3)) ↦ ArrayModel(identity) \t# 1 arrays, 3 params (all zero), 52 bytes\n  └── ArrayModel(Dense(4, 3)) \t# 2 arrays, 15 params, 140 bytes\n\nSee also: AbstractMillModel, AbstractAggregation, BagCount,     AbstractBagNode, BagNode, WeightedBagNode.\n\n\n\n\n\n","category":"method"},{"location":"api/model_nodes/#Mill.ProductModel","page":"Model nodes","title":"Mill.ProductModel","text":"ProductModel{T <: Mill.VecOrTupOrNTup{<:AbstractMillModel}, U <: ArrayModel} <: AbstractMillModel\n\nA model node for processing ProductNodes. For each subtree of the data node it applies one (sub)model from ms and then applies m on the concatenation of results.\n\nExamples\n\njulia> Random.seed!(0);\n\njulia> n = ProductNode(a=ArrayNode([0 1; 2 3]), b=ArrayNode([4 5; 6 7]))\nProductNode \t# 2 obs, 16 bytes\n  ├── a: ArrayNode(2×2 Array with Int64 elements) \t# 2 obs, 80 bytes\n  └── b: ArrayNode(2×2 Array with Int64 elements) \t# 2 obs, 80 bytes\n\njulia> m1 = ProductModel((a=ArrayModel(Dense(2, 2)), b=ArrayModel(Dense(2, 2))))\nProductModel ↦ ArrayModel(identity)\n  ├── a: ArrayModel(Dense(2, 2)) \t# 2 arrays, 6 params, 104 bytes\n  └── b: ArrayModel(Dense(2, 2)) \t# 2 arrays, 6 params, 104 bytes\n\njulia> m1(n)\n4×2 ArrayNode{Matrix{Float32}, Nothing}:\n -1.284...  -1.254...\n  1.802...   3.711...\n -4.036...  -5.013...\n  0.587...   0.372...\n\njulia> m2 = ProductModel((a=identity, b=identity))\nProductModel ↦ ArrayModel(identity)\n  ├── a: ArrayModel(identity)\n  └── b: ArrayModel(identity)\n\njulia> m2(n)\n4×2 ArrayNode{Matrix{Int64}, Nothing}:\n 0  1\n 2  3\n 4  5\n 6  7\n\nSee also: AbstractMillModel, AbstractProductNode, ProductNode.\n\n\n\n\n\n","category":"type"},{"location":"api/model_nodes/#Mill.ProductModel-Tuple{Tuple{AbstractMillModel}, ArrayModel}","page":"Model nodes","title":"Mill.ProductModel","text":"ProductModel(ms, m=identity_model())\n\nConstruct a ProductModel from the arguments. ms should an iterable (Tuple, NamedTuple or Vector) of one or more AbstractMillModels, and m should be an ArrayModel.\n\nIt is also possible to pass any function (Flux.Dense, Flux.Chain, identity...) as elements of ms or as m. In that case, they are wrapped into an ArrayNode.\n\nIf ms is AbstractMillModel, a one-element Tuple is constructed from it.\n\nExamples\n\njulia> ProductModel((a=ArrayModel(Dense(2, 2)), b=identity))\nProductModel ↦ ArrayModel(identity)\n  ├── a: ArrayModel(Dense(2, 2)) \t# 2 arrays, 6 params, 104 bytes\n  └── b: ArrayModel(identity)\n\njulia> ProductModel((identity_model(), BagModel(ArrayModel(Dense(2, 2)), SegmentedMean(2), identity)))\nProductModel ↦ ArrayModel(identity)\n  ├── ArrayModel(identity)\n  └── BagModel ↦ SegmentedMean(2) ↦ ArrayModel(identity) \t# 1 arrays, 2 params (all zero), 48 bytes\n        └── ArrayModel(Dense(2, 2)) \t# 2 arrays, 6 params, 104 bytes\n\njulia> ProductModel(identity)\nProductModel ↦ ArrayModel(identity)\n  └── ArrayModel(identity)\n\nSee also: AbstractMillModel, AbstractProductNode, ProductNode.\n\n\n\n\n\n","category":"method"},{"location":"api/model_nodes/#Mill.LazyModel","page":"Model nodes","title":"Mill.LazyModel","text":"LazyModel{Name, T} <: AbstractMillModel\n\nA model node for processing LazyNodes. It applies a (sub)model m stored in it to data of the LazyNode after calling Mill.unpack2mill.\n\nExamples\n\nfunction Mill.unpack2mill(ds::LazyNode{:Sentence})\n    s = split.(ds.data, \" \")\n    x = NGramMatrix(reduce(vcat, s))\n    BagNode(ArrayNode(x), Mill.length2bags(length.(s)))\nend\n# output\n\njulia> Random.seed!(0);\n\njulia> n = LazyNode{:Sentence}([\"foo bar\", \"baz\"])\nLazyNode{:Sentence, Vector{String}, Nothing}:\n \"foo bar\"\n \"baz\"\n\njulia> m = LazyModel{:Sentence}(BagModel(Dense(2053, 3), SegmentedMean(3), identity))\nLazyModel{Sentence}\n  └── BagModel ↦ SegmentedMean(3) ↦ ArrayModel(identity) \t# 1 arrays, 3 params (all zero), 52 bytes\n        └── ArrayModel(Dense(2053, 3)) \t# 2 arrays, 6_162 params, 24.148 KiB\n\njulia> m(n)\n3×2 ArrayNode{Matrix{Float32}, Nothing}:\n -0.006...  -0.022...\n  0.034...   0.055...\n -0.062...   0.071...\n\nSee also: AbstractMillModel, LazyNode, Mill.unpack2mill.\n\n\n\n\n\n","category":"type"},{"location":"api/model_nodes/#Mill.LazyModel-Tuple{Symbol, AbstractMillModel}","page":"Model nodes","title":"Mill.LazyModel","text":"LazyModel([Name::Symbol], m::AbstractMillModel)\nLazyModel{Name}(m::AbstractMillModel)\n\nConstruct a new LazyModel with name Name, and model m.\n\nExamples\n\njulia> LazyModel{:Sentence}(ArrayModel(Dense(2, 2)))\nLazyModel{Sentence}\n  └── ArrayModel(Dense(2, 2)) \t# 2 arrays, 6 params, 104 bytes\n\nSee also: AbstractMillModel, LazyNode, Mill.unpack2mill.\n\n\n\n\n\n","category":"method"},{"location":"api/model_nodes/#Mill.reflectinmodel","page":"Model nodes","title":"Mill.reflectinmodel","text":"reflectinmodel(x::AbstractMillNode, fm=d -> Dense(d, 10), fa=BagCount ∘ SegmentedMeanMax;\n    fsm=Dict(), fsa=Dict(), single_key_identity=true, single_scalar_identity=true, all_imputing=false)\n\nBuild a Mill.jl model capable of processing x.\n\nAll inner Dense layers are constructed using fm, a function accepting input dimension d and returning a suitable model. All aggregation operators are constructed using fa in a similar manner.\n\nMore fine-grained control can be achieved with fsm and fsa keyword arguments, which should be Dicts of c => f pairs, where c is a String traversal code from HierarchicalUtils.jl and f is a function. These definitions override fm and fa.\n\nIf a ProductNode with only a single child (subtree) is encountered, its final m model is instantiated as identity instead of using fm and fsm. This can be controlled with single_key_identity.\n\nSimilarly, if an ArrayNode contains data X where size(X, 1) is 1, the corresponding model is instantiated as identity unless single_scalar_identity is false.\n\nBy default, reflectinmodel makes first Dense layers in leafs imputing only if the datatype suggests that missing data is present. If all_imputing is true, all such Dense layers are replaced by their imputing variants.\n\nExamples\n\njulia> n1 = ProductNode(a=ArrayNode(NGramMatrix([\"a\", missing])))\nProductNode \t# 2 obs, 32 bytes\n  └── a: ArrayNode(2053×2 NGramMatrix with Union{Missing, Int64} elements) \t# 2 obs, 129 bytes\n\njulia> n2 = ProductNode((ArrayNode([0 1]), BagNode(ArrayNode([0 1; 2 3]), bags([1:1, 2:2]))))\nProductNode \t# 2 obs, 24 bytes\n  ├── ArrayNode(1×2 Array with Int64 elements) \t# 2 obs, 64 bytes\n  └── BagNode \t# 2 obs, 96 bytes\n        └── ArrayNode(2×2 Array with Int64 elements) \t# 2 obs, 80 bytes\n\njulia> n = ProductNode((n1, n2))\nProductNode \t# 2 obs, 56 bytes\n  ├── ProductNode \t# 2 obs, 32 bytes\n  │     └── a: ArrayNode(2053×2 NGramMatrix with Union{Missing, Int64} elements) \t# 2 obs, 129 bytes\n  └── ProductNode \t# 2 obs, 24 bytes\n        ├── ArrayNode(1×2 Array with Int64 elements) \t# 2 obs, 64 bytes\n        └── BagNode \t# 2 obs, 96 bytes\n              ⋮\n\njulia> printtree(n; trav=true)\nProductNode [\"\"] \t# 2 obs, 56 bytes\n  ├── ProductNode [\"E\"] \t# 2 obs, 32 bytes\n  │     └── a: ArrayNode(2053×2 NGramMatrix with Union{Missing, Int64} elements) [\"M\"] \t# 2 obs, 129 bytes\n  └── ProductNode [\"U\"] \t# 2 obs, 24 bytes\n        ├── ArrayNode(1×2 Array with Int64 elements) [\"Y\"] \t# 2 obs, 64 bytes\n        └── BagNode [\"c\"] \t# 2 obs, 96 bytes\n              └── ArrayNode(2×2 Array with Int64 elements) [\"e\"] \t# 2 obs, 80 bytes\n\njulia> reflectinmodel(n) |> printtree\nProductModel ↦ ArrayModel(Dense(20, 10)) \t# 2 arrays, 210 params, 920 bytes\n  ├── ProductModel ↦ ArrayModel(identity)\n  │     └── a: ArrayModel([postimputing]Dense(2053, 10)) \t# 3 arrays, 20_550 params, 80.391 KiB\n  └── ProductModel ↦ ArrayModel(Dense(11, 10)) \t# 2 arrays, 120 params, 560 bytes\n        ├── ArrayModel(identity)\n        └── BagModel ↦ BagCount([SegmentedMean(10); SegmentedMax(10)]) ↦ ArrayModel(Dense(21, 10)) \t# 4 arrays, 240 params, 1.094 KiB\n              └── ArrayModel(Dense(2, 10)) \t# 2 arrays, 30 params, 200 bytes\n\njulia> reflectinmodel(n, d -> Dense(d, 3), SegmentedMean, all_imputing=true) |> printtree\nProductModel ↦ ArrayModel(Dense(6, 3)) \t# 2 arrays, 21 params, 164 bytes\n  ├── ProductModel ↦ ArrayModel(identity)\n  │     └── a: ArrayModel([postimputing]Dense(2053, 3)) \t# 3 arrays, 6_165 params, 24.199 KiB\n  └── ProductModel ↦ ArrayModel(Dense(4, 3)) \t# 2 arrays, 15 params, 140 bytes\n        ├── ArrayModel([preimputing]Dense(1, 1)) \t# 3 arrays, 3 params, 132 bytes\n        └── BagModel ↦ SegmentedMean(3) ↦ ArrayModel(Dense(3, 3)) \t# 3 arrays, 15 params, 180 bytes\n              └── ArrayModel([preimputing]Dense(2, 3)) \t# 3 arrays, 11 params, 164 bytes\n\njulia> reflectinmodel(n, d -> Dense(d, 3), SegmentedMean;\n                        fsm=Dict(\"e\" => d -> Chain(Dense(d, 2), Dense(2, 2))),\n                        fsa=Dict(\"c\" => SegmentedLSE),\n                        single_key_identity=false,\n                        single_scalar_identity=false) |> printtree\nProductModel ↦ ArrayModel(Dense(6, 3)) \t# 2 arrays, 21 params, 164 bytes\n  ├── ProductModel ↦ ArrayModel(Dense(3, 3)) \t# 2 arrays, 12 params, 128 bytes\n  │     └── a: ArrayModel([postimputing]Dense(2053, 3)) \t# 3 arrays, 6_165 params, 24.199 KiB\n  └── ProductModel ↦ ArrayModel(Dense(6, 3)) \t# 2 arrays, 21 params, 164 bytes\n        ├── ArrayModel(Dense(1, 3)) \t# 2 arrays, 6 params, 104 bytes\n        └── BagModel ↦ SegmentedLSE(2) ↦ ArrayModel(Dense(2, 3)) \t# 4 arrays, 13 params, 212 bytes\n              └── ArrayModel(Chain(Dense(2, 2), Dense(2, 2))) \t# 4 arrays, 12 params, 208 bytes\n\nSee also: AbstractMillNode, AbstractMillModel, ProductNode, BagNode, ArrayNode.\n\n\n\n\n\n","category":"function"},{"location":"api/model_nodes/#Mill.modelsummary","page":"Model nodes","title":"Mill.modelsummary","text":"modelsummary(m::AbstractMillModel)\n\nPrint summary of parameters of model m.\n\nExamples\n\njulia> m = ProductModel(ArrayModel(Dense(2, 3)))\nProductModel ↦ ArrayModel(identity)\n  └── ArrayModel(Dense(2, 3)) \t# 2 arrays, 9 params, 116 bytes\n\njulia> modelsummary(m)\n\"Model summary: 2 arrays, 9 params, 116 bytes\"\n\nSee also: datasummary.\n\n\n\n\n\n","category":"function"},{"location":"examples/musk/musk/","page":"Musk","title":"Musk","text":"using Pkg\nold_path = Pkg.project().path\nPkg.activate(pwd())\nPkg.instantiate()","category":"page"},{"location":"examples/musk/musk/#Musk","page":"Musk","title":"Musk","text":"","category":"section"},{"location":"examples/musk/musk/","page":"Musk","title":"Musk","text":"Musk dataset is a classic MIL problem of the field, introduced in Thomas G. Dietterich, Richard H. Lathrop, Tomás Lozano-Pérez (1997). Below we demonstrate how to solve this problem using Mill.jl. The full example is also accessible here, as well as a Julia environment to run it.","category":"page"},{"location":"examples/musk/musk/","page":"Musk","title":"Musk","text":"For the demo, we load all dependencies and fix the seed:","category":"page"},{"location":"examples/musk/musk/","page":"Musk","title":"Musk","text":"using FileIO, JLD2, Statistics, Mill, Flux\nusing Flux: throttle, @epochs\nusing Mill: reflectinmodel\nusing Base.Iterators: repeated\n\nusing Random; Random.seed!(42)","category":"page"},{"location":"examples/musk/musk/","page":"Musk","title":"Musk","text":"and then load the dataset and transform it into a Mill structure. The musk.jld2 file contains:","category":"page"},{"location":"examples/musk/musk/","page":"Musk","title":"Musk","text":"a matrix with features fMat:","category":"page"},{"location":"examples/musk/musk/","page":"Musk","title":"Musk","text":"Pkg.status()","category":"page"},{"location":"examples/musk/musk/","page":"Musk","title":"Musk","text":"fMat = load(\"musk.jld2\", \"fMat\")          # matrix with instances, each column is one sample","category":"page"},{"location":"examples/musk/musk/","page":"Musk","title":"Musk","text":"the id of sample (bag in MIL terminology) specifying to which each instance (column in fMat) belongs to:","category":"page"},{"location":"examples/musk/musk/","page":"Musk","title":"Musk","text":"bagids = load(\"musk.jld2\", \"bagids\")      # ties instances to bags","category":"page"},{"location":"examples/musk/musk/","page":"Musk","title":"Musk","text":"The resulting BagNode is a structure which holds (i) feature matrix and (ii) ranges identifying which columns in the feature matrix each bag spans. This representation ensures that feed-forward networks do not need to deal with bag boundaries and always process full continuous matrices:","category":"page"},{"location":"examples/musk/musk/","page":"Musk","title":"Musk","text":"ds = BagNode(ArrayNode(fMat), bagids)     # create a BagNode dataset","category":"page"},{"location":"examples/musk/musk/","page":"Musk","title":"Musk","text":"the label of each instance in y.  The label of a bag is a maximum of labels of its instances, i.e. one positive instance in a bag makes it positive:","category":"page"},{"location":"examples/musk/musk/","page":"Musk","title":"Musk","text":"y = load(\"musk.jld2\", \"y\")                # load labels\ny = map(i -> maximum(y[i]) + 1, ds.bags)  # create labels on bags\ny_oh = Flux.onehotbatch(y, 1:2)           # one-hot encoding","category":"page"},{"location":"examples/musk/musk/","page":"Musk","title":"Musk","text":"Once the data are in Mill internal format, we will manually create a model. BagModel is designed to implement a basic multi-instance learning model utilizing two feed-forward networks with an aggregaton operator in between:","category":"page"},{"location":"examples/musk/musk/","page":"Musk","title":"Musk","text":"model = BagModel(\n    ArrayModel(Dense(166, 10, Flux.tanh)),                      # model on the level of Flows\n    BagCount(SegmentedMeanMax(10)),                             # aggregation\n    ArrayModel(Chain(Dense(21, 10, Flux.tanh), Dense(10, 2))))  # model on the level of bags","category":"page"},{"location":"examples/musk/musk/","page":"Musk","title":"Musk","text":"Instances are first passed through a single layer with 10 neurons (input dimension is 166) with tanh non-linearity, then we use mean and max aggregation functions simultaneously (for some problems, max is better then mean, therefore we use both), and then we use one layer with 10 neurons and tanh nonlinearity followed by linear layer with 2 neurons (output dimension).","category":"page"},{"location":"examples/musk/musk/","page":"Musk","title":"Musk","text":"Let's check that forward pass works:","category":"page"},{"location":"examples/musk/musk/","page":"Musk","title":"Musk","text":"model(ds)","category":"page"},{"location":"examples/musk/musk/","page":"Musk","title":"Musk","text":"Since Mill is entirely compatible with Flux.jl, we can use its cross-entropy loss function:","category":"page"},{"location":"examples/musk/musk/","page":"Musk","title":"Musk","text":"loss(ds, y_oh) = Flux.logitcrossentropy(model(ds) |> Mill.data, y_oh)","category":"page"},{"location":"examples/musk/musk/","page":"Musk","title":"Musk","text":"and run simple training procedure using its tooling:","category":"page"},{"location":"examples/musk/musk/","page":"Musk","title":"Musk","text":"opt = Flux.ADAM()\n@epochs 10 begin\n    Flux.train!(loss, params(model), repeated((ds, y_oh), 1000), opt)\n    println(loss(ds, y_oh))\nend","category":"page"},{"location":"examples/musk/musk/","page":"Musk","title":"Musk","text":"We can also calculate training error, which should be not so surprisingly low:","category":"page"},{"location":"examples/musk/musk/","page":"Musk","title":"Musk","text":"mean(mapslices(argmax, model(ds).data, dims=1)' .!= y)","category":"page"},{"location":"examples/musk/musk/","page":"Musk","title":"Musk","text":"Pkg.activate(old_path)","category":"page"},{"location":"api/bags/#Bags","page":"Bags","title":"Bags","text":"","category":"section"},{"location":"api/bags/#Index","page":"Bags","title":"Index","text":"","category":"section"},{"location":"api/bags/","page":"Bags","title":"Bags","text":"Pages = [\"bags.md\"]","category":"page"},{"location":"api/bags/#API","page":"Bags","title":"API","text":"","category":"section"},{"location":"api/bags/","page":"Bags","title":"Bags","text":"AbstractBags\n\nAlignedBags\nAlignedBags()\nAlignedBags(::UnitRange{<:Integer}...)\nAlignedBags(::Vector{<:Integer})\n\nScatteredBags\nScatteredBags()\nScatteredBags(::Vector{<:Integer})\n\nlength2bags\nbags\nremapbags\nadjustbags","category":"page"},{"location":"api/bags/#Mill.AbstractBags","page":"Bags","title":"Mill.AbstractBags","text":"AbstractBags{T}\n\nSupertype for structures storing indices of type T of bags' instances in BagNodes.\n\n\n\n\n\n","category":"type"},{"location":"api/bags/#Mill.AlignedBags","page":"Bags","title":"Mill.AlignedBags","text":"AlignedBags{T <: Integer} <: AbstractBags{T}\n\nAlignedBags struct stores indices of bags' instances in one or more UnitRange{T}s. This is only possible if instances of every bag are stored in one contiguous block.\n\nSee also: ScatteredBags.\n\n\n\n\n\n","category":"type"},{"location":"api/bags/#Mill.AlignedBags-Tuple{}","page":"Bags","title":"Mill.AlignedBags","text":"AlignedBags()\n\nConstruct a new AlignedBags struct containing no bags.\n\nExamples\n\njulia> AlignedBags()\nAlignedBags{Int64}(UnitRange{Int64}[])\n\n\n\n\n\n","category":"method"},{"location":"api/bags/#Mill.AlignedBags-Tuple{Vararg{UnitRange{<:Integer}}}","page":"Bags","title":"Mill.AlignedBags","text":"AlignedBags(bags::UnitRange{<:Integer}...)\n\nConstruct a new AlignedBags struct from bags in arguments.\n\nExamples\n\njulia> AlignedBags(1:3, 4:8)\nAlignedBags{Int64}(UnitRange{Int64}[1:3, 4:8])\n\n\n\n\n\n","category":"method"},{"location":"api/bags/#Mill.AlignedBags-Tuple{Vector{<:Integer}}","page":"Bags","title":"Mill.AlignedBags","text":"AlignedBags(k::Vector{<:Integer})\n\nConstruct a new AlignedBags struct from Vector k specifying the index of the bag each instance belongs to. Throws ArgumentError if this is not possible.\n\nExamples\n\njulia> AlignedBags([2, 2, 1, 1, 1, 3])\nAlignedBags{Int64}(UnitRange{Int64}[1:2, 3:5, 6:6])\n\n\n\n\n\n","category":"method"},{"location":"api/bags/#Mill.ScatteredBags","page":"Bags","title":"Mill.ScatteredBags","text":"ScatteredBags{T <: Integer} <: AbstractBags{T}\n\nScatteredBags struct stores indices of bags' instances that are not necessarily contiguous.\n\nSee also: AlignedBags.\n\n\n\n\n\n","category":"type"},{"location":"api/bags/#Mill.ScatteredBags-Tuple{}","page":"Bags","title":"Mill.ScatteredBags","text":"ScatteredBags()\n\nConstruct a new ScatteredBags struct containing no bags.\n\nExamples\n\njulia> ScatteredBags()\nScatteredBags{Int64}(Vector{Int64}[])\n\n\n\n\n\n","category":"method"},{"location":"api/bags/#Mill.ScatteredBags-Tuple{Vector{<:Integer}}","page":"Bags","title":"Mill.ScatteredBags","text":"ScatteredBags(k::Vector{<:Integer})\n\nConstruct a new ScatteredBags struct from Vector k specifying the index of the bag each instance belongs to.\n\nExamples\n\njulia> ScatteredBags([2, 2, 1, 1, 1, 3])\nScatteredBags{Int64}([[3, 4, 5], [1, 2], [6]])\n\n\n\n\n\n","category":"method"},{"location":"api/bags/#Mill.length2bags","page":"Bags","title":"Mill.length2bags","text":"length2bags(ls::Vector{<:Integer})\n\nConvert lengths of bags given in ls to AlignedBags with contiguous blocks.\n\nExamples\n\njulia> length2bags([1, 3, 2])\nAlignedBags{Int64}(UnitRange{Int64}[1:1, 2:4, 5:6])\n\nSee also: AlignedBags.\n\n\n\n\n\n","category":"function"},{"location":"api/bags/#Mill.bags","page":"Bags","title":"Mill.bags","text":"bags(k::Vector{<:Integer})\nbags(k::Vector{T}) where T <: UnitRange{<:Integer}\nbags(b::AbstractBags)\n\nConstruct an AbstractBags structure that is most suitable for the input (AlignedBags if possible, ScatteredBags otherwise).\n\nExamples\n\njulia> bags([2, 2, 3, 1])\nAlignedBags{Int64}(UnitRange{Int64}[1:2, 3:3, 4:4])\n\njulia> bags([2, 3, 1, 2])\nScatteredBags{Int64}([[3], [1, 4], [2]])\n\njulia> bags([1:3, 4:5])\nAlignedBags{Int64}(UnitRange{Int64}[1:3, 4:5])\n\njulia> bags(ScatteredBags())\nScatteredBags{Int64}(Vector{Int64}[])\n\nSee also: AlignedBags, ScatteredBags.\n\n\n\n\n\n","category":"function"},{"location":"api/bags/#Mill.remapbags","page":"Bags","title":"Mill.remapbags","text":"remapbags(b::AbstractBags, idcs::VecOrRange{<:Integer}) -> (rb, I)\n\nSelect a subset of bags in b corresponding to indices idcs and remap instance indices appropriately. Return new bags rb as well as a Vector of remapped instances I.\n\nExamples\n\njulia> remapbags(AlignedBags([1:1, 2:3, 4:5]), [1, 3])\n(AlignedBags{Int64}(UnitRange{Int64}[1:1, 2:3]), [1, 4, 5])\n\njulia> remapbags(ScatteredBags([[1,3], [2], Int[]]), [2])\n(ScatteredBags{Int64}([[1]]), [2])\n\n\n\n\n\n","category":"function"},{"location":"api/bags/#Mill.adjustbags","page":"Bags","title":"Mill.adjustbags","text":"adjustbags(b::AlignedBags, mask::AbstractVector{Bool})\n\nRemove indices of instances brom bags b and remap the remaining instances accordingly.\n\nExamples\n\njulia> adjustbags(AlignedBags([1:2, 0:-1, 3:4]), [false, false, true, true])\nAlignedBags{Int64}(UnitRange{Int64}[0:-1, 0:-1, 1:2])\n\n\n\n\n\n","category":"function"},{"location":"manual/leaf_data/","page":"Data in leaves","title":"Data in leaves","text":"using Mill, Flux","category":"page"},{"location":"manual/leaf_data/#Data-in-leaves","page":"Data in leaves","title":"Data in leaves","text":"","category":"section"},{"location":"manual/leaf_data/","page":"Data in leaves","title":"Data in leaves","text":"In Mill.jl tree-like data representations, there are always some raw data on the leaf level, whereas on higher levels instances are grouped into bags (BagNodes), and different sets are joined together with Cartesion products (ProductNodes) and thus more abstract concepts are created. In this section we look into several examples how the lowest-level data can be represented.","category":"page"},{"location":"manual/leaf_data/","page":"Data in leaves","title":"Data in leaves","text":"For this purpose, let's assume that we would like to identify infected clients in a network from their HTTP traffic. Since one client can make an arbitrary number of connections during the observation period, modelling the client as a bag of connections seems like the most natural approach:","category":"page"},{"location":"manual/leaf_data/","page":"Data in leaves","title":"Data in leaves","text":"connections = AlignedBags([1:2, 3:3, 4:6])","category":"page"},{"location":"manual/leaf_data/","page":"Data in leaves","title":"Data in leaves","text":"Thus, each of the six connections becomes an instance in one of the three bags representing clients. How to represent such connections instances? Each HTTP flow has properties that can be expressed as standard numerical features, categorical features or strings of characters.","category":"page"},{"location":"manual/leaf_data/#Numerical-features","page":"Data in leaves","title":"Numerical features","text":"","category":"section"},{"location":"manual/leaf_data/","page":"Data in leaves","title":"Data in leaves","text":"We have already shown how to represent standard numerical features in previous parts of this manual. It is as simple as wrapping a type that behaves like a matrix into an ArrayNode:","category":"page"},{"location":"manual/leaf_data/","page":"Data in leaves","title":"Data in leaves","text":"content_lengths = [4446, 1957, 4310,\n                   11604, 17019, 13947]\ndates = [1435420950, 1376190532, 1316869962,\n         1302775198, 1555598383, 1562237892]\nnumerical_node = ArrayNode([content_lengths'; dates'])","category":"page"},{"location":"manual/leaf_data/","page":"Data in leaves","title":"Data in leaves","text":"We use Content-Length  and Date request headers, the latter converted to Unix timestamp.","category":"page"},{"location":"manual/leaf_data/#Categorical-features","page":"Data in leaves","title":"Categorical features","text":"","category":"section"},{"location":"manual/leaf_data/","page":"Data in leaves","title":"Data in leaves","text":"For categorical variables, we proceed in the same way, but we use one-hot encoding implemented in Flux.jl. This way, we can encode for example a verb of the request:","category":"page"},{"location":"manual/leaf_data/","page":"Data in leaves","title":"Data in leaves","text":"ALL_VERBS = [\"GET\", \"HEAD\", \"POST\", \"PUT\", \"DELETE\"] # etc...\nverbs = [\"GET\", \"GET\", \"POST\", \"HEAD\", \"HEAD\", \"PUT\"]\nverb_node = ArrayNode(Flux.onehotbatch(verbs, ALL_VERBS))","category":"page"},{"location":"manual/leaf_data/","page":"Data in leaves","title":"Data in leaves","text":"or Content-Encoding header:","category":"page"},{"location":"manual/leaf_data/","page":"Data in leaves","title":"Data in leaves","text":"ALL_ENCODINGS = [\"bzip2\", \"gzip\", \"xz\", \"identity\"] # etc...\nencodings = [\"xz\", \"gzip\", \"bzip2\", \"xz\", \"identity\", \"bzip2\"]\nencoding_node = ArrayNode(Flux.onehotbatch(encodings, ALL_ENCODINGS))","category":"page"},{"location":"manual/leaf_data/","page":"Data in leaves","title":"Data in leaves","text":"Because Flux.OneHotMatrix supports multiplication it is possible to wrap it into an ArrayNode.","category":"page"},{"location":"manual/leaf_data/#Strings","page":"Data in leaves","title":"Strings","text":"","category":"section"},{"location":"manual/leaf_data/","page":"Data in leaves","title":"Data in leaves","text":"The last example we will consider are string features. This could for example be the Host header:","category":"page"},{"location":"manual/leaf_data/","page":"Data in leaves","title":"Data in leaves","text":"hosts = [\n    \"www.foo.com\", \"www.foo.com\", \"www.baz.com\",\n    \"www.foo.com\", \"www.baz.com\", \"www.foo.com\"\n]","category":"page"},{"location":"manual/leaf_data/","page":"Data in leaves","title":"Data in leaves","text":"Mill offers ngram histogram-based representation for strings. To get started, we pass the vector of strings into the constructor of NGramMatrix:","category":"page"},{"location":"manual/leaf_data/","page":"Data in leaves","title":"Data in leaves","text":"hosts_ngrams = NGramMatrix(hosts, 3, 256, 7)","category":"page"},{"location":"manual/leaf_data/","page":"Data in leaves","title":"Data in leaves","text":"Each string gets processed into ngrams (trigram in this case as specified in the first parameter). Then, each character is transformed into an integer via the codeunits function and the whole trigram is interpreted as a three digit number using a base b specified in the second parameter. Here, we use a base of 256, which is the most reasonable choice for ascii URLs. For example, for foo trigram, we obtain:","category":"page"},{"location":"manual/leaf_data/","page":"Data in leaves","title":"Data in leaves","text":"c = codeunits(\"foo\")\nc[1] * 256^2 + c[2] * 256 + c[3]","category":"page"},{"location":"manual/leaf_data/","page":"Data in leaves","title":"Data in leaves","text":"The last step is taking the modulo of this result with respect to some prime modulo m, in this case 7 (last parameter in the constructor), leaving us with 3 as a result. Therefore, for this trigram foo, we would add 1 to the third row[1]. We can convert this NGramMatrix into a sparse array and then to the standard array:","category":"page"},{"location":"manual/leaf_data/","page":"Data in leaves","title":"Data in leaves","text":"[1]: One appropriate value for modulo m for real problems is 2053","category":"page"},{"location":"manual/leaf_data/","page":"Data in leaves","title":"Data in leaves","text":"using SparseArrays","category":"page"},{"location":"manual/leaf_data/","page":"Data in leaves","title":"Data in leaves","text":"hosts_dense = hosts_ngrams |> SparseMatrixCSC |> Matrix","category":"page"},{"location":"manual/leaf_data/","page":"Data in leaves","title":"Data in leaves","text":"Again, we get one column for each string, and the matrix has the same number of rows as modulo m. For each string s, we get length(s) + n - 1 ngrams:","category":"page"},{"location":"manual/leaf_data/","page":"Data in leaves","title":"Data in leaves","text":"sum(hosts_dense; dims=1)","category":"page"},{"location":"manual/leaf_data/","page":"Data in leaves","title":"Data in leaves","text":"This is because we use special abstract characters (or tokens) for the start and the end of the string. If we denote these ^ and $, respectively, from string \"foo\", we get trigrams ^^f, ^fo, foo, oo$, o$$. Note that these special characters are purely abstract whereas ^ and $ used only for illustration purposes here are characters like any other. Both string start and string end special characters have a unique mapping to integers, which can be obtained as well as set:","category":"page"},{"location":"manual/leaf_data/","page":"Data in leaves","title":"Data in leaves","text":"Mill.string_start_code()\nMill.string_end_code()\nMill.string_start_code!(42)\nMill.string_start_code()","category":"page"},{"location":"manual/leaf_data/","page":"Data in leaves","title":"Data in leaves","text":"NGramMatrix behaves like a matrix, implements an efficient left-multiplication and thus can be used in ArrayNode:","category":"page"},{"location":"manual/leaf_data/","page":"Data in leaves","title":"Data in leaves","text":"hosts_ngrams::AbstractMatrix{Int64}\nhost_node = ArrayNode(hosts_ngrams)","category":"page"},{"location":"manual/leaf_data/","page":"Data in leaves","title":"Data in leaves","text":"Custom nodes section shows one more slightly more complex way of processing strings, specifically Unix paths.","category":"page"},{"location":"manual/leaf_data/#Putting-it-all-together","page":"Data in leaves","title":"Putting it all together","text":"","category":"section"},{"location":"manual/leaf_data/","page":"Data in leaves","title":"Data in leaves","text":"Now, we can finally put wrap all features of all six connections into one ProductNode and construct a BagNode representing a bag of all connections (corresponding to three different clients):","category":"page"},{"location":"manual/leaf_data/","page":"Data in leaves","title":"Data in leaves","text":"ds = BagNode(ProductNode(\n    numerical=numerical_node,\n    verb=verb_node,\n    encoding=encoding_node,\n    hosts=host_node\n), connections)","category":"page"},{"location":"manual/leaf_data/","page":"Data in leaves","title":"Data in leaves","text":"create a model for training and run one forward pass:","category":"page"},{"location":"manual/leaf_data/","page":"Data in leaves","title":"Data in leaves","text":"m = reflectinmodel(ds)\nm(ds)","category":"page"},{"location":"manual/leaf_data/","page":"Data in leaves","title":"Data in leaves","text":"We now obtain a matrix with three columns, each corresponding to one of the clients. Now we can for example calculate gradients with respect to the model parameters:","category":"page"},{"location":"manual/leaf_data/","page":"Data in leaves","title":"Data in leaves","text":"gradient(() -> sum(Mill.data(m(ds))), Flux.params(m))","category":"page"},{"location":"manual/leaf_data/","page":"Data in leaves","title":"Data in leaves","text":"ukn: Numerical features\nTo put all numerical features into one ArrayNode is a design choice. We could as well introduce more keys in the final ProductNode. The model treats these two cases slightly differently (see Nodes section).","category":"page"},{"location":"manual/leaf_data/","page":"Data in leaves","title":"Data in leaves","text":"This dummy example illustrates the versatility of Mill. With little to no preprocessing we are able to process complex hierarchical structures and avoid manually designing feature extraction procedures. For a more involved study on processing Internet traffic with Mill, see for example Tomáš Pevný, Marek Dědič (2020).","category":"page"},{"location":"manual/nodes/","page":"Nodes","title":"Nodes","text":"using Mill","category":"page"},{"location":"manual/nodes/","page":"Nodes","title":"Nodes","text":"ukw: Tip\nIt is recommended to read the Motivation section first to understand the crucial ideas behind hierarchical multiple instance learning.","category":"page"},{"location":"manual/nodes/#Nodes","page":"Nodes","title":"Nodes","text":"","category":"section"},{"location":"manual/nodes/","page":"Nodes","title":"Nodes","text":"Mill.jl enables representation of arbitrarily complex tree-like hierarchies and appropriate models for these hierarchies. It defines two core abstract types:","category":"page"},{"location":"manual/nodes/","page":"Nodes","title":"Nodes","text":"AbstractMillNode which stores data on any level of abstraction and its subtypes can be further nested\nAbstractMillModel which helps to define a corresponding model. For each specific implementation of AbstractMillNode we have one or more specific AbstractMillModels for processing it.","category":"page"},{"location":"manual/nodes/","page":"Nodes","title":"Nodes","text":"Below we will introduce ArrayNode, BagNode and ProductNode together with their corresponding models. It is possible to define data and model nodes for more complex behaviors (see Custom nodes), however, these three core types are already sufficient for most tasks. For instance, we can represent any JSON document and use appropriate models to convert it to a vector represention or classify it (see Processing JSONs).","category":"page"},{"location":"manual/nodes/#[ArrayNode](@ref)-and-[ArrayModel](@ref)","page":"Nodes","title":"ArrayNode and ArrayModel","text":"","category":"section"},{"location":"manual/nodes/","page":"Nodes","title":"Nodes","text":"ArrayNode thinly wraps an array of features (specifically any subtype of AbstractArray):","category":"page"},{"location":"manual/nodes/","page":"Nodes","title":"Nodes","text":"X = Float32.([1 2 3 ; 4 5 6])\nAN = ArrayNode(X)","category":"page"},{"location":"manual/nodes/","page":"Nodes","title":"Nodes","text":"Data carried by any AbstractMillNode can be accessed with the Mill.data function as follows:","category":"page"},{"location":"manual/nodes/","page":"Nodes","title":"Nodes","text":"Mill.data(AN)","category":"page"},{"location":"manual/nodes/","page":"Nodes","title":"Nodes","text":"Similarly, ArrayModel wraps any function performing operation over this array. In example below, we wrap a feature matrix X and a Dense model from Flux.jl:","category":"page"},{"location":"manual/nodes/","page":"Nodes","title":"Nodes","text":"using Flux: Dense","category":"page"},{"location":"manual/nodes/","page":"Nodes","title":"Nodes","text":"f = Dense(2, 3)\nAM = ArrayModel(f)","category":"page"},{"location":"manual/nodes/","page":"Nodes","title":"Nodes","text":"We can apply the model now with AM(AN) to get another ArrayNode and verify that the feedforward layer f is really applied:","category":"page"},{"location":"manual/nodes/","page":"Nodes","title":"Nodes","text":"AM(AN)\nf(X) == AM(AN) |> Mill.data","category":"page"},{"location":"manual/nodes/","page":"Nodes","title":"Nodes","text":"ukn: Model outputs\nA convenient property of all Mill models is that after applying them to a corresponding data node we always obtain an ArrayNode as output regardless of the type and complexity of the model. This becomes important later.","category":"page"},{"location":"manual/nodes/","page":"Nodes","title":"Nodes","text":"The most common interpretation of the data inside ArrayNodes is that each column contains features of one sample and therefore the node AN carries size(Mill.data(AN), 2) samples. In this sense, ArrayNodes wrap the standard machine learning problem, where each sample is represented with a vector, a matrix or a more general tensor of features. Alternatively, one can obtain a number of samples of any AbstractMillNode with nobs function from StatsBase.jl package:","category":"page"},{"location":"manual/nodes/","page":"Nodes","title":"Nodes","text":"using StatsBase: nobs","category":"page"},{"location":"manual/nodes/","page":"Nodes","title":"Nodes","text":"nobs(AN)","category":"page"},{"location":"manual/nodes/#[BagNode](@ref)","page":"Nodes","title":"BagNode","text":"","category":"section"},{"location":"manual/nodes/","page":"Nodes","title":"Nodes","text":"BagNode is represents the standard multiple instance learning problem, that is, each sample is a bag containing an arbitrary number of instances. In the simplest case, each instance is a vector:","category":"page"},{"location":"manual/nodes/","page":"Nodes","title":"Nodes","text":"BN = BagNode(AN, [1:3, 4:4])","category":"page"},{"location":"manual/nodes/","page":"Nodes","title":"Nodes","text":"where for simplicity we used AN from the previous example. Each BagNode carries data and bags fields:","category":"page"},{"location":"manual/nodes/","page":"Nodes","title":"Nodes","text":"Mill.data(BN)\nBN.bags","category":"page"},{"location":"manual/nodes/","page":"Nodes","title":"Nodes","text":"Here, data can be an arbitrary AbstractMillNode storing representation of instances (ArrayNode in this case) and bags field contains information, which instances belong to which bag. In this specific case bn stores three bags (samples). The first one consists of a single instance {[1.0, 4.0]} (first column of AN) and the second one of two instances {[2.0, 5.0], [3.0, 6.0]}. We can see that we deal with two top-level samples (bags):","category":"page"},{"location":"manual/nodes/","page":"Nodes","title":"Nodes","text":"nobs(BN)","category":"page"},{"location":"manual/nodes/","page":"Nodes","title":"Nodes","text":"whereas they are formed using three instances:","category":"page"},{"location":"manual/nodes/","page":"Nodes","title":"Nodes","text":"nobs(AN)","category":"page"},{"location":"manual/nodes/","page":"Nodes","title":"Nodes","text":"In Mill, there are two ways to store indices of the bag's instances:","category":"page"},{"location":"manual/nodes/","page":"Nodes","title":"Nodes","text":"in AlignedBags structure, which accepts a Vector of UnitRanges and requires all bag's instances stored continuously:","category":"page"},{"location":"manual/nodes/","page":"Nodes","title":"Nodes","text":"AlignedBags([1:2, 3:3])","category":"page"},{"location":"manual/nodes/","page":"Nodes","title":"Nodes","text":"and in ScatteredBags structure, which accepts a Vector of Vectorss storing not necessarily contiguous indices:","category":"page"},{"location":"manual/nodes/","page":"Nodes","title":"Nodes","text":"ScatteredBags([[2, 1], [3]])","category":"page"},{"location":"manual/nodes/","page":"Nodes","title":"Nodes","text":"The two examples above are semantically equivalent, as bags are unordered collections of instances. An empty bag with no instances is in AlignedBags specified as empty range 0:-1 and in ScatteredBags as an empty vector Int[]. The constructor of BagNode accepts directly one of these two structures and tries to automagically decide the better type in other cases.","category":"page"},{"location":"manual/nodes/#[BagModel](@ref)","page":"Nodes","title":"BagModel","text":"","category":"section"},{"location":"manual/nodes/","page":"Nodes","title":"Nodes","text":"Each BagNode is processed by a BagModel, which contains two (sub)models and an aggregation operator:","category":"page"},{"location":"manual/nodes/","page":"Nodes","title":"Nodes","text":"im = ArrayModel(Dense(2, 3))\na = SegmentedMax(3)\nbm = ArrayModel(Dense(3, 4))\nBM = BagModel(im, a, bm)","category":"page"},{"location":"manual/nodes/","page":"Nodes","title":"Nodes","text":"The first network submodel (called instance model im) is responsible for converting the instance representation to a vector form:","category":"page"},{"location":"manual/nodes/","page":"Nodes","title":"Nodes","text":"y = im(AN)","category":"page"},{"location":"manual/nodes/","page":"Nodes","title":"Nodes","text":"Note that because of the property mentioned above, the output of instance model im will always be an ArrayNode wrapping a matrix. We get four columns, one for each instance. This result is then used in SegmentedMax operator a which takes vector representation of all instances and produces a single vector per bag:","category":"page"},{"location":"manual/nodes/","page":"Nodes","title":"Nodes","text":"y = a(y, BN.bags)","category":"page"},{"location":"manual/nodes/","page":"Nodes","title":"Nodes","text":"unk: More about aggregation\nTo read more about aggregation operators, see the Bag aggregation section.","category":"page"},{"location":"manual/nodes/","page":"Nodes","title":"Nodes","text":"Finally, y is then passed to a feed forward model (called bag model bm) producing the final output per bag. In our example we therefore get a matrix with three columns:","category":"page"},{"location":"manual/nodes/","page":"Nodes","title":"Nodes","text":"y = bm(y)","category":"page"},{"location":"manual/nodes/","page":"Nodes","title":"Nodes","text":"However, the best way to use a bag model node is to simply apply it, which results into the same output:","category":"page"},{"location":"manual/nodes/","page":"Nodes","title":"Nodes","text":"BM(BN) == y","category":"page"},{"location":"manual/nodes/","page":"Nodes","title":"Nodes","text":"The whole procedure is depicted in the following picture:","category":"page"},{"location":"manual/nodes/","page":"Nodes","title":"Nodes","text":"(Image: )","category":"page"},{"location":"manual/nodes/","page":"Nodes","title":"Nodes","text":"Three instances of the BagNode are represented by red subtrees are first mapped with instance model im, aggregated (aggregation operator here is a concatenation of two different operators a_1 and a_2), and the results of aggregation are transformed with bag model bm.","category":"page"},{"location":"manual/nodes/","page":"Nodes","title":"Nodes","text":"ukn: Musk example\nAnother handy feature of Mill models is that they are completely differentiable and therefore fit in the Flux.jl framework. Nodes for processing arrays and bags are sufficient to solve the classical Musk problem.","category":"page"},{"location":"manual/nodes/#[ProductNode](@ref)s-and-[ProductModel](@ref)s","page":"Nodes","title":"ProductNodes and ProductModels","text":"","category":"section"},{"location":"manual/nodes/","page":"Nodes","title":"Nodes","text":"ProductNode can be thought of as a Cartesian Product or a Dictionary. It holds a Tuple or NamedTuple of nodes (not necessarily of the same type). For example, a ProductNode with the BagNode and the ArrayNode from above as children would look like this:","category":"page"},{"location":"manual/nodes/","page":"Nodes","title":"Nodes","text":"PN = ProductNode(a=AN, b=BN)","category":"page"},{"location":"manual/nodes/","page":"Nodes","title":"Nodes","text":"Analogically, the ProductModel contains a (Named)Tuple of (sub)models processing each of its children (stored in ms field standing for models), as well as one more (sub)model m:","category":"page"},{"location":"manual/nodes/","page":"Nodes","title":"Nodes","text":"ms = (a=AM, b=BM)\nm = ArrayModel(Dense(7, 2))\nPM = ProductModel(ms, m)","category":"page"},{"location":"manual/nodes/","page":"Nodes","title":"Nodes","text":"Again, since the library is based on the property that the output of each model is an ArrayNode, the product model applies models from ms to appropriate children and vertically concatenates the output, which is then processed by model m. An example of model processing the above sample would be:","category":"page"},{"location":"manual/nodes/","page":"Nodes","title":"Nodes","text":"y = PM.m(vcat(PM[:a](PN[:a]), PM[:b](PN[:b])))","category":"page"},{"location":"manual/nodes/","page":"Nodes","title":"Nodes","text":"which is equivalent to:","category":"page"},{"location":"manual/nodes/","page":"Nodes","title":"Nodes","text":"PM(PN) == y","category":"page"},{"location":"manual/nodes/","page":"Nodes","title":"Nodes","text":"Application of another product model (this time with four subtrees (keys)) can be visualized as follows:","category":"page"},{"location":"manual/nodes/","page":"Nodes","title":"Nodes","text":"(Image: )","category":"page"},{"location":"manual/nodes/","page":"Nodes","title":"Nodes","text":"unk: Indexing in product nodes\nIn general, we recommend to use NamedTuples, because the key can be used for indexing both ProductNodes and ProductModels.","category":"page"},{"location":"citation/#Citation","page":"Citation","title":"Citation","text":"","category":"section"},{"location":"citation/","page":"Citation","title":"Citation","text":"For citing, please use the following entry for the original paper:","category":"page"},{"location":"citation/","page":"Citation","title":"Citation","text":"@misc{mandlik2021milljl,\n      title={Mill.jl and JsonGrinder.jl: automated differentiable feature extraction for learning from raw JSON data}, \n      author={Simon Mandlik and Matej Racinsky and Viliam Lisy and Tomas Pevny},\n      year={2021},\n      eprint={2105.09107},\n      archivePrefix={arXiv},\n      primaryClass={stat.ML}\n}","category":"page"},{"location":"citation/","page":"Citation","title":"Citation","text":"and the following for the implementation (fill in the used version):","category":"page"},{"location":"citation/","page":"Citation","title":"Citation","text":"@software{mill2018,\n  author = {Tomas Pevny and Simon Mandlik},\n  title = {Mill.jl framework: a flexible library for (hierarchical) multi-instance learning},\n  url = {https://github.com/CTUAvastLab/Mill.jl},\n  version = {...},\n}","category":"page"},{"location":"manual/reflectin/","page":"Model reflection","title":"Model reflection","text":"using Mill, Flux","category":"page"},{"location":"manual/reflectin/#Model-reflection","page":"Model reflection","title":"Model reflection","text":"","category":"section"},{"location":"manual/reflectin/","page":"Model reflection","title":"Model reflection","text":"Since constructions of large models can be a tedious and error-prone process, Mill.jl provides reflectinmodel function that helps to automate it. The simplest definition accepts only one argument, a sample ds, and returns a compatible model:","category":"page"},{"location":"manual/reflectin/","page":"Model reflection","title":"Model reflection","text":"ds = BagNode(ProductNode((BagNode(ArrayNode(randn(4, 10)),\n                                  [1:2, 3:4, 5:5, 6:7, 8:10]),\n                          ArrayNode(randn(3, 5)),\n                          BagNode(BagNode(ArrayNode(randn(2, 30)),\n                                          [i:i+1 for i in 1:2:30]),\n                                  [1:3, 4:6, 7:9, 10:12, 13:15]),\n                          ArrayNode(randn(2, 5)))),\n             [1:1, 2:3, 4:5]);\nprinttree(ds)\n\nm = reflectinmodel(ds, d -> Dense(d, 2));\nprinttree(m)\n\nm(ds)","category":"page"},{"location":"manual/reflectin/","page":"Model reflection","title":"Model reflection","text":"The sample ds serves here as a specimen needed to specify a structure of the problem and calculate dimensions.","category":"page"},{"location":"manual/reflectin/#Optional-arguments","page":"Model reflection","title":"Optional arguments","text":"","category":"section"},{"location":"manual/reflectin/","page":"Model reflection","title":"Model reflection","text":"To have better control over the topology, reflectinmodel accepts up to two more optional arguments and four keyword arguments:","category":"page"},{"location":"manual/reflectin/","page":"Model reflection","title":"Model reflection","text":"The first optional argument expects a function that returns a layer (or a set of layers) given input dimension d (defaults to d -> Flux.Dense(d, 10)).\nThe second optional argument is a function returning aggregation function for BagModel nodes (defaults to BagCount ∘ SegmentedMeanMax).","category":"page"},{"location":"manual/reflectin/","page":"Model reflection","title":"Model reflection","text":"Compare the following example to the previous one:","category":"page"},{"location":"manual/reflectin/","page":"Model reflection","title":"Model reflection","text":"m = reflectinmodel(ds, d -> Dense(d, 5, relu), SegmentedMax);\nprinttree(m)\n\nm(ds)","category":"page"},{"location":"manual/reflectin/#Keyword-arguments","page":"Model reflection","title":"Keyword arguments","text":"","category":"section"},{"location":"manual/reflectin/","page":"Model reflection","title":"Model reflection","text":"The reflectinmodel allows even further customization. To index into the sample (or model), we can use printtree(ds; trav=true) from HierarchicalUtils.jl that prints the sample together with identifiers of individual nodes:","category":"page"},{"location":"manual/reflectin/","page":"Model reflection","title":"Model reflection","text":"using HierarchicalUtils","category":"page"},{"location":"manual/reflectin/","page":"Model reflection","title":"Model reflection","text":"printtree(ds; trav=true)","category":"page"},{"location":"manual/reflectin/","page":"Model reflection","title":"Model reflection","text":"These identifiers can be used to override the default construction functions. Note that the output, i.e. the last feed-forward network of the whole model is always tagged with an empty string \"\", which simplifies putting linear layer with an appropriate output dimension on the end. Dictionaries with these overrides can be passed in as keyword arguments:","category":"page"},{"location":"manual/reflectin/","page":"Model reflection","title":"Model reflection","text":"fsm overrides constructions of feed-forward models\nfsa overrides construction of aggregation functions.","category":"page"},{"location":"manual/reflectin/","page":"Model reflection","title":"Model reflection","text":"For example to specify just the last feed forward neural network:","category":"page"},{"location":"manual/reflectin/","page":"Model reflection","title":"Model reflection","text":"reflectinmodel(ds, d -> Dense(d, 5, relu), SegmentedMeanMax;\n    fsm = Dict(\"\" => d -> Chain(Dense(d, 20, relu), Dense(20, 12)))) |> printtree","category":"page"},{"location":"manual/reflectin/","page":"Model reflection","title":"Model reflection","text":"Both keyword arguments in action:","category":"page"},{"location":"manual/reflectin/","page":"Model reflection","title":"Model reflection","text":"reflectinmodel(ds, d -> Dense(d, 5, relu), SegmentedMeanMax;\n    fsm = Dict(\"\" => d -> Chain(Dense(d, 20, relu), Dense(20, 12))),\n    fsa = Dict(\"Y\" => SegmentedMean, \"g\" => SegmentedPNorm)) |> printtree","category":"page"},{"location":"manual/reflectin/","page":"Model reflection","title":"Model reflection","text":"There are even more ways to modify the reflection behavior, see the reflectinmodel api reference.","category":"page"},{"location":"manual/aggregation/","page":"Bag aggregation","title":"Bag aggregation","text":"using Mill, Flux","category":"page"},{"location":"manual/aggregation/#Bag-aggregation","page":"Bag aggregation","title":"Bag aggregation","text":"","category":"section"},{"location":"manual/aggregation/","page":"Bag aggregation","title":"Bag aggregation","text":"Aggregation operators in Mill.jl are all subtypes of AbstractAggregation. These structures are responsible for mapping of vector representations of multiple instances into a single vector. They all operate element-wise and independently of dimension and thus the output has the same size as representations on the input, unless the Concatenation of multiple operators is used.","category":"page"},{"location":"manual/aggregation/","page":"Bag aggregation","title":"Bag aggregation","text":"Some setup:","category":"page"},{"location":"manual/aggregation/","page":"Bag aggregation","title":"Bag aggregation","text":"d = 2\nX = Float32.([1 2 3 4; 8 7 6 5])\nn = ArrayNode(X)\nbags = AlignedBags([1:1, 2:3, 4:4])","category":"page"},{"location":"manual/aggregation/","page":"Bag aggregation","title":"Bag aggregation","text":"Different choice of operator, or their combinations, are suitable for different problems. Nevertheless, because the input is interpreted as an unordered bag of instances, every operator is invariant to permutation and also does not scale when increasing size of the bag.","category":"page"},{"location":"manual/aggregation/#Non-parametric-aggregation","page":"Bag aggregation","title":"Non-parametric aggregation","text":"","category":"section"},{"location":"manual/aggregation/#Max-aggregation","page":"Bag aggregation","title":"Max aggregation","text":"","category":"section"},{"location":"manual/aggregation/","page":"Bag aggregation","title":"Bag aggregation","text":"SegmentedMax implements a simple max and is the most straightforward operator defined in one dimension as follows:","category":"page"},{"location":"manual/aggregation/","page":"Bag aggregation","title":"Bag aggregation","text":"a_max(x_1 ldots x_k) = max_i = 1 ldots k x_i","category":"page"},{"location":"manual/aggregation/","page":"Bag aggregation","title":"Bag aggregation","text":"where x_1 ldots x_k are all instances of the given bag. In Mill, the operator is constructed this way:","category":"page"},{"location":"manual/aggregation/","page":"Bag aggregation","title":"Bag aggregation","text":"a_max = SegmentedMax(d)","category":"page"},{"location":"manual/aggregation/","page":"Bag aggregation","title":"Bag aggregation","text":"The application is straightforward and can be performed on both raw AbstractArrays or ArrayNodes:","category":"page"},{"location":"manual/aggregation/","page":"Bag aggregation","title":"Bag aggregation","text":"a_max(X, bags)\na_max(n, bags)","category":"page"},{"location":"manual/aggregation/","page":"Bag aggregation","title":"Bag aggregation","text":"Since we have three bags, we have three columns in the output, each storing the maximal element over all instances of the given bag.","category":"page"},{"location":"manual/aggregation/#Mean-aggregation","page":"Bag aggregation","title":"Mean aggregation","text":"","category":"section"},{"location":"manual/aggregation/","page":"Bag aggregation","title":"Bag aggregation","text":"SegmentedMean implements mean function, defined as:","category":"page"},{"location":"manual/aggregation/","page":"Bag aggregation","title":"Bag aggregation","text":"a_operatornamemean(x_1 ldots x_k) = frac1k sum_i = 1^k x_i","category":"page"},{"location":"manual/aggregation/","page":"Bag aggregation","title":"Bag aggregation","text":"and used the same way:","category":"page"},{"location":"manual/aggregation/","page":"Bag aggregation","title":"Bag aggregation","text":"a_mean = SegmentedMean(d)\na_mean(X, bags)\na_mean(n, bags)","category":"page"},{"location":"manual/aggregation/","page":"Bag aggregation","title":"Bag aggregation","text":"ukn: Sufficiency of the mean operator\nIn theory, mean aggregation is sufficient for approximation as proven in Tomáš Pevný, Vojtěch Kovařík (2019), but in practice, a combination of multiple operators performes better.","category":"page"},{"location":"manual/aggregation/","page":"Bag aggregation","title":"Bag aggregation","text":"The max aggregation is suitable for cases when one instance in the bag may give evidence strong enough to predict the label. On the other side of the spectrum lies the mean aggregation function, which detects well trends identifiable globally over the whole bag.","category":"page"},{"location":"manual/aggregation/#Sum-aggregation","page":"Bag aggregation","title":"Sum aggregation","text":"","category":"section"},{"location":"manual/aggregation/","page":"Bag aggregation","title":"Bag aggregation","text":"The last non-parametric operator is SegmentedSum, defined as:","category":"page"},{"location":"manual/aggregation/","page":"Bag aggregation","title":"Bag aggregation","text":"a_operatornamemean(x_1 ldots x_k) = sum_i = 1^k x_i","category":"page"},{"location":"manual/aggregation/","page":"Bag aggregation","title":"Bag aggregation","text":"and used the same way:","category":"page"},{"location":"manual/aggregation/","page":"Bag aggregation","title":"Bag aggregation","text":"a_sum = SegmentedSum(d)\na_sum(X, bags)\na_sum(n, bags)","category":"page"},{"location":"manual/aggregation/#Parametric-aggregation","page":"Bag aggregation","title":"Parametric aggregation","text":"","category":"section"},{"location":"manual/aggregation/","page":"Bag aggregation","title":"Bag aggregation","text":"Whereas non-parametric aggregations do not use any parameter, parametric aggregations represent an entire class of functions parametrized by one or more real vectors of parameters, which can be even learned during training.","category":"page"},{"location":"manual/aggregation/#Log-sum-exp-(LSE)-aggregation","page":"Bag aggregation","title":"Log-sum-exp (LSE) aggregation","text":"","category":"section"},{"location":"manual/aggregation/","page":"Bag aggregation","title":"Bag aggregation","text":"SegmentedLSE (log-sum-exp) aggregation (Oren Z. Kraus, Lei Jimmy Ba, Brendan Frey (2015)) is parametrized by a vector of positive numbers bmr in (mathbbR^+)^d m that specifies one real parameter for computation in each output dimension:","category":"page"},{"location":"manual/aggregation/","page":"Bag aggregation","title":"Bag aggregation","text":"a_operatornamelse(x_1 ldots x_k r) = frac1rlog left(frac1k sum_i = 1^k exp(rcdot x_i)right)","category":"page"},{"location":"manual/aggregation/","page":"Bag aggregation","title":"Bag aggregation","text":"With different values of r, LSE behaves differently and in fact both max and mean operators are limiting cases of LSE. If r is very small, the output approaches simple mean, and on the other hand, if r is a large number, LSE becomes a smooth approximation of the max function. Naively implementing the definition above may lead to numerical instabilities, however, the Mill implementation is numerically stable.","category":"page"},{"location":"manual/aggregation/","page":"Bag aggregation","title":"Bag aggregation","text":"a_lse = SegmentedLSE(d)\na_lse(X, bags)","category":"page"},{"location":"manual/aggregation/#p-norm-aggregation","page":"Bag aggregation","title":"p-norm aggregation","text":"","category":"section"},{"location":"manual/aggregation/","page":"Bag aggregation","title":"Bag aggregation","text":"(Normalized) p-norm operator (Caglar Gulcehre, Kyunghyun Cho, Razvan Pascanu, Yoshua Bengio (2014)) is parametrized by a vector of real numbers bmp in (mathbbR^+)^d, where forall i in 1 ldots m  colon p_i geq 1, and another vector bmc in (mathbbR^+)^d. It is computed with formula:","category":"page"},{"location":"manual/aggregation/","page":"Bag aggregation","title":"Bag aggregation","text":"a_operatornamepnorm(x_1 ldots x_k p c) = left(frac1k sum_i = 1^k vert x_i - c vert ^ p right)^frac1p","category":"page"},{"location":"manual/aggregation/","page":"Bag aggregation","title":"Bag aggregation","text":"Again, the Mill implementation is stable.","category":"page"},{"location":"manual/aggregation/","page":"Bag aggregation","title":"Bag aggregation","text":"a_pnorm = SegmentedPNorm(d)\na_pnorm(X, bags)","category":"page"},{"location":"manual/aggregation/","page":"Bag aggregation","title":"Bag aggregation","text":"Because all parameter constraints are included implicitly (field ρ in both types is a real number that undergoes appropriate transformation before being used), both parametric operators are easy to use and do not require any special treatment. Replacing the definition of aggregation operators while constructing a model (either manually or with reflectinmodel) is enough.","category":"page"},{"location":"manual/aggregation/#Concatenation","page":"Bag aggregation","title":"Concatenation","text":"","category":"section"},{"location":"manual/aggregation/","page":"Bag aggregation","title":"Bag aggregation","text":"To use a concatenation of two or more operators, one can use the AggregationStack constructor:","category":"page"},{"location":"manual/aggregation/","page":"Bag aggregation","title":"Bag aggregation","text":"a = AggregationStack(a_mean, a_max)\na(X, bags)","category":"page"},{"location":"manual/aggregation/","page":"Bag aggregation","title":"Bag aggregation","text":"For the most common combinations, Mill provides some convenience definitions:","category":"page"},{"location":"manual/aggregation/","page":"Bag aggregation","title":"Bag aggregation","text":"SegmentedMeanMax(d)\nSegmentedPNormLSE(d)","category":"page"},{"location":"manual/aggregation/#Weighted-aggregation","page":"Bag aggregation","title":"Weighted aggregation","text":"","category":"section"},{"location":"manual/aggregation/","page":"Bag aggregation","title":"Bag aggregation","text":"Sometimes, different instances in the bag are not equally important and contribute to output to a different extent. For instance, this may come in handy when performing importance sampling over very large bags. SegmentedMean and SegmentedPNorm have definitions taking weights into account:","category":"page"},{"location":"manual/aggregation/","page":"Bag aggregation","title":"Bag aggregation","text":"a_operatornamemean((x_i w_i)_i=1^k) = frac1sum_i=1^k w_i sum_i = 1^k w_i cdot x_i","category":"page"},{"location":"manual/aggregation/","page":"Bag aggregation","title":"Bag aggregation","text":"a_operatornamepnorm(x_i w_i_i=1^k p c) = left(frac1sum_i=1^k w_i sum_i = 1^k w_icdotvert x_i - c vert ^ p right)^frac1p","category":"page"},{"location":"manual/aggregation/","page":"Bag aggregation","title":"Bag aggregation","text":"This is done in Mill by passing an additional parameter:","category":"page"},{"location":"manual/aggregation/","page":"Bag aggregation","title":"Bag aggregation","text":"w = Float32.([1.0, 0.2, 0.8, 0.5])\na_mean(X, bags, w)\na_pnorm(X, bags, w)","category":"page"},{"location":"manual/aggregation/","page":"Bag aggregation","title":"Bag aggregation","text":"For SegmentedMax and SegmentedLSE it is possible to pass in weights, but they are ignored during computation:","category":"page"},{"location":"manual/aggregation/","page":"Bag aggregation","title":"Bag aggregation","text":"a_max(X, bags, w) == a_max(X, bags)","category":"page"},{"location":"manual/aggregation/#Weighted-nodes","page":"Bag aggregation","title":"Weighted nodes","text":"","category":"section"},{"location":"manual/aggregation/","page":"Bag aggregation","title":"Bag aggregation","text":"WeightedBagNode is used to store instance weights into a dataset. It accepts weights in the constructor:","category":"page"},{"location":"manual/aggregation/","page":"Bag aggregation","title":"Bag aggregation","text":"wbn = WeightedBagNode(n, bags, w)","category":"page"},{"location":"manual/aggregation/","page":"Bag aggregation","title":"Bag aggregation","text":"and passes them to aggregation operators:","category":"page"},{"location":"manual/aggregation/","page":"Bag aggregation","title":"Bag aggregation","text":"m = reflectinmodel(wbn, d -> Dense(d, 3))\nm(wbn)","category":"page"},{"location":"manual/aggregation/","page":"Bag aggregation","title":"Bag aggregation","text":"Otherwise, WeightedBagNode behaves exactly like the standard BagNode.","category":"page"},{"location":"manual/aggregation/#Bag-count","page":"Bag aggregation","title":"Bag count","text":"","category":"section"},{"location":"manual/aggregation/","page":"Bag aggregation","title":"Bag aggregation","text":"For some problems, it may be beneficial to use the size of the bag directly and feed it to subsequent layers. To do this, wrap an instance of AbstractAggregation or AggregationStack in the BagCount type.","category":"page"},{"location":"manual/aggregation/","page":"Bag aggregation","title":"Bag aggregation","text":"In the aggregation phase, bag count appends one more element which stores the bag size to the output after all operators are applied. Furthermore, Mill, performs a mapping x mapsto log(x) + 1 on top of that:","category":"page"},{"location":"manual/aggregation/","page":"Bag aggregation","title":"Bag aggregation","text":"a_mean_bc = BagCount(a_mean)\na_mean_bc(X, bags)","category":"page"},{"location":"manual/aggregation/","page":"Bag aggregation","title":"Bag aggregation","text":"The matrix now has three rows, the last one storing the size of the bag.","category":"page"},{"location":"manual/aggregation/","page":"Bag aggregation","title":"Bag aggregation","text":"Model reflection adds BagCount after each aggregation operator by default.","category":"page"},{"location":"manual/aggregation/","page":"Bag aggregation","title":"Bag aggregation","text":"bn = BagNode(n, bags)\nbm = reflectinmodel(bn, d -> Dense(d, 3))","category":"page"},{"location":"manual/aggregation/","page":"Bag aggregation","title":"Bag aggregation","text":"Note that the bm (sub)model field of the BagNode has size of (7, 3), 3 for each of two aggregation outputs and 1 for sizes of bags.","category":"page"},{"location":"manual/aggregation/","page":"Bag aggregation","title":"Bag aggregation","text":"bm(bn)","category":"page"},{"location":"manual/aggregation/#Default-aggregation-values","page":"Bag aggregation","title":"Default aggregation values","text":"","category":"section"},{"location":"manual/aggregation/","page":"Bag aggregation","title":"Bag aggregation","text":"When all aggregation operators are printed, one may notice that all of them store one additional vector ψ. This is a vector of default parameters, initialized to all zeros, that are used for empty bags:","category":"page"},{"location":"manual/aggregation/","page":"Bag aggregation","title":"Bag aggregation","text":"bags = AlignedBags([1:1, 0:-1, 2:3, 0:-1, 4:4])\na_mean(X, bags)","category":"page"},{"location":"manual/aggregation/","page":"Bag aggregation","title":"Bag aggregation","text":"That's why the dimension of input is required in the constructor. See Missing data page for more information.","category":"page"},{"location":"#Mill.jl-(Multiple-Instance-Learning-Library)","page":"Home","title":"Mill.jl (Multiple Instance Learning Library)","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Mill.jl is a library built on top of Flux.jl aimed to flexibly prototype hierarchical multiple instance learning models as described in Simon Mandlik, Matej Racinsky, Viliam Lisy, Tomas Pevny (2021), Tomáš Pevný, Petr Somol (2017) and  Tomáš Pevný, Petr Somol (2016). It is developed to be:","category":"page"},{"location":"","page":"Home","title":"Home","text":"flexible and versatile\nas general as possible\nfast \nand dependent on only handful of other packages","category":"page"},{"location":"","page":"Home","title":"Home","text":"Watch our introductory talk from JuliaCon 2021.","category":"page"},{"location":"","page":"Home","title":"Home","text":"Go to","category":"page"},{"location":"","page":"Home","title":"Home","text":"Motivation for a brief introduction into the philosophy of Mill\nManual for a brief tutorial into Mill\nExamples for some examples of Mill use\nExternal tools to see examples of integration with other packages\nPublic API for an extensive API reference\nReferences for related literature\nCitation for preferred citation entries","category":"page"},{"location":"#Installation","page":"Home","title":"Installation","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Run the following in REPL:","category":"page"},{"location":"","page":"Home","title":"Home","text":"] add Mill","category":"page"},{"location":"","page":"Home","title":"Home","text":"Julia v1.6 or later is required.","category":"page"},{"location":"api/switches/#General","page":"Switches","title":"General","text":"","category":"section"},{"location":"api/switches/#Index","page":"Switches","title":"Index","text":"","category":"section"},{"location":"api/switches/","page":"Switches","title":"Switches","text":"Pages = [\"general.md\"]","category":"page"},{"location":"api/switches/#API","page":"Switches","title":"API","text":"","category":"section"},{"location":"api/switches/","page":"Switches","title":"Switches","text":"Mill.emptyismissing\nMill.emptyismissing!\n\nMill.string_start_code\nMill.string_start_code!\n\nMill.string_end_code\nMill.string_end_code!","category":"page"},{"location":"api/switches/#Mill.emptyismissing","page":"Switches","title":"Mill.emptyismissing","text":"Mill.emptyismissing()\n\nGet the current value of the emptyismissing parameter.\n\nSee also: Mill.emptyismissing!.\n\n\n\n\n\n","category":"function"},{"location":"api/switches/#Mill.emptyismissing!","page":"Switches","title":"Mill.emptyismissing!","text":"Mill.emptyismissing!(::Bool; persist=false)\n\nSet the new value to the emptyismissing parameter.\n\nSet persist=true to persist this setting between sessions.\n\nSee also: Mill.emptyismissing.\n\n\n\n\n\n","category":"function"},{"location":"api/switches/#Mill.string_start_code","page":"Switches","title":"Mill.string_start_code","text":"Mill.string_start_code()\n\nGet the current value of the string_start_code parameter used as a code point of the abstract string-start character. The default value of the parameter is 0x02, which corresponds to the STX character in ASCII encoding.\n\nSee also: Mill.string_start_code!, Mill.string_end_code, Mill.string_end_code!.\n\n\n\n\n\n","category":"function"},{"location":"api/switches/#Mill.string_start_code!","page":"Switches","title":"Mill.string_start_code!","text":"Mill.string_start_code!(c::Integer; persist=false)\n\nSet the new value to the string_start_code parameter used as a code point of the abstract string-start character to c. The default value of the parameter is 0x02, which corresponds to the STX character in ASCII encoding.\n\nc should fit into UInt8.\n\nSet persist=true to persist this setting between sessions.\n\nSee also: Mill.string_start_code, Mill.string_end_code, Mill.string_end_code!.\n\n\n\n\n\n","category":"function"},{"location":"api/switches/#Mill.string_end_code","page":"Switches","title":"Mill.string_end_code","text":"Mill.string_end_code()\n\nGet the current value of the string_end_code parameter used as a code point of the abstract string-end character. The default value of the parameter is 0x03, which corresponds to the ETX character in ASCII encoding.\n\nSee also: Mill.string_end_code!, Mill.string_start_code, Mill.string_start_code!.\n\n\n\n\n\n","category":"function"},{"location":"api/switches/#Mill.string_end_code!","page":"Switches","title":"Mill.string_end_code!","text":"Mill.string_end_code!(c::Integer; persist=false)\n\nSet the new value to the string_end_code parameter used as a code point of the abstract string-end character to c. The default value of the parameter is 0x03, which corresponds to the ETX character in ASCII encoding.\n\nc should fit into UInt8.\n\nSet persist=true to persist this setting between sessions.\n\nSee also: Mill.string_end_code, Mill.string_start_code, Mill.string_start_code!.\n\n\n\n\n\n","category":"function"}]
}
