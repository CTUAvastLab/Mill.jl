var documenterSearchIndex = {"docs":
[{"location":"architecture/aggregation/#TEST","page":"Aggregations","title":"TEST","text":"","category":"section"},{"location":"architecture/aggregation/#Default-aggregation-values","page":"Aggregations","title":"Default aggregation values","text":"","category":"section"},{"location":"architecture/aggregation/","page":"Aggregations","title":"Aggregations","text":"With the latest version of Mill, it is also possible to work with missing data, replacing a missing bag with a default constant value, and even to learn this value as well. Everything is done automatically.","category":"page"},{"location":"examples/graphs/#GNNs-with-Mill-in-16-lines","page":"GNN in 16 lines","title":"GNNs with Mill in 16 lines","text":"","category":"section"},{"location":"examples/graphs/","page":"GNN in 16 lines","title":"GNN in 16 lines","text":"As has been mentioned in [Simon], the multi-instance learning is a essential piece to implement message passing in inference over graphs (called spatial Graph Neural Networks). It is really simple to make the idea fly is really small.","category":"page"},{"location":"examples/graphs/","page":"GNN in 16 lines","title":"GNN in 16 lines","text":"Let's assume a graph g, in this case created by barabasi_albert function, and let's assume that each vertex is described by a feature matrix x","category":"page"},{"location":"examples/graphs/","page":"GNN in 16 lines","title":"GNN in 16 lines","text":"using LightGraphs, Mill, Flux\ng = barabasi_albert(10, 3, 2)\nx = ArrayNode(randn(Float32, 7, 10))","category":"page"},{"location":"examples/graphs/","page":"GNN in 16 lines","title":"GNN in 16 lines","text":"What we do, is that we use Mill.ScatteredBags from Mill.jl to encode the neighbors of each vertex. That means that each vertex will be described by a bag of its neighbors. This information is by convenience stored in fadjlist of a graph g, therefore the bags are constructed as","category":"page"},{"location":"examples/graphs/","page":"GNN in 16 lines","title":"GNN in 16 lines","text":"b = Mill.ScatteredBags(g.fadjlist)","category":"page"},{"location":"examples/graphs/","page":"GNN in 16 lines","title":"GNN in 16 lines","text":"Finally, we create two models. First pre-process the description of vertices to some latent dimension for message passing, we will call this lift, and then a network realizing the message passing, we will call this one mp","category":"page"},{"location":"examples/graphs/","page":"GNN in 16 lines","title":"GNN in 16 lines","text":"lift = reflectinmodel(x, d -> Dense(d, 10), d -> SegmentedMean(d))\nxx = lift(x)\nmp = reflectinmodel(BagNode(xx, b), d -> Dense(d, 10), d -> SegmentedMean(d))","category":"page"},{"location":"examples/graphs/","page":"GNN in 16 lines","title":"GNN in 16 lines","text":"Notice that BagNode(xx, b) now essentially encodes the features about vertices and also the adjacency matrix. This also means that one step of message passing algorithm can be realized as mp(BagNode(xx,b)) and it is differentiable, which can be verified by executing gradient(() -> sum(sin.(mp(BagNode(xx,b)).data)), Flux.params(mp)). ","category":"page"},{"location":"examples/graphs/","page":"GNN in 16 lines","title":"GNN in 16 lines","text":"So if we put everything together, the GNN implementation is following block of code (16 lines of mostly sugar).","category":"page"},{"location":"examples/graphs/","page":"GNN in 16 lines","title":"GNN in 16 lines","text":"using Flux, Mill, LightGraphs, Statistics\n\nstruct GNN{L,M, R}\n\tlift::L\n\tmp::M\n\tm::R\nend\n\nFlux.@functor GNN\n\nfunction mpstep(m::GNN, xx::ArrayNode, bags, n)\n\tn == 0 && return(xx)\n\tmpstep(m, m.mp(BagNode(xx, bags)), bags, n - 1)\nend\n\nfunction (m::GNN)(g, x, n)\n\txx = m.lift(x)\n\tbags = Mill.ScatteredBags(g.fadjlist)\n\to = mpstep(m, xx, bags, n)\n\tm.m(vcat(mean(o.data, dims = 2), maximum(o.data, dims = 2)))\nend","category":"page"},{"location":"examples/graphs/","page":"GNN in 16 lines","title":"GNN in 16 lines","text":"The initialization of the model is little tedious, but defining two helper functions for creating feed-forward neural networks ffnn and aggregation agg helps a bit. On the end, the graph neural network is properly integrated with Flux ecosystem and suports automatic differentiation.","category":"page"},{"location":"examples/graphs/","page":"GNN in 16 lines","title":"GNN in 16 lines","text":"zdim = 10\nrounds = 5\n\nffnn(d) = Chain(Dense(d, zdim, relu), Dense(zdim, zdim))\nagg(d) = SegmentedMeanMax(d)\n\ng = barabasi_albert(10, 3, 2)\nx = ArrayNode(randn(Float32, 7, 10))\ngnn = GNN(reflectinmodel(x, ffnn, agg),\n\tBagModel(ffnn(zdim), agg(zdim), ffnn(2zdim)),\n\tffnn(2zdim)\n\t)\n\ngnn(g, x, rounds)\ngradient(() -> sum(sin.(gnn(g, x, rounds))), Flux.params(gnn))","category":"page"},{"location":"examples/graphs/","page":"GNN in 16 lines","title":"GNN in 16 lines","text":"The above implementation is surprisingly general, as it supports a rich description of vertices, by which we mean that the description can be anything expressible by Mill (full JSONs). The missing piece is putting weights on edges, which would be a bit more complicated.","category":"page"},{"location":"architecture/missing/#Missing-values","page":"Missing values","title":"Missing values","text":"","category":"section"},{"location":"architecture/missing/","page":"Missing values","title":"Missing values","text":"At the moment, Mill.jl features an initial and naive approach to missing values. We assume that ArrayNode have missing values replaced by zeros, which is not optimal but in many situations it works reasonably well.","category":"page"},{"location":"architecture/missing/","page":"Missing values","title":"Missing values","text":"BagNodes with missing features are indicated by Bags being set to [0:-1] with missing as a data and metadata. This can be seamlessly concatenated or sub-set, if the operation makes sense.","category":"page"},{"location":"architecture/missing/","page":"Missing values","title":"Missing values","text":"Couple examples from unit tests. Let's define full and empty BagNode","category":"page"},{"location":"architecture/missing/","page":"Missing values","title":"Missing values","text":"julia> a = BagNode(ArrayNode(rand(3,4)),[1:4], nothing)\nBagNode with 1 bag(s)\n  └── ArrayNode(3, 4)\n\njulia> e = BagNode(missing, AlignedBags([0:-1]), nothing)\nBagNode with 1 empty bag(s)","category":"page"},{"location":"architecture/missing/","page":"Missing values","title":"Missing values","text":"We can concatenate them as follows.","category":"page"},{"location":"architecture/missing/","page":"Missing values","title":"Missing values","text":"julia> x = reduce(catobs,[a, e])\nBagNode with 2 bag(s)\n  └── ArrayNode(3, 4)","category":"page"},{"location":"architecture/missing/","page":"Missing values","title":"Missing values","text":"Notice, that the ArrayNode has still the same dimension as ArrayNode of just a. The missing second element, corresponding to e is indicated by the second bags being 0:-1 as follows:","category":"page"},{"location":"architecture/missing/","page":"Missing values","title":"Missing values","text":"julia> x.bags\nAlignedBags(UnitRange{Int64}[1:4, 0:-1])","category":"page"},{"location":"architecture/missing/","page":"Missing values","title":"Missing values","text":"We can get back the missing second element as","category":"page"},{"location":"architecture/missing/","page":"Missing values","title":"Missing values","text":"julia> x[2]\nBagNode with 1 empty bag(s)","category":"page"},{"location":"architecture/missing/","page":"Missing values","title":"Missing values","text":"During forward (and backward) pass, the missing values in BagNodes are filled in aggregation by zeros. ** In order this feature to work, the Aggregation needs to know dimension, therefore use MissingAggregation, which can handle this.** In the future, MissingAggregation will be made default.","category":"page"},{"location":"architecture/missing/","page":"Missing values","title":"Missing values","text":"Last but not least, ProductNodes cannot handle missing values, as the missingness is propagated to its leaves, i.e.","category":"page"},{"location":"architecture/missing/","page":"Missing values","title":"Missing values","text":"julia> ProductNode((a,e))\nProductNode{2}\n  ├── BagNode with 1 bag(s)\n  │     └── ArrayNode(3, 4)\n  └── BagNode with 1 empty bag(s)","category":"page"},{"location":"architecture/missing/#Representing-missing-values","page":"Missing values","title":"Representing missing values","text":"","category":"section"},{"location":"architecture/missing/","page":"Missing values","title":"Missing values","text":"The library currently support two ways to represent bags with missing values. First one represent missing data using missing as a = BagNode(missing, [0:-1]) while the second as an empty vector as a = BagNode(zero(4,0), [0:-1]).  While off the shelf the library supports both approaches transparently, the difference is mainly when one uses getindex, and therefore there is a switch Mill.emptyismissing(false), which is by default false. Let me demonstrate the difference.","category":"page"},{"location":"architecture/missing/","page":"Missing values","title":"Missing values","text":"julia> a = BagNode(ArrayNode(rand(3,2)), [1:2, 0:-1])\nBagNode with 2 bag(s)\n  └── ArrayNode(3, 2)\n\njulia> Mill.emptyismissing(false);\n\njulia> a[2].data\nArrayNode(3, 0)\n\njulia> Mill.emptyismissing(true)\ntrue\n\njulia> a[2].data\nmissing","category":"page"},{"location":"architecture/missing/","page":"Missing values","title":"Missing values","text":"The advantage of the first approach, default, is that types are always the same, which is nice to the compiler (and Zygote). The advantage of the latter is that it is more compact and nicer.","category":"page"},{"location":"tools/hierarchical/#Hierarchical-utils","page":"HierarichalUtils.jl","title":"Hierarchical utils","text":"","category":"section"},{"location":"tools/hierarchical/","page":"HierarichalUtils.jl","title":"HierarichalUtils.jl","text":"Mill.jl uses HierarchicalUtils.jl which brings a lot of additional features. For instance, if you want to print a non-truncated version of a model, call:","category":"page"},{"location":"tools/hierarchical/","page":"HierarichalUtils.jl","title":"HierarichalUtils.jl","text":"julia> printtree(m; trunc=Inf)\n\nBagModel ↦ ⟨SegmentedMean(3), SegmentedMax(3)⟩ ↦ ArrayModel(Dense(6, 3, relu))\n  └── ProductModel ↦ ArrayModel(Dense(12, 3, relu))\n        ├── BagModel ↦ ⟨SegmentedMean(3), SegmentedMax(3)⟩ ↦ ArrayModel(Dense(6, 3, relu))\n        │     └── ArrayModel(Dense(4, 3, relu))\n        ├── ArrayModel(Dense(3, 3, relu))\n        ├── BagModel ↦ ⟨SegmentedMean(3), SegmentedMax(3)⟩ ↦ ArrayModel(Dense(6, 3, relu))\n        │     └── BagModel ↦ ⟨SegmentedMean(3), SegmentedMax(3)⟩ ↦ ArrayModel(Dense(6, 3, relu))\n        │           └── ArrayModel(Dense(2, 3, relu))\n        └── ArrayModel(Dense(2, 3, relu))","category":"page"},{"location":"tools/hierarchical/","page":"HierarichalUtils.jl","title":"HierarichalUtils.jl","text":"Callling with trav=true enables convenient traversal functionality with string indexing:","category":"page"},{"location":"tools/hierarchical/","page":"HierarichalUtils.jl","title":"HierarichalUtils.jl","text":"julia>  printtree(m; trunc=Inf, trav=true)\n\nBagModel ↦ ⟨SegmentedMean(3), SegmentedMax(3)⟩ ↦ ArrayModel(Dense(6, 3, relu)) [\"\"]\n  └── ProductModel ↦ ArrayModel(Dense(12, 3, relu)) [\"U\"]\n        ├── BagModel ↦ ⟨SegmentedMean(3), SegmentedMax(3)⟩ ↦ ArrayModel(Dense(6, 3, relu)) [\"Y\"]\n        │     └── ArrayModel(Dense(4, 3, relu)) [\"a\"]\n        ├── ArrayModel(Dense(3, 3, relu)) [\"c\"]\n        ├── BagModel ↦ ⟨SegmentedMean(3), SegmentedMax(3)⟩ ↦ ArrayModel(Dense(6, 3, relu)) [\"g\"]\n        │     └── BagModel ↦ ⟨SegmentedMean(3), SegmentedMax(3)⟩ ↦ ArrayModel(Dense(6, 3, relu)) [\"i\"]\n        │           └── ArrayModel(Dense(2, 3, relu)) [\"j\"]\n        └── ArrayModel(Dense(2, 3, relu)) [\"k\"]","category":"page"},{"location":"tools/hierarchical/","page":"HierarichalUtils.jl","title":"HierarichalUtils.jl","text":"This way any node in the model tree is swiftly accessible, which may come in handy when inspecting model parameters or simply deleting/replacing/inserting nodes to tree (for instance when constructing adversarial samples). All tree nodes are accessible by indexing with the traversal code:.","category":"page"},{"location":"tools/hierarchical/","page":"HierarichalUtils.jl","title":"HierarichalUtils.jl","text":"julia> m[\"Y\"]\n\nBagModel ↦ ⟨SegmentedMean(3), SegmentedMax(3)⟩ ↦ ArrayModel(Dense(6, 3, relu))\n  └── ArrayModel(Dense(4, 3, relu))","category":"page"},{"location":"tools/hierarchical/","page":"HierarichalUtils.jl","title":"HierarichalUtils.jl","text":"The following two approaches give the same result:","category":"page"},{"location":"tools/hierarchical/","page":"HierarichalUtils.jl","title":"HierarichalUtils.jl","text":"julia> m[\"Y\"] === m.im.ms[1]\n\ntrue","category":"page"},{"location":"tools/hierarchical/","page":"HierarichalUtils.jl","title":"HierarichalUtils.jl","text":"Other functions provided by HierarchicalUtils.jl:","category":"page"},{"location":"tools/hierarchical/","page":"HierarichalUtils.jl","title":"HierarichalUtils.jl","text":"julia> nnodes(m)\n\n9\n\njulia> nleafs(m)\n\n4\n\njulia> NodeIterator(m) |> collect\n\n9-element Array{AbstractMillModel,1}:\n BagModel\n ProductModel\n BagModel\n ArrayModel\n ArrayModel\n BagModel\n BagModel\n ArrayModel\n ArrayModel\n\njulia> LeafIterator(m) |> collect\n\n4-element Array{ArrayModel{Dense{typeof(relu),Array{Float32,2},Array{Float32,1}}},1}:\n ArrayModel\n ArrayModel\n ArrayModel\n ArrayModel\n\njulia> TypeIterator(m, BagModel) |> collect\n\n4-element Array{BagModel{T,Aggregation{2},ArrayModel{Dense{typeof(relu),Array{Float32,2},Array{Float32,1}}}} where T<:AbstractMillModel,1}:\n BagModel\n BagModel\n BagModel\n BagModel","category":"page"},{"location":"tools/hierarchical/","page":"HierarichalUtils.jl","title":"HierarichalUtils.jl","text":"... and many others, see HierarchicalUtils.jl.","category":"page"},{"location":"architecture/strings/","page":"Handling strings","title":"Handling strings","text":"#NGRAMs","category":"page"},{"location":"examples/simple/#Multiple-instance-learning","page":"Simple","title":"Multiple instance learning","text":"","category":"section"},{"location":"examples/simple/","page":"Simple","title":"Simple","text":"Musk dataset is a classic MIL problem of the field, introduced in the problem defining publication [1]. Below we use to demonstrate, how to solve the problem using Mill.jl. The full example is shown in example/musk.jl, which also contains Julia environment to run it","category":"page"},{"location":"examples/simple/","page":"Simple","title":"Simple","text":"For the demo, we need following libraries and functions.","category":"page"},{"location":"examples/simple/","page":"Simple","title":"Simple","text":"julia> using FileIO, JLD2, Statistics, Mill, Flux\njulia> using Flux: throttle, @epochs\njulia> using Mill: reflectinmodel\njulia> using Base.Iterators: repeated","category":"page"},{"location":"examples/simple/","page":"Simple","title":"Simple","text":"The musk.jld2 contains matrix with features, fMat, the id of sample (called bag in MIL terminology) to which each instance (column in fMat) belongs to, and the label of each instance in y (the label of a bag is a maximum of labels of its instances, i.e. one positive instance in a bag makes itp positive.  BagNode is a structure which holds (i) feature matrix and (ii) ranges identifying which columns in the feature matrix each bag spans. This representation ensures that feed-forward networks do not need to deal with bag boundaries and always process full continuous maatrices. BagNode (and generally every Node) can be concatenated using cat or  catobs and indexed using getindex.","category":"page"},{"location":"examples/simple/","page":"Simple","title":"Simple","text":"julia> fMat = load(\"musk.jld2\", \"fMat\");         # matrix with instances, each column is one sample\njulia> bagids = load(\"musk.jld2\", \"bagids\");     # ties instances to bags\njulia> x = BagNode(ArrayNode(fMat), bagids);     # create BagDataset\njulia> y = load(\"musk.jld2\", \"y\");               # load labels\njulia> y = map(i -> maximum(y[i]) + 1, x.bags);  # create labels on bags\njulia> y = Flux.onehotbatch(y, 1:2);          # one-hot encoding","category":"page"},{"location":"examples/simple/","page":"Simple","title":"Simple","text":"Once data are in Mill internal format, we can manually create a model. BagModel is designed to implement a basic multi-instance learning model utilizing two feed-forward networks with aggregaton layer in between. Below, we use a simple model, where instances are first passed through a single layer with 10 neurons (input dimension is 166) with relu non-linearity, then we use mean and max aggregation functions simultaneously (for some problems, max is better then mean, therefore we use both), and then we use one layer with 10 neurons and relu nonlinearity followed by output linear layer with 2 neurons (output dimension).","category":"page"},{"location":"examples/simple/","page":"Simple","title":"Simple","text":"julia> model = BagModel(\n    ArrayModel(Dense(166, 10, Flux.relu)),                      # model on the level of Flows\n    SegmentedMeanMax(10),                                       # aggregation\n    ArrayModel(Chain(Dense(20, 10, Flux.relu), Dense(10, 2))))  # model on the level of bags\n    \nBagModel ↦ ⟨SegmentedMean(10), SegmentedMax(10)⟩ ↦ ArrayModel(Chain(Dense(20, 10, relu), Dense(10, 2)))\n  └── ArrayModel(Dense(166, 10, relu))","category":"page"},{"location":"examples/simple/","page":"Simple","title":"Simple","text":"Since Mill is made maximally compatible with Flux, we can use its cross-entropy","category":"page"},{"location":"examples/simple/","page":"Simple","title":"Simple","text":"julia> loss(x, y) = Flux.logitcrossentropy(model(x).data, y);","category":"page"},{"location":"examples/simple/","page":"Simple","title":"Simple","text":"and train it using its tooling","category":"page"},{"location":"examples/simple/","page":"Simple","title":"Simple","text":"julia> evalcb = () -> @show(loss(x, y));\njulia> opt = Flux.ADAM();\njulia> @epochs 10 Flux.train!(loss, params(model), repeated((x, y), 1000), opt, cb=throttle(evalcb, 1))\n\n[ Info: Epoch 1\nloss(x, y) = 87.793724f0\n[ Info: Epoch 2\nloss(x, y) = 4.3207192f0\n[ Info: Epoch 3\nloss(x, y) = 4.2778687f0\n[ Info: Epoch 4\nloss(x, y) = 0.662226f0\n[ Info: Epoch 5\nloss(x, y) = 5.76351f-6\n[ Info: Epoch 6\nloss(x, y) = 3.8146973f-6\n[ Info: Epoch 7\nloss(x, y) = 2.8195589f-6\n[ Info: Epoch 8\nloss(x, y) = 2.4878461f-6\n[ Info: Epoch 9\nloss(x, y) = 2.1561332f-6\n[ Info: Epoch 10\nloss(x, y) = 1.7414923f-6","category":"page"},{"location":"examples/simple/","page":"Simple","title":"Simple","text":"Because I was lazy and I have not left any data for validation, we can only calculate error on the training data, which should be not so surprisingly low.","category":"page"},{"location":"examples/simple/","page":"Simple","title":"Simple","text":"mean(mapslices(argmax, model(x).data, dims=1)' .!= y)\n\n0.0","category":"page"},{"location":"examples/simple/","page":"Simple","title":"Simple","text":"<a name=\"cit1\"><b>1</b></a> Solving the multiple instance problem with axis-parallel rectangles, Dietterich, Thomas G., Richard H. Lathrop, and Tomás Lozano-Pérez, 1997","category":"page"},{"location":"examples/dag/#Directed-Acyclic-Graphs-in-Mill.jl","page":"DAGs","title":"Directed Acyclic Graphs in Mill.jl","text":"","category":"section"},{"location":"examples/dag/","page":"DAGs","title":"DAGs","text":"In this exercice, I was interesting in a following problem. Imagine a data / knowlege base represented in a form of a directed acyclic graph (DAG), where the vertex would be modelled on basis of its parents (and their parents), but not on its descendants. I was further interested in a case, where the descendants of some vertex i would represent only subset of all nodes in the graph. How to do this with a least pain and efficiently?","category":"page"},{"location":"examples/dag/","page":"DAGs","title":"DAGs","text":"In the course of calculating the value of a vertex i, there can be vertices which can be used more that one time. This means that we would like to have a cache of already calculated values, which is difficult since Zygote does not like setindex operation. But, the cache is assigned only once, which means that it can be realized through Zygote.buffer. So, here we go.","category":"page"},{"location":"examples/dag/","page":"DAGs","title":"DAGs","text":"At first, we initiate the cache as","category":"page"},{"location":"examples/dag/","page":"DAGs","title":"DAGs","text":"initcache(g, k) = [Zygote.Buffer(zeros(Float32, k, 1)) for _ in 1:nv(g)]","category":"page"},{"location":"examples/dag/","page":"DAGs","title":"DAGs","text":"To get the value of a vertex, we just delegate the question to cache as ","category":"page"},{"location":"examples/dag/","page":"DAGs","title":"DAGs","text":"function (model::DagModel)(g::DagGraph, i)\n  cache = initcache(g.g, model.odim)\n  ArrayNode(getfromcache!(cache, g, model, i))\nend","category":"page"},{"location":"examples/dag/","page":"DAGs","title":"DAGs","text":"which means that the getfromcache! will do all the heavy lifting. But that function will just check, if the value in cache has been already calculated, or it calculates the value (applying model on millvertex!) and freezes the calculated item in cache.","category":"page"},{"location":"examples/dag/","page":"DAGs","title":"DAGs","text":"function getfromcache!(cache, g::DagGraph, model::DagModel, i::Int)\n  cache[i].freeze && return(copy(cache[i]))\n  ds = millvertex!(cache, g, model, i)\n  cache[i][:] = model.m(ds).data\n  return(copy(cache[i]))\nend","category":"page"},{"location":"examples/dag/","page":"DAGs","title":"DAGs","text":"and what millvertex! function does? It just takes the representation of ancestors (from cache) and put them together","category":"page"},{"location":"examples/dag/","page":"DAGs","title":"DAGs","text":"function millvertex!(cache, g::DagGraph, model::DagModel, i)\n  ProductNode((neighbours = millneighbors!(cache, g, model, i), \n    vertex = vertex_features[i])\n  )\nend","category":"page"},{"location":"examples/dag/","page":"DAGs","title":"DAGs","text":"Wait a sec, am I running in circles? Yes, and that is the art of recursion. Below is the complete example I have once written. Note that it is not the most efficient approach to implement this. It would be better to spent a little time with graphs to identify sets of vertices that can be processed in parallel and for which all ancestors are know. But this was a fun little exercise.","category":"page"},{"location":"examples/dag/","page":"DAGs","title":"DAGs","text":"using Flux, Zygote\nusing LightGraphs, MetaGraphs, Mill, Setfield\n\n\nstruct DagGraph{G<:SimpleDiGraph,T}\n  g::G\n  vertex_features::T\nend\n\nZygote.@nograd LightGraphs.inneighbors\n\nstruct DagModel{M}\n  m::M\n  odim::Int\nend\n\nFlux.@functor DagModel\n\n\nfunction (model::DagModel)(g::DagGraph, i)\n  cache = initcache(g.g, model.odim)\n  ArrayNode(getfromcache!(cache, g, model, i))\nend\n\n(model::DagModel)(g::SimpleDiGraph, vertex_features, i) = model(DagGraph(g, vertex_features), i)\n\n\ninitcache(g, k) = [Zygote.Buffer(zeros(Float32, k, 1)) for _ in 1:nv(g)]\nZygote.@nograd initcache\n\nfunction millvertex!(cache, g::DagGraph, model::DagModel, i)\n  ProductNode((neighbours = millneighbors!(cache, g, model, i), \n    vertex = vertex_features[i])\n  )\nend\n\nfunction getfromcache!(cache, g::DagGraph, model::DagModel, ii::Vector{Int}) \n  reduce(catobs, [getfromcache!(cache, g, model, i) for i in ii])\nend\n\nfunction getfromcache!(cache, g::DagGraph, model::DagModel, i::Int)\n  cache[i].freeze && return(copy(cache[i]))\n  ds = millvertex!(cache, g, model, i)\n  cache[i][:] = model.m(ds).data\n  return(copy(cache[i]))\nend\n\nfunction millneighbors!(cache, g::DagGraph, model::DagModel, ii::Vector{Int})\n  isempty(ii) && return(BagNode(missing, [0:-1]))\n  xs = [getfromcache!(cache, g, model, i) for i in  ii]\n  BagNode(ArrayNode(reduce(catobs, xs)), [1:length(xs)])\nend\n\nmillneighbors!(cache, g::DagGraph, model::DagModel, i::Int) = millneighbors!(cache, g, model, inneighbors(g.g, i))","category":"page"},{"location":"examples/advanced/#More-complicated-models","page":"Advanced","title":"More complicated models","text":"","category":"section"},{"location":"examples/advanced/","page":"Advanced","title":"Advanced","text":"The main advantage of the Mill library is that it allows to arbitrarily nest and cross-product BagModels, as is described in Theorem 5 of [3].  Let's start the demonstration by nesting two MIL problems. The outer MIL model contains three samples. The first sample contains another bag (inner MIL) problem with two instances, the second sample contains two inner bags with total of three instances, and finally the third sample contains two inner bags with four instances.","category":"page"},{"location":"examples/advanced/","page":"Advanced","title":"Advanced","text":"julia> ds = BagNode(BagNode(ArrayNode(randn(4,10)),[1:2,3:4,5:5,6:7,8:10]),[1:1,2:3,4:5])\nBagNode with 3 bag(s)\n  └── BagNode with 5 bag(s)\n        └── ArrayNode(4, 10)","category":"page"},{"location":"examples/advanced/","page":"Advanced","title":"Advanced","text":"We can create the model manually as in the case of Musk as","category":"page"},{"location":"examples/advanced/","page":"Advanced","title":"Advanced","text":"julia> m = BagModel(\n    BagModel(\n        ArrayModel(Dense(4, 3, Flux.relu)),   \n        SegmentedMeanMax(3),\n        ArrayModel(Dense(6, 3, Flux.relu))),\n    SegmentedMeanMax(3),\n    ArrayModel(Chain(Dense(6, 3, Flux.relu), Dense(3,2))))\n\nBagModel ↦ ⟨SegmentedMean(3), SegmentedMax(3)⟩ ↦ ArrayModel(Chain(Dense(6, 3, relu), Dense(3, 2)))\n  └── BagModel ↦ ⟨SegmentedMean(3), SegmentedMax(3)⟩ ↦ ArrayModel(Dense(6, 3, relu))\n        └── ArrayModel(Dense(4, 3, relu))","category":"page"},{"location":"examples/advanced/","page":"Advanced","title":"Advanced","text":"and we can apply the model as","category":"page"},{"location":"examples/advanced/","page":"Advanced","title":"Advanced","text":"julia> m(ds)\n\nArrayNode(2, 3)","category":"page"},{"location":"examples/advanced/","page":"Advanced","title":"Advanced","text":"Since constructions of large models can be a process prone to errors, there is a function reflectinmodel which tries to automatize it keeping track of dimensions. It accepts as a first parameter a sample ds. Using the function on the above example creates a model:","category":"page"},{"location":"examples/advanced/","page":"Advanced","title":"Advanced","text":"julia> m = reflectinmodel(ds)\n\nBagModel ↦ SegmentedMean(10) ↦ ArrayModel(Dense(10, 10))\n  └── BagModel ↦ SegmentedMean(10) ↦ ArrayModel(Dense(10, 10))\n        └── ArrayModel(Dense(4, 10))","category":"page"},{"location":"examples/advanced/","page":"Advanced","title":"Advanced","text":"To have better control over the topology, reflectinmodel accepts up to four additional parameters. The second parameter is a function returning layer (or set of layers) with input dimension d, and the third function is a function returning aggregation functions for BagModel:","category":"page"},{"location":"examples/advanced/","page":"Advanced","title":"Advanced","text":"julia> m = reflectinmodel(ds, d -> Dense(d, 5, relu), d -> SegmentedMeanMax(d))\n\nBagModel ↦ ⟨SegmentedMean(5), SegmentedMax(5)⟩ ↦ ArrayModel(Dense(10, 5, relu))\n  └── BagModel ↦ ⟨SegmentedMean(5), SegmentedMax(5)⟩ ↦ ArrayModel(Dense(10, 5, relu))\n        └── ArrayModel(Dense(4, 5, relu))","category":"page"},{"location":"examples/advanced/","page":"Advanced","title":"Advanced","text":"Let's test the model","category":"page"},{"location":"examples/advanced/","page":"Advanced","title":"Advanced","text":"julia> m(ds).data\n\n5×3 Array{Float32,2}:\n 0.0542484   0.733629  0.553823\n 0.062246    0.866254  1.03062 \n 0.027454    1.04703   1.63135 \n 0.00796955  0.36415   1.18108 \n 0.034735    0.17383   0.0","category":"page"},{"location":"examples/advanced/#Even-more-complicated-models","page":"Advanced","title":"Even more complicated models","text":"","category":"section"},{"location":"examples/advanced/","page":"Advanced","title":"Advanced","text":"As already mentioned above, the datasets can contain Cartesian products of MIL and normal (non-MIL) problems. Let's do a quick demo.","category":"page"},{"location":"examples/advanced/","page":"Advanced","title":"Advanced","text":"julia> ds = BagNode(\n    ProductNode(\n        (BagNode(ArrayNode(randn(4,10)),[1:2,3:4,5:5,6:7,8:10]),\n        ArrayNode(randn(3,5)),\n        BagNode(\n            BagNode(ArrayNode(randn(2,30)),[i:i+1 for i in 1:2:30]),\n            [1:3,4:6,7:9,10:12,13:15]),\n        ArrayNode(randn(2,5)))),\n    [1:1,2:3,4:5])\n\nBagNode with 3 bag(s)\n  └── ProductNode\n        ├── BagNode with 5 bag(s)\n        │     ⋮\n        ├── ArrayNode(3, 5)\n        ├── BagNode with 5 bag(s)\n        │     ⋮\n        └── ArrayNode(2, 5)","category":"page"},{"location":"examples/advanced/","page":"Advanced","title":"Advanced","text":"For this, we really want to create model automatically despite it being sub-optimal.","category":"page"},{"location":"examples/advanced/","page":"Advanced","title":"Advanced","text":"julia> m = reflectinmodel(ds, d -> Dense(d, 3, relu), d -> SegmentedMeanMax(d))\n\nBagModel ↦ ⟨SegmentedMean(3), SegmentedMax(3)⟩ ↦ ArrayModel(Dense(6, 3, relu))\n  └── ProductModel ↦ ArrayModel(Dense(12, 3, relu))\n        ├── BagModel ↦ ⟨SegmentedMean(3), SegmentedMax(3)⟩ ↦ ArrayModel(Dense(6, 3, relu))\n        │     ⋮\n        ├── ArrayModel(Dense(3, 3, relu))\n        ├── BagModel ↦ ⟨SegmentedMean(3), SegmentedMax(3)⟩ ↦ ArrayModel(Dense(6, 3, relu))\n        │     ⋮\n        └── ArrayModel(Dense(2, 3, relu))","category":"page"},{"location":"architecture/overview/","page":"Overview","title":"Overview","text":"Mill.jl thinly wraps data and models, such that complicated hierarchies can be build. The core idea is that data are wrapped to DataNode <:AbstractNode. To each DataNode corresponds to a ModelNode <: AbstractModelNode, such that output of ModelNode on DataNode is always an ArrayNode, which encapsulates a Matrix with features. By doing so, we ensure that the models know, what to expect if they rely on other models. ","category":"page"},{"location":"architecture/overview/","page":"Overview","title":"Overview","text":"Below we will go through implementation of ArrayNode, BagNode and ProductNode together with their models. These three types are sufficient to represent any JSON file and using corresponding models to convert it to vector represention (or classify it)","category":"page"},{"location":"architecture/overview/#ArrayNode-and-ArrayModel","page":"Overview","title":"ArrayNode and ArrayModel","text":"","category":"section"},{"location":"architecture/overview/","page":"Overview","title":"Overview","text":"ArrayNode wraps a feature matrix. Similarly ArrayModel wraps any function performing operation over this feature matrix. In example below, we wrap feature matrix x and Flux.Dense model.","category":"page"},{"location":"architecture/overview/","page":"Overview","title":"Overview","text":"using Mill, Flux\nds = ArrayNode(Float32.([1 2 3; 4 5 6]))\nm = ArrayModel(Dense(2,3,relu))\nm(ds).data == m.m(ds.data)","category":"page"},{"location":"architecture/overview/","page":"Overview","title":"Overview","text":"As mentioned, the ArrayNode supports \"slicing\" and concatenation as follows","category":"page"},{"location":"architecture/overview/","page":"Overview","title":"Overview","text":"julia> ds[1].data\n2×1 Array{Float32,2}:\n 1.0\n 4.0\n\njulia> ds[[1,3]].data\n2×2 Array{Float32,2}:\n 1.0  3.0\n 4.0  6.0\n\njulia> catobs(ds[1], ds[3]).data\n2×2 Array{Float32,2}:\n 1.0  3.0\n 4.0  6.0","category":"page"},{"location":"architecture/overview/","page":"Overview","title":"Overview","text":"which is useful for creating minibatches and their permutations.","category":"page"},{"location":"architecture/overview/#BagNode-and-BagModel","page":"Overview","title":"BagNode and BagModel","text":"","category":"section"},{"location":"architecture/overview/","page":"Overview","title":"Overview","text":"BagNode is the darling of the Mill library, as it implements the basic mill problem. BagModel wraps some node <:AbstractNode and information, which instances belongs to which bag which we assume to be independent. Continuing with the above example","category":"page"},{"location":"architecture/overview/","page":"Overview","title":"Overview","text":"julia> ds = BagNode(ArrayNode(Float32.([1 2 3; 4 5 6])), [1:2, 0:-1, 3:3])\nBagNode with 3 bag(s)\n  └── ArrayNode(2, 3)","category":"page"},{"location":"architecture/overview/","page":"Overview","title":"Overview","text":"defines a BagNode which contains three bags. The first one contains two instances {(1,4), (2,5)}, the second is empty, and the third one contains single instance {(3,6)}. Again, the BagNode supports indexing and concatenation","category":"page"},{"location":"architecture/overview/","page":"Overview","title":"Overview","text":"julia> ds[1]\nBagNode with 1 bag(s)\n  └── ArrayNode(2, 2)\n\njulia> ds[[1,3]]\nBagNode with 2 bag(s)\n  └── ArrayNode(2, 3)\n\njulia> catobs(ds[1], ds[3])\nBagNode with 2 bag(s)\n  └── ArrayNode(2, 3)","category":"page"},{"location":"architecture/overview/","page":"Overview","title":"Overview","text":"In this example, we have already shown an important feature, which is handling missing values. More on this topic is in TODO.","category":"page"},{"location":"architecture/overview/","page":"Overview","title":"Overview","text":"BagNode is processed by BagModel, which contains a two neural networks and an aggregation operator. The first network (im for instance model) converts the data of bag to a matrix. This matrix representation is used in aggreagation which produces a single vector per bag. This matrix is then passed to another feed forward model (bm for bag model) producing the final output.  For the above simple BagNode, the corresponding BagModel with mean aggregation function would look like","category":"page"},{"location":"architecture/overview/","page":"Overview","title":"Overview","text":"julia> m = BagModel(ArrayModel(Dense(2,3)),\n           SegmentedMean(3),\n           Dense(3,4)\n       )\nBagModel ↦ SegmentedMean(3) ↦ ArrayModel(Dense(3, 4))\n  └── ArrayModel(Dense(2, 3))\njulia> m(ds)\nArrayNode(4, 3)","category":"page"},{"location":"architecture/overview/","page":"Overview","title":"Overview","text":"where im = ArrayModel(Dense(2,3)) and bm = Dense(3,4).","category":"page"},{"location":"architecture/overview/","page":"Overview","title":"Overview","text":"Notice that even though the BagNode contained a simple Matrix, it has been wrapped in ArrayNode and the same holds for BagModel. This is important for consistency, since the basic assumption of the library is that model applied on corresponding data produces an ArrayNode. That means that the BagNode expects that m.im(ds.data) returns an ArrayNode with a single observation per instances. ","category":"page"},{"location":"architecture/overview/#ProductNodes-and-ProductModels","page":"Overview","title":"ProductNodes and ProductModels","text":"","category":"section"},{"location":"architecture/overview/","page":"Overview","title":"Overview","text":"ProductNode can be thougt as about a Cartesian product or a Dictionary. It holds a Tuple or NamedTuple of nodes. For example a ProductNode with a BagNode and ArrayNode as childs would look like","category":"page"},{"location":"architecture/overview/","page":"Overview","title":"Overview","text":"julia> ds = ProductNode(\n    (a = BagNode(ArrayNode(Float32.([1 2 3; 4 5 6])), [1:2, 0:-1, 3:3]),\n    b = ArrayNode(Float32.([4 5 6; 1 2 3]))))\nProductNode\n  ├── a: BagNode with 3 bag(s)\n  │        └── ArrayNode(2, 3)\n  └── b: ArrayNode(2, 3)","category":"page"},{"location":"architecture/overview/","page":"Overview","title":"Overview","text":"Similarly, the ProductModel contains a (Named)Tuple of models procesing in childs (stored in ms standing for models). Again, since the library is based on the property that output of model is an ArrayNode, the product model applies models from ms to appropriate nodes from ds.data and vertically concatenates the output, which is then processed by a feedforward network in m. An example of model processing the above sample would be","category":"page"},{"location":"architecture/overview/","page":"Overview","title":"Overview","text":"m = ProductModel((\n    a = BagModel(ArrayModel(Dense(2,3)),\n           SegmentedMean(3),\n           Dense(3,4)\n       ),\n    b = ArrayModel(Dense(2,3)),\n    ),\n    ArrayModel(Dense(7,5))\n    )\n\njulia> m(ds)\nArrayNode(5, 3)","category":"page"},{"location":"architecture/overview/","page":"Overview","title":"Overview","text":"In general, we recommend to use NamedTuple, because key is used to index models in ProductModel.","category":"page"},{"location":"architecture/overview/#Nesting-of-nodes","page":"Overview","title":"Nesting of nodes","text":"","category":"section"},{"location":"architecture/overview/","page":"Overview","title":"Overview","text":"Recall the basic design decision, that output of each model on appropriate data is always an ArrayNode, the nesting of nodes is a key feature. In the example below, we nest two BagNodes and create the appropriate model.","category":"page"},{"location":"architecture/overview/","page":"Overview","title":"Overview","text":"julia> ds = BagNode(BagNode(ArrayNode(randn(4,10)),[1:2,3:4,5:5,6:7,8:10]),[1:1,2:3,4:5])\nBagNode with 3 bag(s)\n  └── BagNode with 5 bag(s)\n        └── ArrayNode(4, 10)\n\njulia> m = BagModel(\n    BagModel(\n        ArrayModel(Dense(4, 3, Flux.relu)),   \n        SegmentedMeanMax(3),\n        ArrayModel(Dense(6, 3, Flux.relu))),\n    SegmentedMeanMax(3),\n    ArrayModel(Chain(Dense(6, 3, Flux.relu), Dense(3,2))))\n\nBagModel ↦ ⟨SegmentedMean(3), SegmentedMax(3)⟩ ↦ ArrayModel(Chain(Dense(6, 3, relu), Dense(3, 2)))\n  └── BagModel ↦ ⟨SegmentedMean(3), SegmentedMax(3)⟩ ↦ ArrayModel(Dense(6, 3, relu))\n        └── ArrayModel(Dense(4, 3, relu))\n\njulia> m(ds)\n\nArrayNode(2, 3)","category":"page"},{"location":"architecture/custom/#How-to-add-the-Custom-Node-and-Custom-Model","page":"Custom Nodes","title":"How to add the Custom Node and Custom Model","text":"","category":"section"},{"location":"architecture/custom/","page":"Custom Nodes","title":"Custom Nodes","text":"DataNodes are lightweight wrappers around data, such as Array, DataFrames, etc. Their primary purpose is to allow nesting (if needed), to create subsets using getindex (by implementing subset) and concatenate them using cat (implementing by catobs). Note that internally, nodes are concatenated using reduce(catobs, ...), since catobs(x...) compiles a new function for each observed length of the argument, which can quicly lead to momery exhaustion. The new Node should be also registered with HierarchicalUtils to support pretty print.","category":"page"},{"location":"architecture/custom/","page":"Custom Nodes","title":"Custom Nodes","text":"Let's walk through an implementation of ArrayNode holding a matrix or a vectors.","category":"page"},{"location":"architecture/custom/","page":"Custom Nodes","title":"Custom Nodes","text":"ArrayNode has a simple structure holding only Array, which is considered the data and optionally some metadata, which can be literally anything.","category":"page"},{"location":"architecture/custom/","page":"Custom Nodes","title":"Custom Nodes","text":"struct ArrayNode{A,C} <: AbstractNode\n    data::A\n    metadata::C\nend","category":"page"},{"location":"architecture/custom/","page":"Custom Nodes","title":"Custom Nodes","text":"ArrayNode had overloaded a getindex to support indexing. But the getindex just calls subset(x::ArrayNode, idxs), which is used to correctly slice arrays according to the last dimension.","category":"page"},{"location":"architecture/custom/","page":"Custom Nodes","title":"Custom Nodes","text":"This mean that if you want to define your own DataNode, in order to be compatible with the rest of the library it has to implement subset and reduce(::typeof{catobs}, Vector{T}) where {T<:YourType}","category":"page"},{"location":"architecture/custom/#A-simple-container-for-unix-pathnames","page":"Custom Nodes","title":"A simple container for unix pathnames","text":"","category":"section"},{"location":"architecture/custom/","page":"Custom Nodes","title":"Custom Nodes","text":"We give it a twist, such that the extractor will be part of the model definition, which is going to be cute.","category":"page"},{"location":"architecture/custom/","page":"Custom Nodes","title":"Custom Nodes","text":"Let's start by defining the structure holding pathnames, supporting nobs joining of two structures and indexing into the structure. A last touch is to extend the pretty printing.","category":"page"},{"location":"architecture/custom/","page":"Custom Nodes","title":"Custom Nodes","text":"struct PathNode{S<:AbstractString,C} <: AbstractNode\n    data::Vector{S}\n    metadata::C\nend\n\nPathNode(data::Vector{S}) where {S<:AbstractString} = PathNode(data, nothing)\n\nBase.ndims(x::PathNode) = Colon()\nStatsBase.nobs(a::PathNode) = length(a.data)\nStatsBase.nobs(a::PathNode, ::Type{ObsDim.Last}) = nobs(a)\n\nfunction Base.reduce(::typeof(Mill.catobs), as::Vector{T}) where {T<:PathNode}\n    data = reduce(vcat, [x.data for x in as])\n    metadata = reduce(catobs, [a.metadata for a in as])\n    PathNode(data, metadata)\nend\n\nBase.getindex(x::PathNode, i::VecOrRange{<:Int}) = PathNode(subset(x.data, i), subset(x.metadata, i))","category":"page"},{"location":"architecture/custom/","page":"Custom Nodes","title":"Custom Nodes","text":"Similarly, we define a ModelNode which will be a counterpart processing the data. Note that the part of the ModelNode is a function which converts the pathanme string to Matrix (or other Mill structures). Again, we add a support for pretty printing.","category":"page"},{"location":"architecture/custom/","page":"Custom Nodes","title":"Custom Nodes","text":"struct PathModel{T,F} <: AbstractMillModel\n    m::T\n    path2mill::F\nend\n\nFlux.@functor PathModel\n\n(m::PathModel)(x::PathNode)  = m.m(m.path2mill(x))\n\nfunction Mill.modelprint(io::IO, m::PathModel; pad=[], s=\"\", tr=false)\n    c = COLORS[(length(pad)%length(COLORS))+1]\n    paddedprint(io, \"PathModel$(tr_repr(s, tr))\\n\", color=c)\n    paddedprint(io, \"  └── \", color=c, pad=pad)\n    modelprint(io, m.m, pad=[pad; (c, \"      \")])\nend","category":"page"},{"location":"architecture/custom/","page":"Custom Nodes","title":"Custom Nodes","text":"Finally, let's define function path2mill, which converts a list of strings to Mill internal structure.","category":"page"},{"location":"architecture/custom/","page":"Custom Nodes","title":"Custom Nodes","text":"function path2mill(s::String)\n\tss = String.(split(s, \"/\"))\n\tBagNode(ArrayNode(Mill.NGramMatrix(ss, 3, 256, 2053)), AlignedBags([1:length(ss)]))\nend\n\npath2mill(ss::Vector{S}) where {S<:AbstractString} = reduce(catobs, map(path2mill, ss))\npath2mill(ds::PathNode) = path2mill(ds.data)\n","category":"page"},{"location":"architecture/custom/","page":"Custom Nodes","title":"Custom Nodes","text":"And then, let's test the solution","category":"page"},{"location":"architecture/custom/","page":"Custom Nodes","title":"Custom Nodes","text":"ds = PathNode([\"/etc/passwd\", \"/home/tonda/.bashrc\"])\npm = PathModel(reflectinmodel(path2mill(ds), d -> Dense(d, 10, relu)), path2mill)\npm(ds).data","category":"page"},{"location":"architecture/custom/","page":"Custom Nodes","title":"Custom Nodes","text":"A final touch would be to overload the reflectinmodel as","category":"page"},{"location":"architecture/custom/","page":"Custom Nodes","title":"Custom Nodes","text":"function Mill.reflectinmodel(ds::PathNode, args...)\n\tpm = reflectinmodel(path2mill(ds), args...)\n\tPathModel(pm, path2mill)\nend\n","category":"page"},{"location":"architecture/custom/","page":"Custom Nodes","title":"Custom Nodes","text":"which can make it seamless","category":"page"},{"location":"architecture/custom/","page":"Custom Nodes","title":"Custom Nodes","text":"ds = PathNode([\"/etc/passwd\", \"/home/tonda/.bashrc\"])\npm = reflectinmodel(ds, d -> Dense(d, 10, relu))\npm(ds).data","category":"page"},{"location":"architecture/reflectin/#ReflectInModel","page":"ReflectInModel","title":"ReflectInModel","text":"","category":"section"},{"location":"architecture/reflectin/","page":"ReflectInModel","title":"ReflectInModel","text":"Constructions of large models can be boring and a process prone to errors. Therefore Mill.jl comes with a convenient function reflectinmodel which does this for you. The result is certainly suboptimal, which can be said about almost any Neural Network. The reflect in model accepts three important parameters.","category":"page"},{"location":"architecture/reflectin/","page":"ReflectInModel","title":"ReflectInModel","text":"The data sample serving as a specimen, which is needed to calculate dimensions and know the structure;\na function returning a feed-forward model (or a function) accepting data in form of a Matrix with dimension d;\na function returing aggregation function in BagModels.","category":"page"},{"location":"architecture/reflectin/","page":"ReflectInModel","title":"ReflectInModel","text":"For example to create a model for the below sample","category":"page"},{"location":"architecture/reflectin/","page":"ReflectInModel","title":"ReflectInModel","text":"ds = BagNode(\n    ProductNode(\n        (BagNode(ArrayNode(randn(4,10)),[1:2,3:4,5:5,6:7,8:10]),\n        ArrayNode(randn(3,5)),\n        BagNode(\n            BagNode(ArrayNode(randn(2,30)),[i:i+1 for i in 1:2:30]),\n            [1:3,4:6,7:9,10:12,13:15]),\n        ArrayNode(randn(2,5)))),\n    [1:1,2:3,4:5])\n\nBagNode with 3 bag(s)\n  └── ProductNode\n        ├── BagNode with 5 bag(s)\n        │     ⋮\n        ├── ArrayNode(3, 5)\n        ├── BagNode with 5 bag(s)\n        │     ⋮\n        └── ArrayNode(2, 5)\n\njulia> m = reflectinmodel(ds, d -> Dense(d, 5, relu), d -> SegmentedMeanMax(d))\n\nBagModel ↦ ⟨SegmentedMean(5), SegmentedMax(5)⟩ ↦ ArrayModel(Dense(10, 5, relu))\n  └── ProductModel ↦ ArrayModel(Dense(20, 5, relu))\n        ├── BagModel ↦ ⟨SegmentedMean(5), SegmentedMax(5)⟩ ↦ ArrayModel(Dense(10, 5, relu))\n        │     ⋮\n        ├── ArrayModel(Dense(3, 5, relu))\n        ├── BagModel ↦ ⟨SegmentedMean(5), SegmentedMax(5)⟩ ↦ ArrayModel(Dense(10, 5, relu))\n        │     ⋮\n        └── ArrayModel(Dense(2, 5, relu))\n\njulia> m(ds)\nArrayNode(5, 3)","category":"page"},{"location":"architecture/reflectin/","page":"ReflectInModel","title":"ReflectInModel","text":"The reflectinmodel allows customization. To index into the sample, printtree(ds, trav = true) prints the sample with identifiers identifying invidual nodes. For example","category":"page"},{"location":"architecture/reflectin/","page":"ReflectInModel","title":"ReflectInModel","text":"julia> Mill.printtree(ds, trav = true)\nBagNode with 3 bag(s) [\"\"]\n  └── ProductNode [\"U\"]\n        ├── BagNode with 5 bag(s) [\"Y\"]\n        │     └── ArrayNode(4, 10) [\"a\"]\n        ├── ArrayNode(3, 5) [\"c\"]\n        ├── BagNode with 5 bag(s) [\"g\"]\n        │     └── BagNode with 15 bag(s) [\"i\"]\n        │           └── ArrayNode(2, 30) [\"j\"]\n        └── ArrayNode(2, 5) [\"k\"]","category":"page"},{"location":"architecture/reflectin/","page":"ReflectInModel","title":"ReflectInModel","text":"These identifiers can be used to override the default construction functions. Note that the output, i.e. the last feed-forward network of the whole model is always tagged \"\", which simpliefies putting linear layer with appropriate out dimension on the end. These overrides are put to dictionaries with b overriding constructions of feed-forward models and a overriding construction of aggregation functions. For example to specify just last Feed Forward Neural Network","category":"page"},{"location":"architecture/reflectin/","page":"ReflectInModel","title":"ReflectInModel","text":"julia> m = reflectinmodel(ds, \n    d -> Dense(d, 5, relu), \n    d -> SegmentedMeanMax(d),\n    b = Dict(\"\" => d -> Chain(Dense(d, 20,relu), Dense(20,12))),\n    )\n\nBagModel ↦ ⟨SegmentedMean(5), SegmentedMax(5)⟩ ↦ ArrayModel(Chain(Dense(10, 20, relu), Dense(20, 12)))\n  └── ProductModel ↦ ArrayModel(Dense(20, 5, relu))\n        ├── BagModel ↦ ⟨SegmentedMean(5), SegmentedMax(5)⟩ ↦ ArrayModel(Dense(10, 5, relu))\n        │     ⋮\n        ├── ArrayModel(Dense(3, 5, relu))\n        ├── BagModel ↦ ⟨SegmentedMean(5), SegmentedMax(5)⟩ ↦ ArrayModel(Dense(10, 5, relu))\n        │     ⋮\n        └── ArrayModel(Dense(2, 5, relu))","category":"page"},{"location":"architecture/reflectin/","page":"ReflectInModel","title":"ReflectInModel","text":"To change the aggregation function in the middle (for some obscure reason)","category":"page"},{"location":"architecture/reflectin/","page":"ReflectInModel","title":"ReflectInModel","text":"m = reflectinmodel(ds, \n    d -> Dense(d, 5, relu), \n    d -> SegmentedMeanMax(d),\n    b = Dict(\"\" => d -> Chain(Dense(d, 20,relu), Dense(20,12))),\n    a = Dict(\"Y\" => d -> SegmentedMean(d), \"g\" => d -> SegmentedMean(d))\n    );\njulia> printtree(m)\nBagModel ↦ ⟨SegmentedMean(5), SegmentedMax(5)⟩ ↦ ArrayModel(Chain(Dense(10, 20, relu), Dense(20, 12)))\n  └── ProductModel ↦ ArrayModel(Dense(20, 5, relu))\n        ├── BagModel ↦ SegmentedMean(5) ↦ ArrayModel(Dense(5, 5, relu))\n        │     └── ArrayModel(Dense(4, 5, relu))\n        ├── ArrayModel(Dense(3, 5, relu))\n        ├── BagModel ↦ SegmentedMean(5) ↦ ArrayModel(Dense(5, 5, relu))\n        │     └── BagModel ↦ ⟨SegmentedMean(5), SegmentedMax(5)⟩ ↦ ArrayModel(Dense(10, 5, relu))\n        │           └── ArrayModel(Dense(2, 5, relu))\n        └── ArrayModel(Dense(2, 5, relu))","category":"page"},{"location":"#Mill.jl","page":"Home","title":"Mill.jl","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Mill.jl is a library build on top of Flux.jl aimed to flexibly prototype hierarchical multi-instance learning models as described in Tomáš Pevný , Petr Somol  (2017) and  Tomáš Pevný , Petr Somol  (2016)","category":"page"},{"location":"#What-is-Multiple-instance-learning-(MIL)-problem?","page":"Home","title":"What is Multiple instance learning (MIL) problem?","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Why should I care about MIL problems? Since the seminal paper of Ronald Fisher, the majority of machine learning problems deals with a problem shown below, ","category":"page"},{"location":"","page":"Home","title":"Home","text":"(Image: )","category":"page"},{"location":"","page":"Home","title":"Home","text":"where the input sample x is a vector (or more generally a tensor) of a fixed dimension, alteranativelly a sequence. ","category":"page"},{"location":"","page":"Home","title":"Home","text":"The consequence is that if we want to use a more elaborate description of iris above, for example we wish to describe each of its leaf, blossoms, and stem, we will have a hard time, because every flower has different number of them. This means that to use the usual \"fix dimension\" paradigm, we have to either use features from a single blossom and single leaf, or aggregate descriptions of their sets, such that the output has a fixed dimension. This is clearly undesirable. We wish a framework that can flexibly and automatically and seamlessly deals with these sets, sets of sets, and cartesian product. ","category":"page"},{"location":"","page":"Home","title":"Home","text":"In Multiple instance learning the sample x is a set of vectors (or matrices) x_1ldotsx_l with x_i in R^d, which means that order does not matter, and which is also the feature making MIL problems different from sequences. The multi-instance problems have been introduced in by Tom Diettrich in Thomas G. Dietterich , Richard H. Lathrop , Tomás Lozano-Pérez  (1997) in 1997, and extended and generalized in a series of works Tomáš Pevný , Petr Somol  (2017), Tomáš Pevný , Petr Somol  (2016), Tomáš Pevný , Vojtěch Kovařík  (2019). The most comprehensive introduction known to authors is Šimon Mandlík  (2020)","category":"page"},{"location":"","page":"Home","title":"Home","text":"The Hierarchical Multiple instance learning would approach the problem of iris classification as outlined below.","category":"page"},{"location":"","page":"Home","title":"Home","text":"(Image: )","category":"page"},{"location":"","page":"Home","title":"Home","text":"It will describe each leaf by a vector implying that all leaves are described by a set of vectors. The same will be done for blossoms (the set will be different of course). Note that such description allows each flower to have a different numbers of each entity. Finally, there will be a single vector describing a stem, since there is only one.","category":"page"},{"location":"","page":"Home","title":"Home","text":"How does the MIL copes with variability in number of flowers and leafs (in MIL parlance they are called instances and their set is called a bag)? For each MIL problem, there are two feed-forward neural networks with element-wise aggregation operator like mean (or maximum) sandwiched between them. Denoting those feed-forward networks (FFN) by  f_1  and f_2, the output of a bag calculated is calculated as f_2 left(frac1lsum_i=1^l f_1(x_i) right), where we have used mean as an aggregation function. In Tomáš Pevný , Vojtěch Kovařík  (2019), authors have further extended the universal approximation theorem to MIL problems, their Cartesian products, and nested  MIL problems, i.e. a case where instances of one bag are in fact bags. ","category":"page"},{"location":"","page":"Home","title":"Home","text":"This means that the flower in the above Iris example would be described by one bag describing leafs, another bag describing blossoms, and a vector describing stem. The HMIL model would have two FFNs to convert set of leafs to a single vector, another set of two FFNs to convert set of blossoms to a single vector. These two outputs would be concatenated with a description of a stem, which would be fed to yet another FFN providing the final classifications. And since whole scheme is differentiable, we can use standard SGD to optimize all FFNs together using only labels on the level of output.","category":"page"},{"location":"","page":"Home","title":"Home","text":"The Mill library simplifies implementation of machine learning problems with (H)MIL representation. In theory, it can represent any problem that can be written represented in JSONs. That is why we have created a separate tool, JsonGrinder, which helps to Mill your JSONs.","category":"page"},{"location":"#Relation-to-Graph-Neural-Networks","page":"Home","title":"Relation to Graph Neural Networks","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"HMIL problems can be seen as a special subset of general graphs. They differ in two important ways","category":"page"},{"location":"","page":"Home","title":"Home","text":"In general graphs, vertices are of a small number of semantic type, whereas in HMIL problems, the number of semantic types of vertices is much higher (it is helpful to think about HMIL problems as about those for which JSON is a natural representation).\nThe computational graph of HMIL is a tree, which implies that there exist an efficient inference. Contrary, in general graphs (with loops) there is no efficient inference and one has to resort to message passing (Loopy belief propagation).\nOne update message in loopy belief propagation can be viewed as a MIL problem, as it has to produce a vector based on infomation inthe neighborhood, which can contain arbitrary number of vertices.","category":"page"},{"location":"#Difference-to-sequences","page":"Home","title":"Difference to sequences","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"The major difference is that sequence does not matter. This means that if a sequence (abc) should be treated as a set, then the output of a function f should be the same for any permutation, i.e. f(abc) = f(cba) = f(bac) =ldots. This property has a dramatic consequence of the computational complexity. Sequences are typically modeled using Recurrent Neural Networks (RNN), where the output is calculated as f(abc) = g(a g(b g(c))) (with a slight abuse of a notation). During optimization, a gradient of g needs to be calculated recursively, giving raise to infamous vanishing / expanding gradient. In constrast, in MIL calculates the output as $ f(\\frac{1}{3}(g(a) + g(b) + g(c)))$ (slightly abusing notation again), which means that the gradient of g is calculated in parallel and not recurrently. ","category":"page"},{"location":"","page":"Home","title":"Home","text":"A more detailed overview of this subject can be found in Šimon Mandlík  (2020).","category":"page"},{"location":"#References","page":"Home","title":"References","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"","category":"page"}]
}
